{
  "metadata": {
    "chunk_number": 3,
    "timestamp": "2025-01-19T18:27:40.119497",
    "total_successful": 48
  },
  "documents": [
    {
      "url": "https://opentextbc.ca/introductiontopsychology/chapter/8-1-memories-as-types-and-stages/",
      "title": "9.1 Memories as Types and Stages – Introduction to Psychology – 1st Canadian Edition",
      "author": "",
      "published_date": "2014-10-17T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<p><small>Chapter 9. Remembering and Judging</small></p>\t<section>\n<header>\n</header>\n<div>\n<h3>Learning Objectives</h3>\n<ol>\n<li>Compare and contrast explicit and implicit memory, identifying the features that define each.</li>\n<li>Explain the function and duration of eidetic and echoic memories.</li>\n<li>Summarize the capacities of short-term memory and explain how working memory is used to process information in it.</li>\n</ol>\n</div>\n<p>As you can see in Table 9.1, “Memory Conceptualized in Terms of Types, Stages, and Processes,” psychologists conceptualize memory in terms of <em>types</em>, in terms of <em>stages</em>, and in terms of <em>processes</em>. In this section we will consider the two<strong> types of memory</strong>, <em>explicit memory</em> and <em>implicit memory</em>, and then the three major <strong>memory stages</strong>: <em>sensory</em>, <em>short-term</em>, and <em>long-term</em> (Atkinson &amp; Shiffrin, 1968). Then, in the next section, we will consider the nature of long-term memory, with a particular emphasis on the cognitive techniques we can use to improve our memories. Our discussion will focus on the three processes that are central to <strong>long-term memory</strong>: <em>encoding</em>, <em>storage</em>, and <em>retrieval</em>.</p>\n<table>\n<caption>Table 9.1 Memory Conceptualized in Terms of Types, Stages, and Processes.</caption>\n<tbody>\n<tr>\n<td>As types</td>\n<td>\n<ul>\n<li>Explicit memory</li>\n<li>Implicit memory</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td>As stages</td>\n<td>\n<ul>\n<li>Sensory memory</li>\n<li>Short-term memory</li>\n<li>Long-term memory</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td>As processes</td>\n<td>\n<ul>\n<li>Encoding</li>\n<li>Storage</li>\n<li>Retrieval</li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<h2>Explicit Memory</h2>\n<p>When we assess memory by asking a person to consciously remember things, we are measuring <em>explicit memory</em>. <strong>Explicit memory</strong> refers to <em>knowledge or experiences that can be consciously remembered</em>. As you can see in Figure 9.2, “Types of Memory,” there are two types of explicit memory: <em>episodic</em> and <em>semantic</em>.<strong> Episodic memory</strong> refers to <em>the firsthand experiences that we have had</em> (e.g., recollections of our high school graduation day or of the fantastic dinner we had in New York last year). <strong>Semantic memory</strong> refers to <em>our knowledge of facts and concepts about the world</em> (e.g., that the absolute value of −90 is greater than the absolute value of 9 and that one definition of the word “affect” is “the experience of feeling or emotion”).</p>\n<figure><a href=\"http://opentextbc.ca/introductiontopsychology/wp-content/uploads/sites/9/2013/11/d3f9b937b11eb7ad6effb49222788ebb.jpg\"></a><figcaption>Figure 9.2 Types of Memory.</figcaption></figure>\n<p>Explicit memory is assessed using measures in which the individual being tested must consciously attempt to remember the information. A <strong>recall memory</strong> test is <em>a measure of explicit memory that involves bringing from memory information that has previously been remembered</em>. We rely on our recall memory when we take an essay test, because the test requires us to generate previously remembered information. A multiple-choice test is an example of a <strong>recognition memory test</strong>, <em>a measure of explicit memory that involves determining whether information has been seen or learned before</em>.</p>\n<p>Your own experiences taking tests will probably lead you to agree with the scientific research finding that recall is more difficult than recognition. Recall, such as required on essay tests, involves two steps: first generating an answer and then determining whether it seems to be the correct one. Recognition, as on multiple-choice test, only involves determining which item from a list seems most correct (Haist, Shimamura, &amp; Squire, 1992). Although they involve different processes, recall and recognition memory measures tend to be correlated. Students who do better on a multiple-choice exam will also, by and large, do better on an essay exam (Bridgeman &amp; Morgan, 1996).</p>\n<p>A third way of measuring memory is known as <em>relearning</em> (Nelson, 1985). Measures of <strong>relearning</strong> (or savings) <em>assess how much more quickly information is processed or learned when it is studied again after it has already been learned but then forgotten</em>. If you have taken some French courses in the past, for instance, you might have forgotten most of the vocabulary you learned. But if you were to work on your French again, you’d learn the vocabulary much faster the second time around. Relearning can be a more sensitive measure of memory than either recall or recognition because it allows assessing memory in terms of “how much” or “how fast” rather than simply “correct” versus “incorrect” responses. Relearning also allows us to measure memory for procedures like driving a car or playing a piano piece, as well as memory for facts and figures.</p>\n<h2>Implicit Memory</h2>\n<p>While explicit memory consists of the things that we can consciously report that we know, implicit memory refers to knowledge that we cannot consciously access. However, implicit memory is nevertheless exceedingly important to us because it has a direct effect on our behaviour. <strong>Implicit memory </strong>refers to <em>the influence of experience on behaviour, even if the individual is not aware of those influences</em>. As you can see in Figure 9.2, “Types of Memory,” there are three general types of implicit memory: procedural memory, classical conditioning effects, and priming.</p>\n<p><strong>Procedural memory </strong>refers to <em>our often unexplainable knowledge of how to do things</em>. When we walk from one place to another, speak to another person in English, dial a cell phone, or play a video game, we are using procedural memory. Procedural memory allows us to perform complex tasks, even though we may not be able to explain to others how we do them. There is no way to tell someone how to ride a bicycle; a person has to learn by doing it. The idea of implicit memory helps explain how infants are able to learn. The ability to crawl, walk, and talk are procedures, and these skills are easily and efficiently developed while we are children despite the fact that as adults we have no conscious memory of having learned them.</p>\n<p>A second type of implicit memory is <strong>classical conditioning effects</strong>, in which <em>we learn, often without effort or awareness, to associate neutral stimuli (such as a sound or a light) with another stimulus (such as food), which creates a naturally occurring response, such as enjoyment or salivation</em>. The memory for the association is demonstrated when the conditioned stimulus (the sound) begins to create the same response as the unconditioned stimulus (the food) did before the learning.</p>\n<p>The final type of implicit memory is known as <strong>priming</strong>, or <em>changes in behaviour as a result of experiences that have happened frequently or recently</em>. Priming refers both to the activation of knowledge (e.g., we can prime the concept of kindness by presenting people with words related to kindness) and to the influence of that activation on behaviour (people who are primed with the concept of kindness may act more kindly).</p>\n<p>One measure of the influence of priming on implicit memory is the <em>word fragment test</em>, in which a person is asked to fill in missing letters to make words. You can try this yourself: First, try to complete the following word fragments, but work on each one for only three or four seconds. Do any words pop into mind quickly?</p>\n<p>_ i b _ a _ y</p>\n<p>_ h _ s _ _ i _ n</p>\n<p>_ o _ k</p>\n<p>_ h _ i s _</p>\n<p>Now read the following sentence carefully:</p>\n<p>“He got his materials from the shelves, checked them out, and then left the building.”</p>\n<p>Then try again to make words out of the word fragments.</p>\n<p>I think you might find that it is easier to complete fragments 1 and 3 as “library” and “book,” respectively, after you read the sentence than it was before you read it. However, reading the sentence didn’t really help you to complete fragments 2 and 4 as “physician” and “chaise.” This difference in implicit memory probably occurred because as you read the sentence, the concept of “library” (and perhaps “book”) was primed, even though they were never mentioned explicitly. Once a concept is primed it influences our behaviours, for instance, on word fragment tests.</p>\n<p>Our everyday behaviours are influenced by priming in a wide variety of situations. Seeing an advertisement for cigarettes may make us start smoking, seeing the flag of our home country may arouse our patriotism, and seeing a student from a rival school may arouse our competitive spirit. And these influences on our behaviours may occur without our being aware of them.</p>\n<div>\n<h3>Research Focus: Priming Outside Awareness Influences Behaviour</h3>\n<p>One of the most important characteristics of implicit memories is that they are frequently formed and used <em>automatically</em>, without much effort or awareness on our part. In one demonstration of the automaticity and influence of priming effects, John Bargh and his colleagues (Bargh, Chen, &amp; Burrows, 1996) conducted a study in which they showed undergraduate students lists of five scrambled words, each of which they were to make into a sentence. Furthermore, for half of the research participants, the words were related to stereotypes of the elderly. These participants saw words such as the following:</p>\n<p>in Victoria retired live people</p>\n<p>bingo man the forgetful plays</p>\n<p>The other half of the research participants also made sentences, but from words that had nothing to do with elderly stereotypes. The purpose of this task was to prime stereotypes of elderly people in memory for some ",
        "html": "<div><div>\n<p><small>Chapter 9. Remembering and Judging</small></p>\t<section>\n<header>\n</header>\n<div>\n<h3>Learning Objectives</h3>\n<ol>\n<li>Compare and contrast explicit and implicit memory, identifying the features that define each.</li>\n<li>Explain the function and duration of eidetic and echoic memories.</li>\n<li>Summarize the capacities of short-term memory and explain how working memory is used to process information in it.</li>\n</ol>\n</div>\n<p>As you can see in Table 9.1, “Memory Conceptualized in Terms of Types, Stages, and Processes,” psychologists conceptualize memory in terms of <em>types</em>, in terms of <em>stages</em>, and in terms of <em>processes</em>. In this section we will consider the two<strong> types of memory</strong>, <em>explicit memory</em> and <em>implicit memory</em>, and then the three major <strong>memory stages</strong>: <em>sensory</em>, <em>short-term</em>, and <em>long-term</em> (Atkinson &amp; Shiffrin, 1968). Then, in the next section, we will consider the nature of long-term memory, with a particular emphasis on the cognitive techniques we can use to improve our memories. Our discussion will focus on the three processes that are central to <strong>long-term memory</strong>: <em>encoding</em>, <em>storage</em>, and <em>retrieval</em>.</p>\n<table>\n<caption>Table 9.1 Memory Conceptualized in Terms of Types, Stages, and Processes.</caption>\n<tbody>\n<tr>\n<td>As types</td>\n<td>\n<ul>\n<li>Explicit memory</li>\n<li>Implicit memory</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td>As stages</td>\n<td>\n<ul>\n<li>Sensory memory</li>\n<li>Short-term memory</li>\n<li>Long-term memory</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td>As processes</td>\n<td>\n<ul>\n<li>Encoding</li>\n<li>Storage</li>\n<li>Retrieval</li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<h2>Explicit Memory</h2>\n<p>When we assess memory by asking a person to consciously remember things, we are measuring <em>explicit memory</em>. <strong>Explicit memory</strong> refers to <em>knowledge or experiences that can be consciously remembered</em>. As you can see in Figure 9.2, “Types of Memory,” there are two types of explicit memory: <em>episodic</em> and <em>semantic</em>.<strong> Episodic memory</strong> refers to <em>the firsthand experiences that we have had</em> (e.g., recollections of our high school graduation day or of the fantastic dinner we had in New York last year). <strong>Semantic memory</strong> refers to <em>our knowledge of facts and concepts about the world</em> (e.g., that the absolute value of −90 is greater than the absolute value of 9 and that one definition of the word “affect” is “the experience of feeling or emotion”).</p>\n<figure><a href=\"http://opentextbc.ca/introductiontopsychology/wp-content/uploads/sites/9/2013/11/d3f9b937b11eb7ad6effb49222788ebb.jpg\"></a><figcaption>Figure 9.2 Types of Memory.</figcaption></figure>\n<p>Explicit memory is assessed using measures in which the individual being tested must consciously attempt to remember the information. A <strong>recall memory</strong> test is <em>a measure of explicit memory that involves bringing from memory information that has previously been remembered</em>. We rely on our recall memory when we take an essay test, because the test requires us to generate previously remembered information. A multiple-choice test is an example of a <strong>recognition memory test</strong>, <em>a measure of explicit memory that involves determining whether information has been seen or learned before</em>.</p>\n<p>Your own experiences taking tests will probably lead you to agree with the scientific research finding that recall is more difficult than recognition. Recall, such as required on essay tests, involves two steps: first generating an answer and then determining whether it seems to be the correct one. Recognition, as on multiple-choice test, only involves determining which item from a list seems most correct (Haist, Shimamura, &amp; Squire, 1992). Although they involve different processes, recall and recognition memory measures tend to be correlated. Students who do better on a multiple-choice exam will also, by and large, do better on an essay exam (Bridgeman &amp; Morgan, 1996).</p>\n<p>A third way of measuring memory is known as <em>relearning</em> (Nelson, 1985). Measures of <strong>relearning</strong> (or savings) <em>assess how much more quickly information is processed or learned when it is studied again after it has already been learned but then forgotten</em>. If you have taken some French courses in the past, for instance, you might have forgotten most of the vocabulary you learned. But if you were to work on your French again, you’d learn the vocabulary much faster the second time around. Relearning can be a more sensitive measure of memory than either recall or recognition because it allows assessing memory in terms of “how much” or “how fast” rather than simply “correct” versus “incorrect” responses. Relearning also allows us to measure memory for procedures like driving a car or playing a piano piece, as well as memory for facts and figures.</p>\n<h2>Implicit Memory</h2>\n<p>While explicit memory consists of the things that we can consciously report that we know, implicit memory refers to knowledge that we cannot consciously access. However, implicit memory is nevertheless exceedingly important to us because it has a direct effect on our behaviour. <strong>Implicit memory </strong>refers to <em>the influence of experience on behaviour, even if the individual is not aware of those influences</em>. As you can see in Figure 9.2, “Types of Memory,” there are three general types of implicit memory: procedural memory, classical conditioning effects, and priming.</p>\n<p><strong>Procedural memory </strong>refers to <em>our often unexplainable knowledge of how to do things</em>. When we walk from one place to another, speak to another person in English, dial a cell phone, or play a video game, we are using procedural memory. Procedural memory allows us to perform complex tasks, even though we may not be able to explain to others how we do them. There is no way to tell someone how to ride a bicycle; a person has to learn by doing it. The idea of implicit memory helps explain how infants are able to learn. The ability to crawl, walk, and talk are procedures, and these skills are easily and efficiently developed while we are children despite the fact that as adults we have no conscious memory of having learned them.</p>\n<p>A second type of implicit memory is <strong>classical conditioning effects</strong>, in which <em>we learn, often without effort or awareness, to associate neutral stimuli (such as a sound or a light) with another stimulus (such as food), which creates a naturally occurring response, such as enjoyment or salivation</em>. The memory for the association is demonstrated when the conditioned stimulus (the sound) begins to create the same response as the unconditioned stimulus (the food) did before the learning.</p>\n<p>The final type of implicit memory is known as <strong>priming</strong>, or <em>changes in behaviour as a result of experiences that have happened frequently or recently</em>. Priming refers both to the activation of knowledge (e.g., we can prime the concept of kindness by presenting people with words related to kindness) and to the influence of that activation on behaviour (people who are primed with the concept of kindness may act more kindly).</p>\n<p>One measure of the influence of priming on implicit memory is the <em>word fragment test</em>, in which a person is asked to fill in missing letters to make words. You can try this yourself: First, try to complete the following word fragments, but work on each one for only three or four seconds. Do any words pop into mind quickly?</p>\n<p>_ i b _ a _ y</p>\n<p>_ h _ s _ _ i _ n</p>\n<p>_ o _ k</p>\n<p>_ h _ i s _</p>\n<p>Now read the following sentence carefully:</p>\n<p>“He got his materials from the shelves, checked them out, and then left the building.”</p>\n<p>Then try again to make words out of the word fragments.</p>\n<p>I think you might find that it is easier to complete fragments 1 and 3 as “library” and “book,” respectively, after you read the sentence than it was before you read it. However, reading the sentence didn’t really help you to complete fragments 2 and 4 as “physician” and “chaise.” This difference in implicit memory probably occurred because as you read the sentence, the concept of “library” (and perhaps “book”) was primed, even though they were never mentioned explicitly. Once a concept is primed it influences our behaviours, for instance, on word fragment tests.</p>\n<p>Our everyday behaviours are influenced by priming in a wide variety of situations. Seeing an advertisement for cigarettes may make us start smoking, seeing the flag of our home country may arouse our patriotism, and seeing a student from a rival school may arouse our competitive spirit. And these influences on our behaviours may occur without our being aware of them.</p>\n<div>\n<h3>Research Focus: Priming Outside Awareness Influences Behaviour</h3>\n<p>One of the most important characteristics of implicit memories is that they are frequently formed and used <em>automatically</em>, without much effort or awareness on our part. In one demonstration of the automaticity and influence of priming effects, John Bargh and his colleagues (Bargh, Chen, &amp; Burrows, 1996) conducted a study in which they showed undergraduate students lists of five scrambled words, each of which they were to make into a sentence. Furthermore, for half of the research participants, the words were related to stereotypes of the elderly. These participants saw words such as the following:</p>\n<p>in Victoria retired live people</p>\n<p>bingo man the forgetful plays</p>\n<p>The other half of the research participants also made sentences, but from words that had nothing to do with elderly stereotypes. The purpose of this task was to prime stereotypes of elderly people in memory for some ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Chapter 9. Remembering and JudgingLearning ObjectivesCompare and contrast explicit and implicit memory, identifying the features that define each.Explain the function and duration of eidetic and echoic memories.Summarize the capacities of short-term memory and explain how working memory is used to process information in it.As you can see in Table 9.1, “Memory Conceptualized in Terms of Types, Stages, and Processes,” psychologists conceptualize memory in terms oftypes, in terms ofstages, and in t",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Chapter 9. Remembering and JudgingLearning ObjectivesCompare and contrast explicit and implicit memory, identifying the features that define each.Explain the function and duration of eidetic and echoic memories.Summarize the capacities of short-term memory and explain how working memory is used to process information in it.As you can see in Table 9.1, “Memory Conceptualized in Terms of Types, Stages, and Processes,” psychologists conceptualize memory in terms oftypes, in terms ofstages, and in t",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Learning ObjectivesCompare and contrast explicit and implicit memory, identifying the features that define each.Explain the function and duration of eidetic and echoic memories.Summarize the capacities of short-term memory and explain how working memory is used to process information in it.As you can see in Table 9.1, “Memory Conceptualized in Terms of Types, Stages, and Processes,” psychologists conceptualize memory in terms oftypes, in terms ofstages, and in terms ofprocesses. In this section ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Learning ObjectivesCompare and contrast explicit and implicit memory, identifying the features that define each.Explain the function and duration of eidetic and echoic memories.Summarize the capacities of short-term memory and explain how working memory is used to process information in it.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Research Focus: Priming Outside Awareness Influences BehaviourOne of the most important characteristics of implicit memories is that they are frequently formed and usedautomatically, without much effort or awareness on our part. In one demonstration of the automaticity and influence of priming effects, John Bargh and his colleagues (Bargh, Chen, & Burrows, 1996) conducted a study in which they showed undergraduate students lists of five scrambled words, each of which they were to make into a sen",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h3",
              "text": "Learning Objectives",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Explicit Memory",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Implicit Memory",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Research Focus: Priming Outside Awareness Influences Behaviour",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.nature.com/articles/s41539-020-0064-y",
      "title": "How to optimize knowledge construction in the brain",
      "author": "Marlieke Tina Renée Van Kesteren, Martijn  Meeter, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands",
      "published_date": "2020-05-01T00:00:00.000Z",
      "content": {
        "text": "Abstract Well-structured knowledge allows us to quickly understand the world around us and make informed decisions to adequately control behavior. Knowledge structures, or schemas, are presumed to aid memory encoding and consolidation of new experiences so we cannot only remember the past, but also guide behavior in the present and predict the future. However, very strong schemas can also lead to unwanted side effects such as false memories and misconceptions. To overcome this overreliance on a schema, we should aim to create robust schemas that are on the one hand strong enough to help to remember and predict, but also malleable enough to avoid such undesirable side effects. This raises the question as to whether there are ways to deliberately influence knowledge construction processes, with the goal to reach such optimally balanced schemas. Here, we will discuss how the mnemonic processes in our brains build long-term knowledge and, more specifically, how different phases of memory formation (encoding, consolidation, retrieval, and reconsolidation) contribute to this schema build-up. We finally provide ways how to best keep a balance between generalized semantic and detailed episodic memories, which can prove very useful in, e.g., educational settings.",
        "html": "Abstract Well-structured knowledge allows us to quickly understand the world around us and make informed decisions to adequately control behavior. Knowledge structures, or schemas, are presumed to aid memory encoding and consolidation of new experiences so we cannot only remember the past, but also guide behavior in the present and predict the future. However, very strong schemas can also lead to unwanted side effects such as false memories and misconceptions. To overcome this overreliance on a schema, we should aim to create robust schemas that are on the one hand strong enough to help to remember and predict, but also malleable enough to avoid such undesirable side effects. This raises the question as to whether there are ways to deliberately influence knowledge construction processes, with the goal to reach such optimally balanced schemas. Here, we will discuss how the mnemonic processes in our brains build long-term knowledge and, more specifically, how different phases of memory formation (encoding, consolidation, retrieval, and reconsolidation) contribute to this schema build-up. We finally provide ways how to best keep a balance between generalized semantic and detailed episodic memories, which can prove very useful in, e.g., educational settings.",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://www.magneticmemorymethod.com/memory-recall-retrieval",
      "title": "Memory Recall and Retrieval: The Definitive Guide",
      "author": "Anthony Metivier",
      "published_date": "2023-02-06T18:35:24.000Z",
      "content": {
        "text": "<div><div>\n<p>Podcast: <a href=\"https://traffic.libsyn.com/secure/magneticmemorymethod/Memory_Recall_and_Retrieval__The_Definitive_Guide.mp3\">Download</a></p><p>Subscribe: <a href=\"https://itunes.apple.com/us/podcast/anthony-metiviers-magnetic-memory-method-podcast/id790315802?mt=2&amp;ls=1#episodeGuid=https%3A%2F%2Fwww.magneticmemorymethod.com%2F%3Fp%3D23428\">Apple Podcasts</a> | <a href=\"https://www.google.com/podcasts?feed=aHR0cHM6Ly93d3cubWFnbmV0aWNtZW1vcnltZXRob2QuY29tL2ZlZWQvcG9kY2FzdC8\">Google Podcasts</a> | <a href=\"https://www.stitcher.com/podcast/anthony-metivier/the-magnetic-memory-method-podcast\">Stitcher</a> | <a href=\"https://www.magneticmemorymethod.com/feed/podcast/\">RSS</a></p><p>Memory recall, sometimes called retrieval, is the experience of remembering information.</p>\n<p>Here’s the catch:</p>\n<p>It can only happen after at least these other levels of memory have succeeded first:<br />\n</p>\n<ul>\n<li>Noticing and recognizing the importance of information (like someone’s name)</li>\n<li>Encoding (making an effort to remember)</li>\n<li>Decoding (making an effort to recall)</li>\n</ul>\n<p>Only after these stages have succeeded do you have memory recall. </p>\n<p>And now you know one of the major reasons we forget: Something has interrupted the retrieval and recall process. </p>\n<p>The good news? </p>\n<p>We <b><i>can</i></b> improve. </p>\n<p>We just need to know more about <a href=\"https://www.magneticmemorymethod.com/how-memory-works/\">how our memory works</a>, so let’s dive in.</p>\n<h2><b>The 3 Forms of Memory Retrieval</b></h2>\n<p>The challenge of learning about recall is that it’s not just one thing. Memory retrieval happens in several different ways that are related to other <a href=\"https://www.magneticmemorymethod.com/stages-of-memory/\">stages of memory</a>, but are ultimately different.</p>\n<p>These forms are:</p>\n<ul>\n<li>Free recall</li>\n<li>Cued recall</li>\n<li>Serial recall</li>\n</ul>\n<p><b>Free recall</b> allows you to retrieve information in any order. For example, if you watch a movie, you can tell your friend about the middle first, then the end before talking about the beginning. You’ve remembered enough to narrow in on different elements without getting confused or <a href=\"https://www.magneticmemorymethod.com/how-to-stop-forgetting-things/\">forgetting</a>.</p>\n<p><b>Cued recall</b>, on the other hand, involves triggers. Here’s an easy way to think of this kind of retrieval:<br />\n</p>\n<div><p></p><p>Certain forms or recall can be compared to a chain or set of linking structures.</p></div>\n<p>When memory athletes <a href=\"https://www.magneticmemorymethod.com/how-many-words-to-be-fluent/\">memorize lists of words</a>, they combine <a href=\"https://www.magneticmemorymethod.com/elaborative-encoding-memory-exercises/\">elaborated associations</a> so that one mnemonic triggers the target information in a chain. </p>\n<p>When I did my memory drills this morning, for example, three of the words were blizzard, tea and piano. Tea and piano were easy to recall because I saw Ozzy Osbourne in a blizzard pouring warm tea on his piano. One image “cued” the recall of the others.</p>\n<p><strong>Note:</strong> Cued recall itself helped me associate Ozzy with the word “blizzard” because he recorded a 1981 album called <i>Blizzard of Oz</i>. When you create all your associations based on links that already exist in your memory, you can remember things a lot faster.</p>\n<p>This kind of recall happens in everyday life too, even if you’re not a memory athlete. </p>\n<p>For example, if you’re trying to remember the name of an actor, thinking about movies he’s been in can help trigger the name. </p>\n<p><b>Serial recall</b> involves recalling information in temporal order. It relates to <a href=\"https://www.magneticmemorymethod.com/episodic-memory/\">episodic memory</a> in some circumstances. Think of it as when you’re relating a series of events and talking in a “and then this happened” kind of pattern. </p>\n<p>Does your mind really use just one of these kinds of recall at a time? Sometimes yes, sometimes no.</p>\n<div><p></p><p>Recall is definitely not like replaying a movie. It’s more like assembling actors on a stage.</p></div>\n<p>For example, you might have started giving details from the beginning of a story using free recall but realize your friend isn’t following the plot. </p>\n<p>Your <em><strong>serial recall</strong></em> kicks in and that’s when you say, “let me start at the beginning.” </p>\n<p>As you relate the plot of the movie, <strong><em>cued recall</em></strong> will likely help you add in nuance and detail that you would have forgotten otherwise. </p>\n<h2><b>What Is Recall Memory?</b></h2>\n<p>One thing that defines memory above all is that it is always dealing with the past. Think about it:</p>\n<p>Let’s say that you imagine a future scenario, like taking a trip. </p>\n<p>Later, when you want to plan the trip, you are recalling that vision of the future up from the past. </p>\n<p>So whether you’re <a href=\"https://www.magneticmemorymethod.com/how-to-become-fluent-in-a-language/\">speaking a language to become fluent</a> or sitting for an exam, improving memory recall involves improving how you access the past.</p>\n<h3><strong>Recall vs Recognition</strong></h3>\n<p>Of course, you will have differences depending on the circumstances. For example, you might struggle in an exam because you cannot recognize several key terms.</p>\n<p></p>\n<p>Or, you could have an advantage because your recognition of some terms help trigger cued recall.</p>\n<p>Complex stuff, isn’t it? Well, the devil is always in the details, which is why it pays to learn <a href=\"https://www.magneticmemorymethod.com/how-to-study-effectively/\">how to study effectively</a>. You don’t have to memorize everything in order for recognition to help you recall the information you need. </p>\n<h2><b>The 4 Types of Memory Recall</b></h2>\n<p>The plot thickens!<br />\n</p>\n<p>The four kinds of memory recall are useful to know about because the brain encodes information in very specific ways.</p>\n<p>Did you know that your emotional state dictates how you encode memories? This is just one reason why certain <a href=\"https://www.magneticmemorymethod.com/types-of-memory/\">types of memory</a> training can help <a href=\"https://www.magneticmemorymethod.com/ptsd-memory-techniques-nicholas-castle/\">relieve conditions like PTSD</a>.</p>\n<p>Let’s dig a little deeper into:</p>\n<ul>\n<li>Recall</li>\n<li>Recognition</li>\n<li>Recollection</li>\n<li>Relearning</li>\n</ul>\n<div><p></p><p>It’s possible to suddenly recall things without a trigger. For example, a memory of hanging out with your friends can arise for no reason.</p></div>\n<p><b>Recall</b> happens when you remember something without a cue or trigger. </p>\n<p>For example, sometimes you just think about a friend out of the blue. The friend isn’t there, and yet somehow the brain has pulled out memories about her.</p>\n<h3><strong>The Role of Questions In Remembering</strong></h3>\n<p>Answering questions relates here as well. If you’re at an interview, you can only prepare so much. You’ll get questions you aren’t expecting, and yet you’ll find that you can remember certain facts without needing a trigger.</p>\n<p><b>Recognition</b> is when you know you’ve seen something before. You might not even remember that you’ve seen it before, but you still recognize it. Take the word “tiger,” for example. You probably don’t remember the first time you learned it, but you recognize it for what it is (a word) and the animal it refers to.</p>\n<p>The opposite of recognition is sometimes called <a href=\"https://en.wikipedia.org/wiki/Jamais_vu\">Jamais vu</a> and can be deeply disturbing. Normally, however, recognition is positive and useful.</p>\n<p><b>Recollection</b> usually involves piecing things together using logic or <a href=\"https://www.magneticmemorymethod.com/objective-reasoning/\">objective reasoning</a>. When you’re trying to recall information, you can deliberately follow a trail of information by leaping from one association to another. </p>\n<p><b>Relearning</b> is required when we forget things we’ve spent time learning. For example, you might have gone through great efforts to memorize vocabulary, but still can’t remember it. </p>\n<p>To establish this form of recall and make learning much more fun, repetition might not be the answer. You might need to try a different approach, such as the <a href=\"https://www.magneticmemorymethod.com/memory-palace-examples/\">Memory Palace technique</a>.</p>\n<p><a href=\"https://www.magneticmemorymethod.com/blog-8\"></a></p>\n<h2><b>How These Memory Processes Work Together</b></h2>\n<p>If your brain is reasonably healthy, I have very good news for you:</p>\n<p>Most of what you need for long term memory recall happens in the background. </p>\n<p>Not for all information, of course. But improving memory recall is why <a href=\"https://www.magneticmemorymethod.com/memory-training-techniques/\">memory training</a> was invented by our clever ancestors. When you want to be sure you can recall something, <a href=\"https://www.magneticmemorymethod.com/mnemonic-devices/\">mnemonic devices</a> and <a href=\"https://www.magneticmemorymethod.com/mnemonic-strategies/\">mnemonic strategies</a> are a must. </p>\n<p>But there are aspects of life that can interfere even with the best memory training efforts. These include:</p>\n<ul>\n<li>Alzheimer’s Disease and Dementia</li>\n<li>Lack of exercise</li>\n<li>Poor nutrition</li>\n<li>Dehydration</li>\n<li>Lack of sleep</li>\n</ul>\n<p>If you have concerns, it’s always a best practice to consult a qualified medical professional.</p>\n<h2><b>How to Improve Long Term Memory Recall</b></h2>\n<p>I’m glad you asked, because helping you improve your memory is my passion.</p>\n<p>First, I would suggest that you pick a technique and <a href=\"https://www.magneticmemorymethod.com/deliberate-practice/\">practice it</a> for at least 90 days. You can choose:</p>\n<ul>\n<li>The <a href=\"https:/",
        "html": "<div><div>\n<p>Podcast: <a href=\"https://traffic.libsyn.com/secure/magneticmemorymethod/Memory_Recall_and_Retrieval__The_Definitive_Guide.mp3\">Download</a></p><p>Subscribe: <a href=\"https://itunes.apple.com/us/podcast/anthony-metiviers-magnetic-memory-method-podcast/id790315802?mt=2&amp;ls=1#episodeGuid=https%3A%2F%2Fwww.magneticmemorymethod.com%2F%3Fp%3D23428\">Apple Podcasts</a> | <a href=\"https://www.google.com/podcasts?feed=aHR0cHM6Ly93d3cubWFnbmV0aWNtZW1vcnltZXRob2QuY29tL2ZlZWQvcG9kY2FzdC8\">Google Podcasts</a> | <a href=\"https://www.stitcher.com/podcast/anthony-metivier/the-magnetic-memory-method-podcast\">Stitcher</a> | <a href=\"https://www.magneticmemorymethod.com/feed/podcast/\">RSS</a></p><p>Memory recall, sometimes called retrieval, is the experience of remembering information.</p>\n<p>Here’s the catch:</p>\n<p>It can only happen after at least these other levels of memory have succeeded first:<br />\n</p>\n<ul>\n<li>Noticing and recognizing the importance of information (like someone’s name)</li>\n<li>Encoding (making an effort to remember)</li>\n<li>Decoding (making an effort to recall)</li>\n</ul>\n<p>Only after these stages have succeeded do you have memory recall. </p>\n<p>And now you know one of the major reasons we forget: Something has interrupted the retrieval and recall process. </p>\n<p>The good news? </p>\n<p>We <b><i>can</i></b> improve. </p>\n<p>We just need to know more about <a href=\"https://www.magneticmemorymethod.com/how-memory-works/\">how our memory works</a>, so let’s dive in.</p>\n<h2><b>The 3 Forms of Memory Retrieval</b></h2>\n<p>The challenge of learning about recall is that it’s not just one thing. Memory retrieval happens in several different ways that are related to other <a href=\"https://www.magneticmemorymethod.com/stages-of-memory/\">stages of memory</a>, but are ultimately different.</p>\n<p>These forms are:</p>\n<ul>\n<li>Free recall</li>\n<li>Cued recall</li>\n<li>Serial recall</li>\n</ul>\n<p><b>Free recall</b> allows you to retrieve information in any order. For example, if you watch a movie, you can tell your friend about the middle first, then the end before talking about the beginning. You’ve remembered enough to narrow in on different elements without getting confused or <a href=\"https://www.magneticmemorymethod.com/how-to-stop-forgetting-things/\">forgetting</a>.</p>\n<p><b>Cued recall</b>, on the other hand, involves triggers. Here’s an easy way to think of this kind of retrieval:<br />\n</p>\n<div><p></p><p>Certain forms or recall can be compared to a chain or set of linking structures.</p></div>\n<p>When memory athletes <a href=\"https://www.magneticmemorymethod.com/how-many-words-to-be-fluent/\">memorize lists of words</a>, they combine <a href=\"https://www.magneticmemorymethod.com/elaborative-encoding-memory-exercises/\">elaborated associations</a> so that one mnemonic triggers the target information in a chain. </p>\n<p>When I did my memory drills this morning, for example, three of the words were blizzard, tea and piano. Tea and piano were easy to recall because I saw Ozzy Osbourne in a blizzard pouring warm tea on his piano. One image “cued” the recall of the others.</p>\n<p><strong>Note:</strong> Cued recall itself helped me associate Ozzy with the word “blizzard” because he recorded a 1981 album called <i>Blizzard of Oz</i>. When you create all your associations based on links that already exist in your memory, you can remember things a lot faster.</p>\n<p>This kind of recall happens in everyday life too, even if you’re not a memory athlete. </p>\n<p>For example, if you’re trying to remember the name of an actor, thinking about movies he’s been in can help trigger the name. </p>\n<p><b>Serial recall</b> involves recalling information in temporal order. It relates to <a href=\"https://www.magneticmemorymethod.com/episodic-memory/\">episodic memory</a> in some circumstances. Think of it as when you’re relating a series of events and talking in a “and then this happened” kind of pattern. </p>\n<p>Does your mind really use just one of these kinds of recall at a time? Sometimes yes, sometimes no.</p>\n<div><p></p><p>Recall is definitely not like replaying a movie. It’s more like assembling actors on a stage.</p></div>\n<p>For example, you might have started giving details from the beginning of a story using free recall but realize your friend isn’t following the plot. </p>\n<p>Your <em><strong>serial recall</strong></em> kicks in and that’s when you say, “let me start at the beginning.” </p>\n<p>As you relate the plot of the movie, <strong><em>cued recall</em></strong> will likely help you add in nuance and detail that you would have forgotten otherwise. </p>\n<h2><b>What Is Recall Memory?</b></h2>\n<p>One thing that defines memory above all is that it is always dealing with the past. Think about it:</p>\n<p>Let’s say that you imagine a future scenario, like taking a trip. </p>\n<p>Later, when you want to plan the trip, you are recalling that vision of the future up from the past. </p>\n<p>So whether you’re <a href=\"https://www.magneticmemorymethod.com/how-to-become-fluent-in-a-language/\">speaking a language to become fluent</a> or sitting for an exam, improving memory recall involves improving how you access the past.</p>\n<h3><strong>Recall vs Recognition</strong></h3>\n<p>Of course, you will have differences depending on the circumstances. For example, you might struggle in an exam because you cannot recognize several key terms.</p>\n<p></p>\n<p>Or, you could have an advantage because your recognition of some terms help trigger cued recall.</p>\n<p>Complex stuff, isn’t it? Well, the devil is always in the details, which is why it pays to learn <a href=\"https://www.magneticmemorymethod.com/how-to-study-effectively/\">how to study effectively</a>. You don’t have to memorize everything in order for recognition to help you recall the information you need. </p>\n<h2><b>The 4 Types of Memory Recall</b></h2>\n<p>The plot thickens!<br />\n</p>\n<p>The four kinds of memory recall are useful to know about because the brain encodes information in very specific ways.</p>\n<p>Did you know that your emotional state dictates how you encode memories? This is just one reason why certain <a href=\"https://www.magneticmemorymethod.com/types-of-memory/\">types of memory</a> training can help <a href=\"https://www.magneticmemorymethod.com/ptsd-memory-techniques-nicholas-castle/\">relieve conditions like PTSD</a>.</p>\n<p>Let’s dig a little deeper into:</p>\n<ul>\n<li>Recall</li>\n<li>Recognition</li>\n<li>Recollection</li>\n<li>Relearning</li>\n</ul>\n<div><p></p><p>It’s possible to suddenly recall things without a trigger. For example, a memory of hanging out with your friends can arise for no reason.</p></div>\n<p><b>Recall</b> happens when you remember something without a cue or trigger. </p>\n<p>For example, sometimes you just think about a friend out of the blue. The friend isn’t there, and yet somehow the brain has pulled out memories about her.</p>\n<h3><strong>The Role of Questions In Remembering</strong></h3>\n<p>Answering questions relates here as well. If you’re at an interview, you can only prepare so much. You’ll get questions you aren’t expecting, and yet you’ll find that you can remember certain facts without needing a trigger.</p>\n<p><b>Recognition</b> is when you know you’ve seen something before. You might not even remember that you’ve seen it before, but you still recognize it. Take the word “tiger,” for example. You probably don’t remember the first time you learned it, but you recognize it for what it is (a word) and the animal it refers to.</p>\n<p>The opposite of recognition is sometimes called <a href=\"https://en.wikipedia.org/wiki/Jamais_vu\">Jamais vu</a> and can be deeply disturbing. Normally, however, recognition is positive and useful.</p>\n<p><b>Recollection</b> usually involves piecing things together using logic or <a href=\"https://www.magneticmemorymethod.com/objective-reasoning/\">objective reasoning</a>. When you’re trying to recall information, you can deliberately follow a trail of information by leaping from one association to another. </p>\n<p><b>Relearning</b> is required when we forget things we’ve spent time learning. For example, you might have gone through great efforts to memorize vocabulary, but still can’t remember it. </p>\n<p>To establish this form of recall and make learning much more fun, repetition might not be the answer. You might need to try a different approach, such as the <a href=\"https://www.magneticmemorymethod.com/memory-palace-examples/\">Memory Palace technique</a>.</p>\n<p><a href=\"https://www.magneticmemorymethod.com/blog-8\"></a></p>\n<h2><b>How These Memory Processes Work Together</b></h2>\n<p>If your brain is reasonably healthy, I have very good news for you:</p>\n<p>Most of what you need for long term memory recall happens in the background. </p>\n<p>Not for all information, of course. But improving memory recall is why <a href=\"https://www.magneticmemorymethod.com/memory-training-techniques/\">memory training</a> was invented by our clever ancestors. When you want to be sure you can recall something, <a href=\"https://www.magneticmemorymethod.com/mnemonic-devices/\">mnemonic devices</a> and <a href=\"https://www.magneticmemorymethod.com/mnemonic-strategies/\">mnemonic strategies</a> are a must. </p>\n<p>But there are aspects of life that can interfere even with the best memory training efforts. These include:</p>\n<ul>\n<li>Alzheimer’s Disease and Dementia</li>\n<li>Lack of exercise</li>\n<li>Poor nutrition</li>\n<li>Dehydration</li>\n<li>Lack of sleep</li>\n</ul>\n<p>If you have concerns, it’s always a best practice to consult a qualified medical professional.</p>\n<h2><b>How to Improve Long Term Memory Recall</b></h2>\n<p>I’m glad you asked, because helping you improve your memory is my passion.</p>\n<p>First, I would suggest that you pick a technique and <a href=\"https://www.magneticmemorymethod.com/deliberate-practice/\">practice it</a> for at least 90 days. You can choose:</p>\n<ul>\n<li>The <a href=\"https:/",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Podcast:DownloadSubscribe:Apple Podcasts|Google Podcasts|Stitcher|RSSMemory recall, sometimes called retrieval, is the experience of remembering information.Here’s the catch:It can only happen after at least these other levels of memory have succeeded first:Noticing and recognizing the importance of information (like someone’s name)Encoding (making an effort to remember)Decoding (making an effort to recall)Only after these stages have succeeded do you have memory recall.And now you know one of t",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Podcast:DownloadSubscribe:Apple Podcasts|Google Podcasts|Stitcher|RSSMemory recall, sometimes called retrieval, is the experience of remembering information.Here’s the catch:It can only happen after at least these other levels of memory have succeeded first:Noticing and recognizing the importance of information (like someone’s name)Encoding (making an effort to remember)Decoding (making an effort to recall)Only after these stages have succeeded do you have memory recall.And now you know one of t",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Certain forms or recall can be compared to a chain or set of linking structures.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Recall is definitely not like replaying a movie. It’s more like assembling actors on a stage.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "It’s possible to suddenly recall things without a trigger. For example, a memory of hanging out with your friends can arise for no reason.",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "The 3 Forms of Memory Retrieval",
              "id": ""
            },
            {
              "level": "h2",
              "text": "What Is Recall Memory?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Recall vs Recognition",
              "id": ""
            },
            {
              "level": "h2",
              "text": "The 4 Types of Memory Recall",
              "id": ""
            },
            {
              "level": "h3",
              "text": "The Role of Questions In Remembering",
              "id": ""
            },
            {
              "level": "h2",
              "text": "How These Memory Processes Work Together",
              "id": ""
            },
            {
              "level": "h2",
              "text": "How to Improve Long Term Memory Recall",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10410470/",
      "title": "Cognitive neuroscience perspective on memory: overview and summary",
      "author": "",
      "published_date": "2023-07-26T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>This paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, including the importance of sleep in the consolidation of memory and the role of the hippocampus in linking new memories to existing cognitive schemata. The paper highlights two types of memory consolidation processes: cellular consolidation and system consolidation. Cellular consolidation is the process of stabilizing information by strengthening synaptic connections. System consolidation models suggest that memories are initially stored in the hippocampus and are gradually consolidated into the neocortex over time. The consolidation process involves a hippocampal-neocortical binding process incorporating newly acquired information into existing cognitive schemata. The paper highlights the role of the medial temporal lobe and its involvement in autobiographical memory. Further, the paper discusses the relationship between episodic and semantic memory and the role of the hippocampus. Finally, the paper underscores the need for further research into the neurobiological mechanisms underlying non-declarative memory, particularly conditioning. Overall, the paper provides a comprehensive overview from a cognitive neuroscience perspective of the different processes involved in memory consolidation of different types of memory.</p>\n<section><p><strong>Keywords:</strong> memory, cellular consolidation, cognitive neuroscience, hippocampus, sleep</p></section></section><section><h2>Introduction</h2>\n<p>Memory is an essential cognitive function that permits individuals to acquire, retain, and recover data that defines a person’s identity (<a href=\"#B207\">Zlotnik and Vansintjan, 2019</a>). Memory is a multifaceted cognitive process that involves different stages: encoding, consolidation, recovery, and reconsolidation. Encoding involves acquiring and processing information that is transformed into a neuronal representation suitable for storage (<a href=\"#B101\">Liu et al., 2021</a>; <a href=\"#B128\">Panzeri et al., 2023</a>). The information can be acquired through various channels, such as visual, auditory, olfactory, or tactile inputs. The acquired sensory stimuli are converted into a format the brain can process and retain. Different factors such as attention, emotional significance, and repetition can influence the encoding process and determine the strength and durability of the resulting memory (<a href=\"#B174\">Squire et al., 2004</a>; <a href=\"#B97\">Lee et al., 2016</a>; <a href=\"#B163\">Serences, 2016</a>).</p>\n<p>Consolidation includes the stabilization and integration of memory into long-term storage to increase resistance to interference and decay (<a href=\"#B66\">Goedert and Willingham, 2002</a>). This process creates enduring structural modification in the brain and thereby has consequential effects on the function by reorganizing and strengthening neural connections. Diverse sources like sleep and stress and the release of neurotransmitters can influence memory consolidation. Many researchers have noted the importance of sleep due to its critical role in enabling a smooth transition of information from transient repositories into more stable engrams (memory traces) (<a href=\"#B112\">McGaugh, 2000</a>; <a href=\"#B35\">Clawson et al., 2021</a>; <a href=\"#B140\">Rakowska et al., 2022</a>).</p>\n<p>Retrieval involves accessing, selecting, and reactivating or reconstructing the stored memory to allow conscious access to previously encoded information (<a href=\"#B47\">Dudai, 2002</a>). Retrieving memories depends on activating relevant neural pathways while reconstructing encoded information. Factors like contextual or retrieval cues and familiarity with the material can affect this process. Forgetting becomes a possibility if there are inadequate triggers for associated memory traces to activate upon recall. Luckily, mnemonic strategies and retrieval practice offer effective tools to enhance recovery rates and benefit overall memory performance (<a href=\"#B154\">Roediger and Butler, 2011</a>).</p>\n<p>Previous research implied that once a memory has been consolidated, it becomes permanent (<a href=\"#B112\">McGaugh, 2000</a>; <a href=\"#B153\">Robins, 2020</a>). However, recent studies have found an additional phase called “reconsolidation,” during which stored memories, when reactivated, enter a fragile or liable state and become susceptible to modification or update (<a href=\"#B159\">Schiller et al., 2009</a>; <a href=\"#B2\">Asthana et al., 2015</a>). The process highlights the notion that memory is not static but a dynamic system influenced by subsequent encounters. The concept of reconsolidation has much significance in memory modification therapies and interventions, as it offers a promising opportunity to target maladaptive or traumatic memories for modification specifically. However, more thorough investigations are needed to gain insight into the mechanisms and concrete implications of employing memory reconsolidation within therapeutic settings (<a href=\"#B16\">Bellfy and Kwapis, 2020</a>).</p>\n<p>The concept of memory is not reducible to a single unitary phenomenon; instead, evidence suggests that it can be subdivided into several distinct but interrelated constituent processes and systems (<a href=\"#B150\">Richter-Levin and Akirav, 2003</a>). There are three major types of human memory: working memory, declarative memory (explicit), and non-declarative memory (implicit). All these types of memories involve different neural systems in the brain. Working memory is a unique transient active store capable of manipulating information essential for many complex cognitive operations, including language processing, reasoning, and judgment (<a href=\"#B3\">Atkinson and Shiffrin, 1968</a>; <a href=\"#B11\">Baddeley and Logie, 1999</a>; <a href=\"#B57\">Funahashi, 2017</a>; <a href=\"#B139\">Quentin et al., 2019</a>). Previous models suggest the existence of three components that make up the working memory (<a href=\"#B10\">Baddeley and Hitch, 1974</a>; <a href=\"#B8\">Baddeley, 1986</a>). One master component, the central executive, controls the two dependent components, the phonological loop (speech perception and language comprehension) and the visuospatial sketchpad (visual images and spatial impressions processing). Some models mention a third component known as the episodic buffer. It is theorized that the episodic buffer serves as an intermediary between perception, long-term memory, and two components of working memory (the phonological loop and visuospatial sketchpad) by storing integrated episodes or chunks from both sources (<a href=\"#B6\">Baddeley, 2000</a>). Declarative memory (explicit memory) can be recalled consciously, including facts and events that took place in one’s life or information learned from books. It encompasses memories of both autobiographical experiences and memories associated with general knowledge. It is usually associated with the hippocampus–medial temporal lobe system (<a href=\"#B182\">Thompson and Kim, 1996</a>; <a href=\"#B124\">Ober, 2014</a>). Non-declarative memory (implicit memory) refers to unconscious forms of learning such as skills, habits, and priming effects; this type of implicit learning does not involve conscious recollection but can include motor skill tasks that often require no thought prior to execution nor later recall upon completion. This type of memory usually involves the amygdala and other systems (<a href=\"#B182\">Thompson and Kim, 1996</a>; <a href=\"#B124\">Ober, 2014</a>).</p></section><section><h2>Working memory</h2>\n<p>Working memory is primarily associated with the prefrontal and posterior parietal cortex (<a href=\"#B158\">Sarnthein et al., 1998</a>; <a href=\"#B184\">Todd and Marois, 2005</a>). Working memory is not localized to a single brain region, and research suggests that it is an emergent property arising from functional interactions between the prefrontal cortex (PFC) and the rest of the brain (<a href=\"#B39\">D’Esposito, 2007</a>). Neuroimaging studies have explored the neural basis for the three components proposed by <a href=\"#B10\">Baddeley and Hitch (1974)</a>, the Central executive, the phonological loop, and the visuospatial sketch pad; there is evidence for the existence of a fourth component called the episodic buffer (<a href=\"#B6\">Baddeley, 2000</a>).</p>\n<p>The central executive plays a significant role in working memory by acting as the control center (<a href=\"#B165\">Shallice, 2002</a>). It facilitates critical functions like attention allocation and coordination between the phonological loop and the visuospatial sketchpad (<a href=\"#B202\">Yu et al., 2023</a>). Recent findings have illuminated the dual-functional network regulation, the cingulo-opercular network (CON) and the frontoparietal network (FPN), that underpins the central executive system (<a href=\"#B202\">Yu et al., 2023</a>). The CON comprises the dorsal anterior cingulate cortex (dACC) and anterior insula (AI). In contrast, the FPN encompasses various regions, such as the dorsolateral prefrontal cortex (DLPFC) and frontal eye field (FEF), along with the intraparietal sulcus (IPS) (<a href=\"#B202\">Yu et al., 2023</a>). Neuroimaging research has found evidence that elucidates the neural underpinnings of the executive attention control system to the dorsolateral prefrontal cortex (DLPFC) and the anterior cingulate cortex (ACC) (<a href=\"#B81\">Jung et al., 2022</a>). The activation patterns indicate that the CON may have a broader top-down cont",
        "html": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>This paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, including the importance of sleep in the consolidation of memory and the role of the hippocampus in linking new memories to existing cognitive schemata. The paper highlights two types of memory consolidation processes: cellular consolidation and system consolidation. Cellular consolidation is the process of stabilizing information by strengthening synaptic connections. System consolidation models suggest that memories are initially stored in the hippocampus and are gradually consolidated into the neocortex over time. The consolidation process involves a hippocampal-neocortical binding process incorporating newly acquired information into existing cognitive schemata. The paper highlights the role of the medial temporal lobe and its involvement in autobiographical memory. Further, the paper discusses the relationship between episodic and semantic memory and the role of the hippocampus. Finally, the paper underscores the need for further research into the neurobiological mechanisms underlying non-declarative memory, particularly conditioning. Overall, the paper provides a comprehensive overview from a cognitive neuroscience perspective of the different processes involved in memory consolidation of different types of memory.</p>\n<section><p><strong>Keywords:</strong> memory, cellular consolidation, cognitive neuroscience, hippocampus, sleep</p></section></section><section><h2>Introduction</h2>\n<p>Memory is an essential cognitive function that permits individuals to acquire, retain, and recover data that defines a person’s identity (<a href=\"#B207\">Zlotnik and Vansintjan, 2019</a>). Memory is a multifaceted cognitive process that involves different stages: encoding, consolidation, recovery, and reconsolidation. Encoding involves acquiring and processing information that is transformed into a neuronal representation suitable for storage (<a href=\"#B101\">Liu et al., 2021</a>; <a href=\"#B128\">Panzeri et al., 2023</a>). The information can be acquired through various channels, such as visual, auditory, olfactory, or tactile inputs. The acquired sensory stimuli are converted into a format the brain can process and retain. Different factors such as attention, emotional significance, and repetition can influence the encoding process and determine the strength and durability of the resulting memory (<a href=\"#B174\">Squire et al., 2004</a>; <a href=\"#B97\">Lee et al., 2016</a>; <a href=\"#B163\">Serences, 2016</a>).</p>\n<p>Consolidation includes the stabilization and integration of memory into long-term storage to increase resistance to interference and decay (<a href=\"#B66\">Goedert and Willingham, 2002</a>). This process creates enduring structural modification in the brain and thereby has consequential effects on the function by reorganizing and strengthening neural connections. Diverse sources like sleep and stress and the release of neurotransmitters can influence memory consolidation. Many researchers have noted the importance of sleep due to its critical role in enabling a smooth transition of information from transient repositories into more stable engrams (memory traces) (<a href=\"#B112\">McGaugh, 2000</a>; <a href=\"#B35\">Clawson et al., 2021</a>; <a href=\"#B140\">Rakowska et al., 2022</a>).</p>\n<p>Retrieval involves accessing, selecting, and reactivating or reconstructing the stored memory to allow conscious access to previously encoded information (<a href=\"#B47\">Dudai, 2002</a>). Retrieving memories depends on activating relevant neural pathways while reconstructing encoded information. Factors like contextual or retrieval cues and familiarity with the material can affect this process. Forgetting becomes a possibility if there are inadequate triggers for associated memory traces to activate upon recall. Luckily, mnemonic strategies and retrieval practice offer effective tools to enhance recovery rates and benefit overall memory performance (<a href=\"#B154\">Roediger and Butler, 2011</a>).</p>\n<p>Previous research implied that once a memory has been consolidated, it becomes permanent (<a href=\"#B112\">McGaugh, 2000</a>; <a href=\"#B153\">Robins, 2020</a>). However, recent studies have found an additional phase called “reconsolidation,” during which stored memories, when reactivated, enter a fragile or liable state and become susceptible to modification or update (<a href=\"#B159\">Schiller et al., 2009</a>; <a href=\"#B2\">Asthana et al., 2015</a>). The process highlights the notion that memory is not static but a dynamic system influenced by subsequent encounters. The concept of reconsolidation has much significance in memory modification therapies and interventions, as it offers a promising opportunity to target maladaptive or traumatic memories for modification specifically. However, more thorough investigations are needed to gain insight into the mechanisms and concrete implications of employing memory reconsolidation within therapeutic settings (<a href=\"#B16\">Bellfy and Kwapis, 2020</a>).</p>\n<p>The concept of memory is not reducible to a single unitary phenomenon; instead, evidence suggests that it can be subdivided into several distinct but interrelated constituent processes and systems (<a href=\"#B150\">Richter-Levin and Akirav, 2003</a>). There are three major types of human memory: working memory, declarative memory (explicit), and non-declarative memory (implicit). All these types of memories involve different neural systems in the brain. Working memory is a unique transient active store capable of manipulating information essential for many complex cognitive operations, including language processing, reasoning, and judgment (<a href=\"#B3\">Atkinson and Shiffrin, 1968</a>; <a href=\"#B11\">Baddeley and Logie, 1999</a>; <a href=\"#B57\">Funahashi, 2017</a>; <a href=\"#B139\">Quentin et al., 2019</a>). Previous models suggest the existence of three components that make up the working memory (<a href=\"#B10\">Baddeley and Hitch, 1974</a>; <a href=\"#B8\">Baddeley, 1986</a>). One master component, the central executive, controls the two dependent components, the phonological loop (speech perception and language comprehension) and the visuospatial sketchpad (visual images and spatial impressions processing). Some models mention a third component known as the episodic buffer. It is theorized that the episodic buffer serves as an intermediary between perception, long-term memory, and two components of working memory (the phonological loop and visuospatial sketchpad) by storing integrated episodes or chunks from both sources (<a href=\"#B6\">Baddeley, 2000</a>). Declarative memory (explicit memory) can be recalled consciously, including facts and events that took place in one’s life or information learned from books. It encompasses memories of both autobiographical experiences and memories associated with general knowledge. It is usually associated with the hippocampus–medial temporal lobe system (<a href=\"#B182\">Thompson and Kim, 1996</a>; <a href=\"#B124\">Ober, 2014</a>). Non-declarative memory (implicit memory) refers to unconscious forms of learning such as skills, habits, and priming effects; this type of implicit learning does not involve conscious recollection but can include motor skill tasks that often require no thought prior to execution nor later recall upon completion. This type of memory usually involves the amygdala and other systems (<a href=\"#B182\">Thompson and Kim, 1996</a>; <a href=\"#B124\">Ober, 2014</a>).</p></section><section><h2>Working memory</h2>\n<p>Working memory is primarily associated with the prefrontal and posterior parietal cortex (<a href=\"#B158\">Sarnthein et al., 1998</a>; <a href=\"#B184\">Todd and Marois, 2005</a>). Working memory is not localized to a single brain region, and research suggests that it is an emergent property arising from functional interactions between the prefrontal cortex (PFC) and the rest of the brain (<a href=\"#B39\">D’Esposito, 2007</a>). Neuroimaging studies have explored the neural basis for the three components proposed by <a href=\"#B10\">Baddeley and Hitch (1974)</a>, the Central executive, the phonological loop, and the visuospatial sketch pad; there is evidence for the existence of a fourth component called the episodic buffer (<a href=\"#B6\">Baddeley, 2000</a>).</p>\n<p>The central executive plays a significant role in working memory by acting as the control center (<a href=\"#B165\">Shallice, 2002</a>). It facilitates critical functions like attention allocation and coordination between the phonological loop and the visuospatial sketchpad (<a href=\"#B202\">Yu et al., 2023</a>). Recent findings have illuminated the dual-functional network regulation, the cingulo-opercular network (CON) and the frontoparietal network (FPN), that underpins the central executive system (<a href=\"#B202\">Yu et al., 2023</a>). The CON comprises the dorsal anterior cingulate cortex (dACC) and anterior insula (AI). In contrast, the FPN encompasses various regions, such as the dorsolateral prefrontal cortex (DLPFC) and frontal eye field (FEF), along with the intraparietal sulcus (IPS) (<a href=\"#B202\">Yu et al., 2023</a>). Neuroimaging research has found evidence that elucidates the neural underpinnings of the executive attention control system to the dorsolateral prefrontal cortex (DLPFC) and the anterior cingulate cortex (ACC) (<a href=\"#B81\">Jung et al., 2022</a>). The activation patterns indicate that the CON may have a broader top-down cont",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "AbstractThis paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, incl",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractThis paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, incl",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "AbstractThis paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, incl",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractThis paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, incl",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractThis paper explores memory from a cognitive neuroscience perspective and examines associated neural mechanisms. It examines the different types of memory: working, declarative, and non-declarative, and the brain regions involved in each type. The paper highlights the role of different brain regions, such as the prefrontal cortex in working memory and the hippocampus in declarative memory. The paper also examines the mechanisms that underlie the formation and consolidation of memory, incl",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Keywords:memory, cellular consolidation, cognitive neuroscience, hippocampus, sleep",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionMemory is an essential cognitive function that permits individuals to acquire, retain, and recover data that defines a person’s identity (Zlotnik and Vansintjan, 2019). Memory is a multifaceted cognitive process that involves different stages: encoding, consolidation, recovery, and reconsolidation. Encoding involves acquiring and processing information that is transformed into a neuronal representation suitable for storage (Liu et al., 2021;Panzeri et al., 2023). The information can ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Working memoryWorking memory is primarily associated with the prefrontal and posterior parietal cortex (Sarnthein et al., 1998;Todd and Marois, 2005). Working memory is not localized to a single brain region, and research suggests that it is an emergent property arising from functional interactions between the prefrontal cortex (PFC) and the rest of the brain (D’Esposito, 2007). Neuroimaging studies have explored the neural basis for the three components proposed byBaddeley and Hitch (1974), the",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Working memory",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://psu.pb.unizin.org/introductorypsychologywede/chapter/chapter-5-memory/",
      "title": "Chapter 5 – Memory – Introductory Psychology",
      "author": "",
      "published_date": "2019-03-01T00:00:00.000Z",
      "content": {
        "text": "<div><section>\n<header>\n</header>\n<div>\n<h2>Introduction</h2>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.1</span><span> </span></strong><span>Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –<a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY 4.0</a></span></figcaption></figure>\n</figure>\n</div>\n<p>We may be top-notch learners, but if we don’t have a way to store what we’ve learned, what good is the knowledge we’ve gained?</p>\n<p>Take a few minutes to imagine what your day might be like if you could not remember anything you had learned. You would have to figure out how to get dressed. What clothing should you wear, and how do buttons and zippers work? You would need someone to teach you how to brush your teeth and tie your shoes. Who would you ask for help with these tasks, since you wouldn’t recognize the faces of these people in your house? Wait . . . is this even your house? Uh oh, your stomach begins to rumble and you feel hungry. You’d like something to eat, but you don’t know where the food is kept or even how to prepare it. Oh dear, this is getting confusing. Maybe it would be best to just go back to bed. A bed . . . what is a bed?</p>\n<p>We have an amazing capacity for memory, but how, exactly, do we process and store information? Are there different kinds of memory, and if so, what characterizes the different types? How, exactly, do we retrieve our memories? And why do we forget? This chapter will explore these questions as we learn about memory.</p>\n</div>\n<div>\n<div>\n<p>\n</p><h2>5.1 How Memory Functions</h2>\n<p></p>\n<div>\n<div>\n<h2>Learning Objectives</h2>\n<section>By the end of this section, you will be able to:\n<ul>\n<li>Discuss the three basic functions of memory</li>\n<li>Describe the three stages of memory storage</li>\n<li>Describe and distinguish between procedural and declarative memory and semantic and episodic memory</li>\n</ul>\n</section>\n</div>\n<p>Memory is an information processing system; therefore, we often compare it to a computer. <span>Memory</span> is the set of processes used to encode, store, and retrieve information over different periods of time (<a href=\"https://cnx.org/contents/Sr8Ev5Og@10.16:-RwqQWzt@17/8-1-How-Memory-Functions#Figure_08_01_Memory\">Figure 5.2</a>).</p>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.2 </span></strong><span>Encoding involves the input of information into the memory system. Storage is the retention of encoded information. Retrieval, or getting the information out of memory and back into awareness, is the third function. Source: <a href=\"https://courses.lumenlearning.com/wsu-sandbox/chapter/how-memory-functions/\">Lumen Learning</a> –<a href=\"https://creativecommons.org/licenses/by/4.0/\">CC BY 4.0</a></span></figcaption></figure>\n</figure>\n</div>\n<div>\n<header>\n<h3><span>LINK TO LEARNING</span></h3>\n</header>\n<section>\n<p>Watch <a href=\"https://www.youtube.com/watch?v=sI_ceF5-OiQ\">this video</a> for more information on some unexpected facts about memory.</p>\n</section>\n</div>\n<section>\n<h3>Encoding</h3>\n<p>We get information into our brains through a process called <span>encoding</span>, which is the input of information into the memory system. Once we receive sensory information from the environment, our brains label or code it. We organize the information with other similar information and connect new concepts to existing concepts. Encoding information occurs through automatic processing and effortful processing.</p>\n<p>If someone asks you what you ate for lunch today, more than likely you could recall this information quite easily. This is known as <span>automatic processing</span>, or the encoding of details like time, space, frequency, and the meaning of words. Automatic processing is usually done without any conscious awareness. Recalling the last time you studied for a test is another example of automatic processing. But what about the actual test material you studied? It probably required a lot of work and attention on your part in order to encode that information. This is known as <span>effortful processing</span> (<a href=\"https://cnx.org/contents/Sr8Ev5Og@10.16:-RwqQWzt@17/8-1-How-Memory-Functions#Figure_08_01_Driving\">Figure 5.3</a>).</p>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.3 </span></strong><span>When you first learn new skills such as driving a car, you have to put forth effort and attention to encoding information about how to start a car, how to brake, how to handle a turn, and so on. Once you know how to drive, you can encode additional information about this skill automatically. Source: <a href=\"https://www.flickr.com/photos/29233640@N07/8197054060/\">Robert Couse-Baker</a> “a good drive was had by all” on Flickr –<a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0</a></span></figcaption></figure>\n</figure>\n</div>\n<p>What are the most effective ways to ensure that important memories are well encoded? Even a simple sentence is easier to recall when it is meaningful (Anderson, 1984). Read the following sentences (Bransford &amp; McCarrell, 1974), then look away and count backward from 30 by threes to zero, and then try to write down the sentences (no peeking back at this page!).</p>\n<ol>\n<li>The notes were sour because the seams split.</li>\n<li>The voyage wasn’t delayed because the bottle shattered.</li>\n<li>The haystack was important because the cloth ripped.</li>\n</ol>\n<p>How well did you do? By themselves, the statements that you wrote down were most likely confusing and difficult for you to recall. Now, try writing them again, using the following prompts: bagpipe, ship christening, and parachutist. Next, count backward from 40 by fours, then check yourself to see how well you recalled the sentences this time. You can see that the sentences are now much more memorable because each of the sentences was placed in context. Material is far better encoded when you make it meaningful.</p>\n<p>There are three types of encoding. The encoding of words and their meaning is known as <span>semantic encoding</span>. It was first demonstrated by William Bousfield (1935) in an experiment in which he asked people to memorize words. The 60 words were actually divided into 4 categories of meaning, although the participants did not know this because the words were randomly presented. When they were asked to remember the words, they tended to recall them in categories, showing that they paid attention to the meanings of the words as they learned them.</p>\n<p><span>Visual encoding</span> is the encoding of images, and <span>acoustic encoding</span> is the encoding of sounds, words in particular. To see how visual encoding works, read over this list of words: <em>car, level, dog, truth, book, value</em>. If you were asked later to recall the words from this list, which ones do you think you’d most likely remember? You would probably have an easier time recalling the words <em>car, dog,</em>and <em>book</em>, and a more difficult time recalling the words <em>level, truth,</em> and <em>value</em>. Why is this? Because you can recall images (mental pictures) more easily than words alone. When you read the words <em>car, dog,</em> and <em>book</em> you created images of these things in your mind. These are concrete, high-imagery words. On the other hand, abstract words like <em>level, truth,</em> and <em>value</em> are low-imagery words. High-imagery words are encoded both visually and semantically (Paivio, 1986), thus building a stronger memory.</p>\n<p>Now let’s turn our attention to acoustic encoding. You are driving in your car and a song comes on the radio that you haven’t heard in at least 10 years, but you sing along, recalling every word. In the United States, children often learn the alphabet through song, and they learn the number of days in each month through rhyme: <em>“</em>Thirty days hath September, / April, June, and November; / All the rest have thirty-one, / Save February, with twenty-eight days clear, / And twenty-nine each leap year.” These lessons are easy to remember because of acoustic encoding. We encode the sounds the words make. This is one of the reasons why much of what we teach young children is done through song, rhyme, and rhythm.</p>\n<p>Which of the three types of encoding do you think would give you the best memory of verbal information? Some years ago, psychologists Fergus Craik and Endel Tulving (1975) conducted a series of experiments to find out. Participants were given words along with questions about them. The questions required the participants to process the words at one of the three levels. The visual processing questions included such things as asking the participants about the font of the letters. The acoustic processing questions asked the participants about the sound or rhyming of the words, and the semantic processing questions asked the participants about the meaning of the words. After participants were presented with the words and questions, they were given an unexpected recall or recognition task.</p>\n<p>Words that had been encoded semantically were better remembered than those encoded visually or acoustically. Semantic encoding involves a deeper level of processing than the shallower visual or acoustic encoding. Craik and Tulving concluded that we process verbal information best through semantic encoding, especially if we apply what is called the self-reference effect. The <span>self-reference effect</span> is the tendency for an individual to have better memory for information that relates to oneself in comparison to material that has less personal relevance (Rogers, Kuiper &amp; Kirker, 1977). Could semantic encoding be beneficial to you as you attempt to memorize the concepts in this chapter?</p>\n</section>\n<section>\n<h3>Storage</h3>\n<p>Once the information has been encoded, we have to somehow retain it. Our brains take t",
        "html": "<div><section>\n<header>\n</header>\n<div>\n<h2>Introduction</h2>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.1</span><span> </span></strong><span>Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –<a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY 4.0</a></span></figcaption></figure>\n</figure>\n</div>\n<p>We may be top-notch learners, but if we don’t have a way to store what we’ve learned, what good is the knowledge we’ve gained?</p>\n<p>Take a few minutes to imagine what your day might be like if you could not remember anything you had learned. You would have to figure out how to get dressed. What clothing should you wear, and how do buttons and zippers work? You would need someone to teach you how to brush your teeth and tie your shoes. Who would you ask for help with these tasks, since you wouldn’t recognize the faces of these people in your house? Wait . . . is this even your house? Uh oh, your stomach begins to rumble and you feel hungry. You’d like something to eat, but you don’t know where the food is kept or even how to prepare it. Oh dear, this is getting confusing. Maybe it would be best to just go back to bed. A bed . . . what is a bed?</p>\n<p>We have an amazing capacity for memory, but how, exactly, do we process and store information? Are there different kinds of memory, and if so, what characterizes the different types? How, exactly, do we retrieve our memories? And why do we forget? This chapter will explore these questions as we learn about memory.</p>\n</div>\n<div>\n<div>\n<p>\n</p><h2>5.1 How Memory Functions</h2>\n<p></p>\n<div>\n<div>\n<h2>Learning Objectives</h2>\n<section>By the end of this section, you will be able to:\n<ul>\n<li>Discuss the three basic functions of memory</li>\n<li>Describe the three stages of memory storage</li>\n<li>Describe and distinguish between procedural and declarative memory and semantic and episodic memory</li>\n</ul>\n</section>\n</div>\n<p>Memory is an information processing system; therefore, we often compare it to a computer. <span>Memory</span> is the set of processes used to encode, store, and retrieve information over different periods of time (<a href=\"https://cnx.org/contents/Sr8Ev5Og@10.16:-RwqQWzt@17/8-1-How-Memory-Functions#Figure_08_01_Memory\">Figure 5.2</a>).</p>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.2 </span></strong><span>Encoding involves the input of information into the memory system. Storage is the retention of encoded information. Retrieval, or getting the information out of memory and back into awareness, is the third function. Source: <a href=\"https://courses.lumenlearning.com/wsu-sandbox/chapter/how-memory-functions/\">Lumen Learning</a> –<a href=\"https://creativecommons.org/licenses/by/4.0/\">CC BY 4.0</a></span></figcaption></figure>\n</figure>\n</div>\n<div>\n<header>\n<h3><span>LINK TO LEARNING</span></h3>\n</header>\n<section>\n<p>Watch <a href=\"https://www.youtube.com/watch?v=sI_ceF5-OiQ\">this video</a> for more information on some unexpected facts about memory.</p>\n</section>\n</div>\n<section>\n<h3>Encoding</h3>\n<p>We get information into our brains through a process called <span>encoding</span>, which is the input of information into the memory system. Once we receive sensory information from the environment, our brains label or code it. We organize the information with other similar information and connect new concepts to existing concepts. Encoding information occurs through automatic processing and effortful processing.</p>\n<p>If someone asks you what you ate for lunch today, more than likely you could recall this information quite easily. This is known as <span>automatic processing</span>, or the encoding of details like time, space, frequency, and the meaning of words. Automatic processing is usually done without any conscious awareness. Recalling the last time you studied for a test is another example of automatic processing. But what about the actual test material you studied? It probably required a lot of work and attention on your part in order to encode that information. This is known as <span>effortful processing</span> (<a href=\"https://cnx.org/contents/Sr8Ev5Og@10.16:-RwqQWzt@17/8-1-How-Memory-Functions#Figure_08_01_Driving\">Figure 5.3</a>).</p>\n<div>\n<figure>\n<figure><figcaption><strong><span>Figure </span><span>5.3 </span></strong><span>When you first learn new skills such as driving a car, you have to put forth effort and attention to encoding information about how to start a car, how to brake, how to handle a turn, and so on. Once you know how to drive, you can encode additional information about this skill automatically. Source: <a href=\"https://www.flickr.com/photos/29233640@N07/8197054060/\">Robert Couse-Baker</a> “a good drive was had by all” on Flickr –<a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0</a></span></figcaption></figure>\n</figure>\n</div>\n<p>What are the most effective ways to ensure that important memories are well encoded? Even a simple sentence is easier to recall when it is meaningful (Anderson, 1984). Read the following sentences (Bransford &amp; McCarrell, 1974), then look away and count backward from 30 by threes to zero, and then try to write down the sentences (no peeking back at this page!).</p>\n<ol>\n<li>The notes were sour because the seams split.</li>\n<li>The voyage wasn’t delayed because the bottle shattered.</li>\n<li>The haystack was important because the cloth ripped.</li>\n</ol>\n<p>How well did you do? By themselves, the statements that you wrote down were most likely confusing and difficult for you to recall. Now, try writing them again, using the following prompts: bagpipe, ship christening, and parachutist. Next, count backward from 40 by fours, then check yourself to see how well you recalled the sentences this time. You can see that the sentences are now much more memorable because each of the sentences was placed in context. Material is far better encoded when you make it meaningful.</p>\n<p>There are three types of encoding. The encoding of words and their meaning is known as <span>semantic encoding</span>. It was first demonstrated by William Bousfield (1935) in an experiment in which he asked people to memorize words. The 60 words were actually divided into 4 categories of meaning, although the participants did not know this because the words were randomly presented. When they were asked to remember the words, they tended to recall them in categories, showing that they paid attention to the meanings of the words as they learned them.</p>\n<p><span>Visual encoding</span> is the encoding of images, and <span>acoustic encoding</span> is the encoding of sounds, words in particular. To see how visual encoding works, read over this list of words: <em>car, level, dog, truth, book, value</em>. If you were asked later to recall the words from this list, which ones do you think you’d most likely remember? You would probably have an easier time recalling the words <em>car, dog,</em>and <em>book</em>, and a more difficult time recalling the words <em>level, truth,</em> and <em>value</em>. Why is this? Because you can recall images (mental pictures) more easily than words alone. When you read the words <em>car, dog,</em> and <em>book</em> you created images of these things in your mind. These are concrete, high-imagery words. On the other hand, abstract words like <em>level, truth,</em> and <em>value</em> are low-imagery words. High-imagery words are encoded both visually and semantically (Paivio, 1986), thus building a stronger memory.</p>\n<p>Now let’s turn our attention to acoustic encoding. You are driving in your car and a song comes on the radio that you haven’t heard in at least 10 years, but you sing along, recalling every word. In the United States, children often learn the alphabet through song, and they learn the number of days in each month through rhyme: <em>“</em>Thirty days hath September, / April, June, and November; / All the rest have thirty-one, / Save February, with twenty-eight days clear, / And twenty-nine each leap year.” These lessons are easy to remember because of acoustic encoding. We encode the sounds the words make. This is one of the reasons why much of what we teach young children is done through song, rhyme, and rhythm.</p>\n<p>Which of the three types of encoding do you think would give you the best memory of verbal information? Some years ago, psychologists Fergus Craik and Endel Tulving (1975) conducted a series of experiments to find out. Participants were given words along with questions about them. The questions required the participants to process the words at one of the three levels. The visual processing questions included such things as asking the participants about the font of the letters. The acoustic processing questions asked the participants about the sound or rhyming of the words, and the semantic processing questions asked the participants about the meaning of the words. After participants were presented with the words and questions, they were given an unexpected recall or recognition task.</p>\n<p>Words that had been encoded semantically were better remembered than those encoded visually or acoustically. Semantic encoding involves a deeper level of processing than the shallower visual or acoustic encoding. Craik and Tulving concluded that we process verbal information best through semantic encoding, especially if we apply what is called the self-reference effect. The <span>self-reference effect</span> is the tendency for an individual to have better memory for information that relates to oneself in comparison to material that has less personal relevance (Rogers, Kuiper &amp; Kirker, 1977). Could semantic encoding be beneficial to you as you attempt to memorize the concepts in this chapter?</p>\n</section>\n<section>\n<h3>Storage</h3>\n<p>Once the information has been encoded, we have to somehow retain it. Our brains take t",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "IntroductionFigure5.1Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –CC-BY 4.0We may be top-notch learners, but if we don’t have a way to store what we’ve learned, what good is the knowledge we’ve gained?Take a few minutes to imagine what your day might be like if you could not remember anything you had learned. You would have to figure out how to get dressed. What clothing should you wear, and how do buttons and zipper",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionFigure5.1Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –CC-BY 4.0We may be top-notch learners, but if we don’t have a way to store what we’ve learned, what good is the knowledge we’ve gained?Take a few minutes to imagine what your day might be like if you could not remember anything you had learned. You would have to figure out how to get dressed. What clothing should you wear, and how do buttons and zipper",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "IntroductionFigure5.1Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –CC-BY 4.0We may be top-notch learners, but if we don’t have a way to store what we’ve learned, what good is the knowledge we’ve gained?Take a few minutes to imagine what your day might be like if you could not remember anything you had learned. You would have to figure out how to get dressed. What clothing should you wear, and how do buttons and zipper",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure5.1Photographs can trigger our memories and bring past experiences back to life. (credit: modification of work by Cory Zanker) –CC-BY 4.0",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "5.1 How Memory FunctionsLearning ObjectivesBy the end of this section, you will be able to:Discuss the three basic functions of memoryDescribe the three stages of memory storageDescribe and distinguish between procedural and declarative memory and semantic and episodic memoryMemory is an information processing system; therefore, we often compare it to a computer.Memoryis the set of processes used to encode, store, and retrieve information over different periods of time (Figure 5.2).Figure5.2Enco",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "5.1 How Memory FunctionsLearning ObjectivesBy the end of this section, you will be able to:Discuss the three basic functions of memoryDescribe the three stages of memory storageDescribe and distinguish between procedural and declarative memory and semantic and episodic memoryMemory is an information processing system; therefore, we often compare it to a computer.Memoryis the set of processes used to encode, store, and retrieve information over different periods of time (Figure 5.2).Figure5.2Enco",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Learning ObjectivesBy the end of this section, you will be able to:Discuss the three basic functions of memoryDescribe the three stages of memory storageDescribe and distinguish between procedural and declarative memory and semantic and episodic memoryMemory is an information processing system; therefore, we often compare it to a computer.Memoryis the set of processes used to encode, store, and retrieve information over different periods of time (Figure 5.2).Figure5.2Encoding involves the input ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Learning ObjectivesBy the end of this section, you will be able to:Discuss the three basic functions of memoryDescribe the three stages of memory storageDescribe and distinguish between procedural and declarative memory and semantic and episodic memory",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "By the end of this section, you will be able to:Discuss the three basic functions of memoryDescribe the three stages of memory storageDescribe and distinguish between procedural and declarative memory and semantic and episodic memory",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure5.2Encoding involves the input of information into the memory system. Storage is the retention of encoded information. Retrieval, or getting the information out of memory and back into awareness, is the third function. Source:Lumen Learning–CC BY 4.0",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "LINK TO LEARNINGWatchthis videofor more information on some unexpected facts about memory.",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Watchthis videofor more information on some unexpected facts about memory.",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "EncodingWe get information into our brains through a process calledencoding, which is the input of information into the memory system. Once we receive sensory information from the environment, our brains label or code it. We organize the information with other similar information and connect new concepts to existing concepts. Encoding information occurs through automatic processing and effortful processing.If someone asks you what you ate for lunch today, more than likely you could recall this i",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure5.3When you first learn new skills such as driving a car, you have to put forth effort and attention to encoding information about how to start a car, how to brake, how to handle a turn, and so on. Once you know how to drive, you can encode additional information about this skill automatically. Source:Robert Couse-Baker“a good drive was had by all” on Flickr –CC BY 2.0",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "StorageOnce the information has been encoded, we have to somehow retain it. Our brains take t",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "5.1 How Memory Functions",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Learning Objectives",
              "id": ""
            },
            {
              "level": "h3",
              "text": "LINK TO LEARNING",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Encoding",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Storage",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC2658622/",
      "title": "The recall of information from working memory: insights from behavioural and chronometric perspectives",
      "author": "",
      "published_date": "1996-01-26T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<main>\n<article><section><div>\n<p>. Author manuscript; available in PMC: 2009 Mar 19.</p></div></section><section><section><h2>Abstract</h2>\n<p>In four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy vary between these task formats, dependent on the orienting task for processing. Experiment 1 compares the chronometry of spoken recall for word span and reading span, in which participants complete an unfinished sentence. Experiment 2 and 3 confirm recall time differences without using word generation requirements, while Experiments 4 used an item and order response choice paradigm with nonspoken responses. We argue that verbal and manual recall timing offers an informative measure for understanding working memory.</p>\n<section><p><strong>Keywords:</strong> working memory, reading span, recall timing, recall method, short-term memory</p></section></section><section><h2>Introduction</h2>\n<p>Working memory reflects the ability to hold in mind transient representations while simultaneously processing and assimilating ongoing events (<a href=\"#R1\">Baddeley &amp; Hitch, 1974</a>). There are a wide variety of circumstances in which we are required to carry out mental operations and remember intermediate information (for instance, retain a carry item in mental arithmetic or a referent for an anaphoric pronoun) as well as situations in which current mental activities need to draw on past episodic knowledge (e.g., mapping the problem space for a current task using knowledge of related situations). They emphasise the importance of understanding active maintenance and transformation processes. Consequently, the concept of working memory has been the focus of considerable research.</p>\n<p>Although theories of working memory differ considerably (see <a href=\"#R27\">Miyake &amp; Shah, 1999</a>), the most common method for assessing its capacity is to draw upon at least one of a family of tasks known as working memory (WM) span. Reading span was the first such task to be reported in adults (<a href=\"#R13\">Daneman &amp; Carpenter, 1980</a>). Participants read a series of unconnected sentences, and the final word in each sentence yields a memorandum to be reported afterwards in serial order. In essence, an individual’s reading span score reflects how many end-of-sentence words can be remembered whilst reading. <a href=\"#R13\">Daneman and Carpenter (1980)</a> showed reading span to be a very good measure of reading skill (see also Daneman &amp; Merikle, 1986). The predictive prowess of WM span tasks (including alternatives such as operation span where the processing task involves arithmetic operations, <a href=\"#R36\">Turner &amp; Engle, 1989</a>) provides empirical support for the conceptual idea that the processing-plus-memory requirements tap an important skill in complex cognition.</p>\n<p>Several theories suggest, in different ways and to different degrees, that a competitive relationship between processing and memory activities is critical to measuring WM capacity. In other words, that the maintenance of information takes place in the face of distraction or interference from concurrent processing. For example, <a href=\"#R4\">Case (1985)</a> proposed that limited-capacity general-purpose cognitive resources were allocated to <span>either</span> processing <span>or</span> memory demands. <a href=\"#R22\">Jarrold and Bayliss (2007)</a> discuss evidence that combining or coordinating processing with memory places demands on WM, above and beyond those imposed by each requirement <span>per se</span>. <a href=\"#R34\">Towse, Hitch and Hutton (1998)</a> argued that processing activity produces informational degradation because memories are not actively or continuously maintained (in this respect, see also <a href=\"#R2\">Barrouillet, Bernadin, &amp; Camos, 2004</a>). <a href=\"#R23\">Kane and Engle (2003)</a> suggested that controlled attention is important to preserve memory representations at the same time as the concurrent processing requirements.</p>\n<p>We can see important insights to be gained from each of these accounts, and we do not intend here to arbitrate between them. Rather, our focus is directed towards the concept that links them; processing and memory are thought to be separable, even exclusive events. We certainly accept that processing can interfere with retention. Nonetheless, we present data that lead us to conclude that this is not the whole story; processing may also complement or support memory (for an early and seminal version of this perspective, see <a href=\"#R12\">Craik &amp; Lockhart, 1972</a>).</p>\n<p>Our core proposal that processing and memory need not always be thought of as completely separate events. Using behavioural and chronometric evidence, we propose that psychological models can be enhanced by considering a broader view of the WM representations that are present at the point of recall.</p>\n<p>Chronometric analysis of memory recall – the timing of correct output sequences – has generally focused on short-term memory (STM) tasks such as word span, where a sequence of unrelated items are presented at a regular rate and then reproduced in their original order (e.g., <a href=\"#R15\">Dosher &amp; Ma, 1998</a>; for an overview, <a href=\"#R32\">Towse &amp; Cowan, 2005</a>). Whilst such research has been undoubtedly productive, given the body of evidence to distinguish STM from WM (e.g., <a href=\"#R13\">Daneman &amp; Carpenter, 1980</a>; <a href=\"#R16\">Engle, Kane &amp; Tuholski, 1999</a>), there is a clear motivation to investigate recall timing in WM. <a href=\"#R10\">Cowan et al. (2003)</a> did just this. They found children’s reading span recall times were dramatically longer than has been obtained in STM studies, and that for both children and adults (but especially the former) response durations for listening span exceeded those of counting span and digit span. Cowan et al. also reported that WM response durations predicted children’s word reading skills over and above the contribution of span scores <span>per se</span>: recall evidently incorporates processes relevant to children’s cognitive development and attainment.</p>\n<p>The particularly long interword pauses in reading span and listening span led <a href=\"#R10\">Cowan et al. (2003)</a> to two related conclusions. First, memory representations may not always be maintained in a highly accessible state during processing. If they had been, one would expect their rapid production during recall. Second, participants sometimes draw on memory of the sentence, in terms of thematic and semantic context, for the elicitation of the target items. Cowan et al. found that recall in counting span was less protracted than listening span, and attributed this to the lack of distinctive processing in the enumeration of visual displays, and thus the absence of a similar scaffolding process. Thus, reading span and listening span recall can involve the consideration of a much richer ensemble of (perhaps loosely encoded) memories of the trial episode than is the case for other tasks.</p>\n<p>To capture these ideas we advance here a ‘recall reconstruction’ hypothesis for WM performance. The central proposal is that participants may bring to recall more than just representations of experimentally-assigned memoranda (i.e. target memory words). In the specific case of reading span, this can involve sentential information, which is combined with other representations over time. As a consequence, we propose that the memory sequence may not be continuously and actively maintained and consequently recall involves the resuscitation of degraded information.</p>\n<p>According to this recall reconstruction perspective, WM potentially involves the intertwined and integrated aspects of processing and memory. Processing and memory activities need not inherently be in complete opposition to each other, dependent on the specific WM task. Consider an example sentence from <a href=\"#R13\">Daneman and Carpenter (1980)</a>: “<em>The lumbermen worked long hours in order to obtain the necessary amount of wood</em>.” According to the position just outlined, later recall of “wood” might be facilitated by gist or episodic memory about lumbermen, or indeed associations made during reading to the implicit concept “trees”. An individual need not commit a sentence to memory verbatim, but relevant linguistic information could nonetheless be accessible, either to help reconstruct the word “wood” or to discount sentence-terminal words appropriate to other serial positions.</p>\n<p>This recall reconstruction hypothesis is quite compatible with the evidence in children that recall pauses predict cognitive ability since item reconstruction using self-generated cues may be a source of skilled performance. Moreover, <a href=\"#R29\">Saito &amp; Miyake (2004)</a> have pointed to a relationship between processing activity and memory within their representation-based interference account of WM. While they concentrated on how processing events can <span>hinder</span> memory, via overlapping representations that interfere, they similarly argue that the content of processing can be relevant to memory performance.</p>\n<p>So far as we are aware, no studies have directly investigated the relationship between processing content and memory requirements in WM. However, <a href=\"#R28\">Osaka, Nishizaki, Komori &amp; Osaka (2002)</a> investigated reading span, and for language-specific reasons underlined the word that was to be remembered. They either underlined a “focus” word –the most important word for sentence comprehension-or a non-focus word, w",
        "html": "<div><div>\n<main>\n<article><section><div>\n<p>. Author manuscript; available in PMC: 2009 Mar 19.</p></div></section><section><section><h2>Abstract</h2>\n<p>In four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy vary between these task formats, dependent on the orienting task for processing. Experiment 1 compares the chronometry of spoken recall for word span and reading span, in which participants complete an unfinished sentence. Experiment 2 and 3 confirm recall time differences without using word generation requirements, while Experiments 4 used an item and order response choice paradigm with nonspoken responses. We argue that verbal and manual recall timing offers an informative measure for understanding working memory.</p>\n<section><p><strong>Keywords:</strong> working memory, reading span, recall timing, recall method, short-term memory</p></section></section><section><h2>Introduction</h2>\n<p>Working memory reflects the ability to hold in mind transient representations while simultaneously processing and assimilating ongoing events (<a href=\"#R1\">Baddeley &amp; Hitch, 1974</a>). There are a wide variety of circumstances in which we are required to carry out mental operations and remember intermediate information (for instance, retain a carry item in mental arithmetic or a referent for an anaphoric pronoun) as well as situations in which current mental activities need to draw on past episodic knowledge (e.g., mapping the problem space for a current task using knowledge of related situations). They emphasise the importance of understanding active maintenance and transformation processes. Consequently, the concept of working memory has been the focus of considerable research.</p>\n<p>Although theories of working memory differ considerably (see <a href=\"#R27\">Miyake &amp; Shah, 1999</a>), the most common method for assessing its capacity is to draw upon at least one of a family of tasks known as working memory (WM) span. Reading span was the first such task to be reported in adults (<a href=\"#R13\">Daneman &amp; Carpenter, 1980</a>). Participants read a series of unconnected sentences, and the final word in each sentence yields a memorandum to be reported afterwards in serial order. In essence, an individual’s reading span score reflects how many end-of-sentence words can be remembered whilst reading. <a href=\"#R13\">Daneman and Carpenter (1980)</a> showed reading span to be a very good measure of reading skill (see also Daneman &amp; Merikle, 1986). The predictive prowess of WM span tasks (including alternatives such as operation span where the processing task involves arithmetic operations, <a href=\"#R36\">Turner &amp; Engle, 1989</a>) provides empirical support for the conceptual idea that the processing-plus-memory requirements tap an important skill in complex cognition.</p>\n<p>Several theories suggest, in different ways and to different degrees, that a competitive relationship between processing and memory activities is critical to measuring WM capacity. In other words, that the maintenance of information takes place in the face of distraction or interference from concurrent processing. For example, <a href=\"#R4\">Case (1985)</a> proposed that limited-capacity general-purpose cognitive resources were allocated to <span>either</span> processing <span>or</span> memory demands. <a href=\"#R22\">Jarrold and Bayliss (2007)</a> discuss evidence that combining or coordinating processing with memory places demands on WM, above and beyond those imposed by each requirement <span>per se</span>. <a href=\"#R34\">Towse, Hitch and Hutton (1998)</a> argued that processing activity produces informational degradation because memories are not actively or continuously maintained (in this respect, see also <a href=\"#R2\">Barrouillet, Bernadin, &amp; Camos, 2004</a>). <a href=\"#R23\">Kane and Engle (2003)</a> suggested that controlled attention is important to preserve memory representations at the same time as the concurrent processing requirements.</p>\n<p>We can see important insights to be gained from each of these accounts, and we do not intend here to arbitrate between them. Rather, our focus is directed towards the concept that links them; processing and memory are thought to be separable, even exclusive events. We certainly accept that processing can interfere with retention. Nonetheless, we present data that lead us to conclude that this is not the whole story; processing may also complement or support memory (for an early and seminal version of this perspective, see <a href=\"#R12\">Craik &amp; Lockhart, 1972</a>).</p>\n<p>Our core proposal that processing and memory need not always be thought of as completely separate events. Using behavioural and chronometric evidence, we propose that psychological models can be enhanced by considering a broader view of the WM representations that are present at the point of recall.</p>\n<p>Chronometric analysis of memory recall – the timing of correct output sequences – has generally focused on short-term memory (STM) tasks such as word span, where a sequence of unrelated items are presented at a regular rate and then reproduced in their original order (e.g., <a href=\"#R15\">Dosher &amp; Ma, 1998</a>; for an overview, <a href=\"#R32\">Towse &amp; Cowan, 2005</a>). Whilst such research has been undoubtedly productive, given the body of evidence to distinguish STM from WM (e.g., <a href=\"#R13\">Daneman &amp; Carpenter, 1980</a>; <a href=\"#R16\">Engle, Kane &amp; Tuholski, 1999</a>), there is a clear motivation to investigate recall timing in WM. <a href=\"#R10\">Cowan et al. (2003)</a> did just this. They found children’s reading span recall times were dramatically longer than has been obtained in STM studies, and that for both children and adults (but especially the former) response durations for listening span exceeded those of counting span and digit span. Cowan et al. also reported that WM response durations predicted children’s word reading skills over and above the contribution of span scores <span>per se</span>: recall evidently incorporates processes relevant to children’s cognitive development and attainment.</p>\n<p>The particularly long interword pauses in reading span and listening span led <a href=\"#R10\">Cowan et al. (2003)</a> to two related conclusions. First, memory representations may not always be maintained in a highly accessible state during processing. If they had been, one would expect their rapid production during recall. Second, participants sometimes draw on memory of the sentence, in terms of thematic and semantic context, for the elicitation of the target items. Cowan et al. found that recall in counting span was less protracted than listening span, and attributed this to the lack of distinctive processing in the enumeration of visual displays, and thus the absence of a similar scaffolding process. Thus, reading span and listening span recall can involve the consideration of a much richer ensemble of (perhaps loosely encoded) memories of the trial episode than is the case for other tasks.</p>\n<p>To capture these ideas we advance here a ‘recall reconstruction’ hypothesis for WM performance. The central proposal is that participants may bring to recall more than just representations of experimentally-assigned memoranda (i.e. target memory words). In the specific case of reading span, this can involve sentential information, which is combined with other representations over time. As a consequence, we propose that the memory sequence may not be continuously and actively maintained and consequently recall involves the resuscitation of degraded information.</p>\n<p>According to this recall reconstruction perspective, WM potentially involves the intertwined and integrated aspects of processing and memory. Processing and memory activities need not inherently be in complete opposition to each other, dependent on the specific WM task. Consider an example sentence from <a href=\"#R13\">Daneman and Carpenter (1980)</a>: “<em>The lumbermen worked long hours in order to obtain the necessary amount of wood</em>.” According to the position just outlined, later recall of “wood” might be facilitated by gist or episodic memory about lumbermen, or indeed associations made during reading to the implicit concept “trees”. An individual need not commit a sentence to memory verbatim, but relevant linguistic information could nonetheless be accessible, either to help reconstruct the word “wood” or to discount sentence-terminal words appropriate to other serial positions.</p>\n<p>This recall reconstruction hypothesis is quite compatible with the evidence in children that recall pauses predict cognitive ability since item reconstruction using self-generated cues may be a source of skilled performance. Moreover, <a href=\"#R29\">Saito &amp; Miyake (2004)</a> have pointed to a relationship between processing activity and memory within their representation-based interference account of WM. While they concentrated on how processing events can <span>hinder</span> memory, via overlapping representations that interfere, they similarly argue that the content of processing can be relevant to memory performance.</p>\n<p>So far as we are aware, no studies have directly investigated the relationship between processing content and memory requirements in WM. However, <a href=\"#R28\">Osaka, Nishizaki, Komori &amp; Osaka (2002)</a> investigated reading span, and for language-specific reasons underlined the word that was to be remembered. They either underlined a “focus” word –the most important word for sentence comprehension-or a non-focus word, w",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2009 Mar 19.AbstractIn four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2009 Mar 19.AbstractIn four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": ". Author manuscript; available in PMC: 2009 Mar 19.AbstractIn four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": ". Author manuscript; available in PMC: 2009 Mar 19.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2009 Mar 19.",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractIn four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy vary between these task formats, dependent on the ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractIn four experiments we test a recall reconstruction hypothesis for working memory, according to which reading span items can be recovered or specified from multiple memory representations. Each reading span experiment involves memoranda either embedded within or unrelated to the sentence content. This manipulation affected the timing of recall, with longer pauses accompanying items that are linked to processing. Levels of recall accuracy vary between these task formats, dependent on the ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Keywords:working memory, reading span, recall timing, recall method, short-term memory",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionWorking memory reflects the ability to hold in mind transient representations while simultaneously processing and assimilating ongoing events (Baddeley & Hitch, 1974). There are a wide variety of circumstances in which we are required to carry out mental operations and remember intermediate information (for instance, retain a carry item in mental arithmetic or a referent for an anaphoric pronoun) as well as situations in which current mental activities need to draw on past episodic k",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6132650/",
      "title": "The storage and recall of memories in the hippocampo-cortical system",
      "author": "",
      "published_date": "2017-12-07T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>A quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, which is also important in episodic memory. The dentate gyrus performs pattern separation by competitive learning to create sparse representations producing, for example, neurons with place-like fields from entorhinal cortex grid cells. The dentate granule cells generate, by the very small number of mossy fibre connections to CA3, a randomizing pattern separation effect that is important during learning but not recall and that separates out the patterns represented by CA3 firing as being very different from each other. This is optimal for an unstructured episodic memory system in which each memory must be kept distinct from other memories. The direct perforant path input to CA3 is quantitatively appropriate for providing the cue for recall in CA3 but not for learning. The CA1 recodes information from CA3 to set up associatively learned backprojections to the neocortex to allow the subsequent retrieval of information to the neocortex, giving a quantitative account of the large number of hippocampo-neocortical and neocortical-neocortical backprojections. Tests of the theory including hippocampal subregion analyses and hippocampal NMDA receptor knockouts are described and support the theory.</p>\n<section><h3>Electronic supplementary material</h3>\n<p>The online version of this article (10.1007/s00441-017-2744-3) contains supplementary material, which is available to authorized users.</p></section><section><p><strong>Keywords:</strong> Completion</p></section></section><section><h2>Introduction</h2>\n<p>A computational theory of the operation of networks in the hippocampus in memory (Kesner and Rolls <a href=\"#CR74\">2015</a>; Rolls <a href=\"#CR145\">2010</a>, <a href=\"#CR154\">2016a</a>) is described. The type of memory is episodic, referring to the memory of a particular event or linked group of events occurring typically at the same time and place. An example might be where dinner was yesterday, who was present, who sat where, what the menu was and the discussion. This must be kept separate from, for example, what happened the day before that. Episodic memory almost always has a spatial component (Dere et al. <a href=\"#CR39\">2008</a>; Rolls <a href=\"#CR157\">2017</a>) and a spatial representation in the hippocampus can be updated by self-motion to produce path integration (McNaughton et al. <a href=\"#CR108\">1996</a>; Robertson et al. <a href=\"#CR132\">1998</a>; Rolls <a href=\"#CR154\">2016a</a>; E.T. Rolls and S. Wirth in preparation).</p>\n<p>Episodic memory can be operationally investigated in animals including humans in the following ways. First is the ability to store rapidly, on a single trial, a unique combination of inputs that typically involve place or time and objects including people and, later, to recall the whole memory from any part. The episodic memory, in being formed rapidly, is relatively unstructured and may be formed simply by associating together the spatial or temporal and object representations. In contrast, a semantic memory has structure and may require many exemplars to learn the representation, as exceptions might occur, such as that an ostrich is a bird but cannot fly (McClelland et al. <a href=\"#CR102\">1995</a>). The recall of many episodic memories from the hippocampus may help to build semantic memories in the neocortex, for example, a map of the world based on the journeys that one has made. An example of a semantic representation is a Jennifer Aniston neuron, which may respond not only to Jennifer Aniston but also to other actors in the same movie and the places with which they are associated (Quiroga <a href=\"#CR128\">2012</a>; Rey et al. <a href=\"#CR131\">2015</a>). These neurons are probably formed in high-order neocortical areas in the temporal lobes and their junction with the parietal lobes; and their presence in the medial temporal lobe (Quiroga <a href=\"#CR128\">2012</a>), for example, the parahippocampal gyrus, is probably because the hippocampal system receives input from these high-order neocortical areas. Autobiographical memory is a semantic memory that involves representations of the self, frequently involving the precuneus (Bubb et al. <a href=\"#CR15\">2017</a>; Cavanna and Trimble <a href=\"#CR22\">2006</a>; Fossati <a href=\"#CR45\">2013</a>) and that might be built by using the recall of episodic memories. A second property of an episodic memory is that it may involve a temporal sequence of events. The hippocampus has mechanisms that help to implement this (Eichenbaum <a href=\"#CR40\">2014</a>; Howard and Eichenbaum <a href=\"#CR65\">2015</a>; Kesner and Rolls <a href=\"#CR74\">2015</a>; Kraus et al. <a href=\"#CR83\">2013b</a>), as described below.</p>\n<p>The theory of the hippocampus and episodic memory is based on the remarkable neural architecture of the hippocampus, on the effects of damage to it and on the neuronal activity recorded in it. Once memories have been stored in the hippocampus, they may later need to be recalled to the neocortex; a theory of the recall mechanism is part of the overall theory (Treves and Rolls <a href=\"#CR203\">1994</a>; see also <a href=\"#Sec25\">Backprojections to the neocortex and memory recall</a>). Once recalled to the neocortex, the memories of particular events or episodes can be reported verbally and, hence, this is a type of declarative memory (Squire and Wixted <a href=\"#CR186\">2011</a>). The recalled information may also be combined with other information to be reorganized and stored semantically in the neocortex, i.e., in a form that reflects meaning and structure, in contrast to the episodic memories captured as discrete memories by the hippocampus (McClelland et al. <a href=\"#CR102\">1995</a>). An example of a semantic representation might be a mental map that includes and describes the relationships between the places to which one has made particular journeys. I start with a description of the underlying architecture and functions of the hippocampus in order to provide a firm foundation for the theory and then show ways in which the theory is being tested experimentally.</p></section><section><h2>Overview</h2>\n<p>Some of the key points in the computational theory are as follows. The hippocampal CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, which is also important in episodic memory. The dentate gyrus performs pattern separation by competitive learning to produce sparse representations, producing, for example, neurons with place-like fields from entorhinal cortex grid cells. The dentate granule cells produce, by the very small number of mossy fibre connections to CA3, a randomizing pattern separation effect that is important during learning but not recall and that separates out the patterns represented by CA3 firing as being very different from each other; this is optimal for an unstructured episodic memory system in which each memory must be kept distinct from other memories. The direct perforant path input to CA3 projection is quantitatively appropriate to provide, as a pattern association mechanism, the cue for recall in CA3. The CA1 recodes information from CA3 in order to set up associatively learned backprojections to the neocortex to allow the subsequent retrieval of information to the neocortex, providing a quantitative account of the large number of hippocampo-neocortical and neocortical-neocortical backprojections. Empirical tests of the theory including hippocampal subregion analyses and selective hippocampal NMDA receptor knockouts are described and support the theory.</p></section><section><h2>Structure and function of the hippocampal system</h2>\n<section><h3>Effects of damage to the hippocampus</h3>\n<p>In the patient H.M., bilateral damage to the hippocampus performed to treat epilepsy produced an inability to remember “recent” events (those since the hippocampal and related damage), while leaving the memory of events that occurred prior to the hippocampal damage and semantic and skill memory relatively unimpaired (Corkin <a href=\"#CR30\">2002</a>; Scoville and Milner <a href=\"#CR184\">1957</a>). In tests to examine the exact brain regions that impair this memory for events, tasks that require objects to be associated with the place in which they are located have been shown to be especially sensitive to hippocampal damage. Examples include memory for the location of an escape platform in a water bath in rats (Andersen et al. <a href=\"#CR9\">2007</a>; Morris and Frey <a href=\"#CR111\">1997</a>) and for the location of an odour signifying the place where a food will be found in a cheeseboard task (Kesner and Rolls <a href=\"#CR74\">2015</a>). Temporal order memory for a sequence of places or objects is also impaired by hippocampal damage (Kesner and Rolls <a href=\"#CR74\">2015</a>) and this functionality may be important in temporally linking a sequence of events within an episodic memory. In monkeys, analogous tasks involving object-place memory are impaired by hippocampal damage (Banta Lavenex and Lavenex <a href=\"#CR10\">2009</a>), whereas damage to the overlying perirhinal cortex, whi",
        "html": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>A quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, which is also important in episodic memory. The dentate gyrus performs pattern separation by competitive learning to create sparse representations producing, for example, neurons with place-like fields from entorhinal cortex grid cells. The dentate granule cells generate, by the very small number of mossy fibre connections to CA3, a randomizing pattern separation effect that is important during learning but not recall and that separates out the patterns represented by CA3 firing as being very different from each other. This is optimal for an unstructured episodic memory system in which each memory must be kept distinct from other memories. The direct perforant path input to CA3 is quantitatively appropriate for providing the cue for recall in CA3 but not for learning. The CA1 recodes information from CA3 to set up associatively learned backprojections to the neocortex to allow the subsequent retrieval of information to the neocortex, giving a quantitative account of the large number of hippocampo-neocortical and neocortical-neocortical backprojections. Tests of the theory including hippocampal subregion analyses and hippocampal NMDA receptor knockouts are described and support the theory.</p>\n<section><h3>Electronic supplementary material</h3>\n<p>The online version of this article (10.1007/s00441-017-2744-3) contains supplementary material, which is available to authorized users.</p></section><section><p><strong>Keywords:</strong> Completion</p></section></section><section><h2>Introduction</h2>\n<p>A computational theory of the operation of networks in the hippocampus in memory (Kesner and Rolls <a href=\"#CR74\">2015</a>; Rolls <a href=\"#CR145\">2010</a>, <a href=\"#CR154\">2016a</a>) is described. The type of memory is episodic, referring to the memory of a particular event or linked group of events occurring typically at the same time and place. An example might be where dinner was yesterday, who was present, who sat where, what the menu was and the discussion. This must be kept separate from, for example, what happened the day before that. Episodic memory almost always has a spatial component (Dere et al. <a href=\"#CR39\">2008</a>; Rolls <a href=\"#CR157\">2017</a>) and a spatial representation in the hippocampus can be updated by self-motion to produce path integration (McNaughton et al. <a href=\"#CR108\">1996</a>; Robertson et al. <a href=\"#CR132\">1998</a>; Rolls <a href=\"#CR154\">2016a</a>; E.T. Rolls and S. Wirth in preparation).</p>\n<p>Episodic memory can be operationally investigated in animals including humans in the following ways. First is the ability to store rapidly, on a single trial, a unique combination of inputs that typically involve place or time and objects including people and, later, to recall the whole memory from any part. The episodic memory, in being formed rapidly, is relatively unstructured and may be formed simply by associating together the spatial or temporal and object representations. In contrast, a semantic memory has structure and may require many exemplars to learn the representation, as exceptions might occur, such as that an ostrich is a bird but cannot fly (McClelland et al. <a href=\"#CR102\">1995</a>). The recall of many episodic memories from the hippocampus may help to build semantic memories in the neocortex, for example, a map of the world based on the journeys that one has made. An example of a semantic representation is a Jennifer Aniston neuron, which may respond not only to Jennifer Aniston but also to other actors in the same movie and the places with which they are associated (Quiroga <a href=\"#CR128\">2012</a>; Rey et al. <a href=\"#CR131\">2015</a>). These neurons are probably formed in high-order neocortical areas in the temporal lobes and their junction with the parietal lobes; and their presence in the medial temporal lobe (Quiroga <a href=\"#CR128\">2012</a>), for example, the parahippocampal gyrus, is probably because the hippocampal system receives input from these high-order neocortical areas. Autobiographical memory is a semantic memory that involves representations of the self, frequently involving the precuneus (Bubb et al. <a href=\"#CR15\">2017</a>; Cavanna and Trimble <a href=\"#CR22\">2006</a>; Fossati <a href=\"#CR45\">2013</a>) and that might be built by using the recall of episodic memories. A second property of an episodic memory is that it may involve a temporal sequence of events. The hippocampus has mechanisms that help to implement this (Eichenbaum <a href=\"#CR40\">2014</a>; Howard and Eichenbaum <a href=\"#CR65\">2015</a>; Kesner and Rolls <a href=\"#CR74\">2015</a>; Kraus et al. <a href=\"#CR83\">2013b</a>), as described below.</p>\n<p>The theory of the hippocampus and episodic memory is based on the remarkable neural architecture of the hippocampus, on the effects of damage to it and on the neuronal activity recorded in it. Once memories have been stored in the hippocampus, they may later need to be recalled to the neocortex; a theory of the recall mechanism is part of the overall theory (Treves and Rolls <a href=\"#CR203\">1994</a>; see also <a href=\"#Sec25\">Backprojections to the neocortex and memory recall</a>). Once recalled to the neocortex, the memories of particular events or episodes can be reported verbally and, hence, this is a type of declarative memory (Squire and Wixted <a href=\"#CR186\">2011</a>). The recalled information may also be combined with other information to be reorganized and stored semantically in the neocortex, i.e., in a form that reflects meaning and structure, in contrast to the episodic memories captured as discrete memories by the hippocampus (McClelland et al. <a href=\"#CR102\">1995</a>). An example of a semantic representation might be a mental map that includes and describes the relationships between the places to which one has made particular journeys. I start with a description of the underlying architecture and functions of the hippocampus in order to provide a firm foundation for the theory and then show ways in which the theory is being tested experimentally.</p></section><section><h2>Overview</h2>\n<p>Some of the key points in the computational theory are as follows. The hippocampal CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, which is also important in episodic memory. The dentate gyrus performs pattern separation by competitive learning to produce sparse representations, producing, for example, neurons with place-like fields from entorhinal cortex grid cells. The dentate granule cells produce, by the very small number of mossy fibre connections to CA3, a randomizing pattern separation effect that is important during learning but not recall and that separates out the patterns represented by CA3 firing as being very different from each other; this is optimal for an unstructured episodic memory system in which each memory must be kept distinct from other memories. The direct perforant path input to CA3 projection is quantitatively appropriate to provide, as a pattern association mechanism, the cue for recall in CA3. The CA1 recodes information from CA3 in order to set up associatively learned backprojections to the neocortex to allow the subsequent retrieval of information to the neocortex, providing a quantitative account of the large number of hippocampo-neocortical and neocortical-neocortical backprojections. Empirical tests of the theory including hippocampal subregion analyses and selective hippocampal NMDA receptor knockouts are described and support the theory.</p></section><section><h2>Structure and function of the hippocampal system</h2>\n<section><h3>Effects of damage to the hippocampus</h3>\n<p>In the patient H.M., bilateral damage to the hippocampus performed to treat epilepsy produced an inability to remember “recent” events (those since the hippocampal and related damage), while leaving the memory of events that occurred prior to the hippocampal damage and semantic and skill memory relatively unimpaired (Corkin <a href=\"#CR30\">2002</a>; Scoville and Milner <a href=\"#CR184\">1957</a>). In tests to examine the exact brain regions that impair this memory for events, tasks that require objects to be associated with the place in which they are located have been shown to be especially sensitive to hippocampal damage. Examples include memory for the location of an escape platform in a water bath in rats (Andersen et al. <a href=\"#CR9\">2007</a>; Morris and Frey <a href=\"#CR111\">1997</a>) and for the location of an odour signifying the place where a food will be found in a cheeseboard task (Kesner and Rolls <a href=\"#CR74\">2015</a>). Temporal order memory for a sequence of places or objects is also impaired by hippocampal damage (Kesner and Rolls <a href=\"#CR74\">2015</a>) and this functionality may be important in temporally linking a sequence of events within an episodic memory. In monkeys, analogous tasks involving object-place memory are impaired by hippocampal damage (Banta Lavenex and Lavenex <a href=\"#CR10\">2009</a>), whereas damage to the overlying perirhinal cortex, whi",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "AbstractA quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to im",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractA quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to im",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "AbstractA quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to im",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractA quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to im",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractA quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to im",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Electronic supplementary materialThe online version of this article (10.1007/s00441-017-2744-3) contains supplementary material, which is available to authorized users.",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Keywords:Completion",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionA computational theory of the operation of networks in the hippocampus in memory (Kesner and Rolls2015; Rolls2010,2016a) is described. The type of memory is episodic, referring to the memory of a particular event or linked group of events occurring typically at the same time and place. An example might be where dinner was yesterday, who was present, who sat where, what the menu was and the discussion. This must be kept separate from, for example, what happened the day before that. Ep",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "OverviewSome of the key points in the computational theory are as follows. The hippocampal CA3 system operates as a single attractor or autoassociation network (1) to enable rapid one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and (2) to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, which",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Structure and function of the hippocampal systemEffects of damage to the hippocampusIn the patient H.M., bilateral damage to the hippocampus performed to treat epilepsy produced an inability to remember “recent” events (those since the hippocampal and related damage), while leaving the memory of events that occurred prior to the hippocampal damage and semantic and skill memory relatively unimpaired (Corkin2002; Scoville and Milner1957). In tests to examine the exact brain regions that impair thi",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Effects of damage to the hippocampusIn the patient H.M., bilateral damage to the hippocampus performed to treat epilepsy produced an inability to remember “recent” events (those since the hippocampal and related damage), while leaving the memory of events that occurred prior to the hippocampal damage and semantic and skill memory relatively unimpaired (Corkin2002; Scoville and Milner1957). In tests to examine the exact brain regions that impair this memory for events, tasks that require objects ",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Electronic supplementary material",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Overview",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Structure and function of the hippocampal system",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Effects of damage to the hippocampus",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://arxiv.org/abs/2405.03988",
      "title": "Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application",
      "author": "Jia; Wang; Yipei; Li; Yan; Chen; Honggang; Bai; Xuehan; Zhaocheng; Quan; Han; Peng; Gai; Kun",
      "published_date": "2024-05-07T00:00:00.000Z",
      "content": {
        "text": "https://arxiv.org/abs/2405.03988\nKnowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application\n2024-05-07T00:00:00Z\n[Submitted on 7 May 2024]\n<html><body><div><div>\n<div><p><span>Authors:</span><a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jia,+J\">Jian Jia</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y\">Yipei Wang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y\">Yan Li</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H\">Honggang Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+X\">Xuehan Bai</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z\">Zhaocheng Liu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J\">Jian Liang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q\">Quan Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H\">Han Li</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+P\">Peng Jiang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gai,+K\">Kun Gai</a></p></div>\n<p><a href=\"https://arxiv.org/pdf/2405.03988\">View PDF</a>\n<a href=\"https://arxiv.org/html/2405.03988v1\">HTML (experimental)</a></p><blockquote>\n<span>Abstract:</span>Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations. Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge. In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach.\n</blockquote>\n</div><div>\n<h2>Submission history</h2><p> From: Jian Jia [<a href=\"https://arxiv.org/show-email/ff12fed4/2405.03988\">view email</a>] <br/> <strong>[v1]</strong>\nTue, 7 May 2024 04:00:30 UTC (3,724 KB)<br/>\n</p></div></div></body></html>",
        "html": "https://arxiv.org/abs/2405.03988\nKnowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application\n2024-05-07T00:00:00Z\n[Submitted on 7 May 2024]\n<html><body><div><div>\n<div><p><span>Authors:</span><a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jia,+J\">Jian Jia</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y\">Yipei Wang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y\">Yan Li</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H\">Honggang Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+X\">Xuehan Bai</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z\">Zhaocheng Liu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J\">Jian Liang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q\">Quan Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H\">Han Li</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+P\">Peng Jiang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gai,+K\">Kun Gai</a></p></div>\n<p><a href=\"https://arxiv.org/pdf/2405.03988\">View PDF</a>\n<a href=\"https://arxiv.org/html/2405.03988v1\">HTML (experimental)</a></p><blockquote>\n<span>Abstract:</span>Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations. Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge. In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach.\n</blockquote>\n</div><div>\n<h2>Submission history</h2><p> From: Jian Jia [<a href=\"https://arxiv.org/show-email/ff12fed4/2405.03988\">view email</a>] <br/> <strong>[v1]</strong>\nTue, 7 May 2024 04:00:30 UTC (3,724 KB)<br/>\n</p></div></div></body></html>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Authors:Jian Jia,Yipei Wang,Yan Li,Honggang Chen,Xuehan Bai,Zhaocheng Liu,Jian Liang,Quan Chen,Han Li,Peng Jiang,Kun GaiView PDFHTML (experimental)Abstract:Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Authors:Jian Jia,Yipei Wang,Yan Li,Honggang Chen,Xuehan Bai,Zhaocheng Liu,Jian Liang,Quan Chen,Han Li,Peng Jiang,Kun GaiView PDFHTML (experimental)Abstract:Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Authors:Jian Jia,Yipei Wang,Yan Li,Honggang Chen,Xuehan Bai,Zhaocheng Liu,Jian Liang,Quan Chen,Han Li,Peng Jiang,Kun Gai",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Submission historyFrom: Jian Jia [view email][v1]Tue, 7 May 2024 04:00:30 UTC (3,724 KB)",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Submission history",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://learninglab.psych.purdue.edu/downloads/2012/2012_Karpicke_CDPS.pdf",
      "title": "",
      "author": "",
      "published_date": "2012-05-12T00:00:00.000Z",
      "content": {
        "text": "Current Directions in Psychological\nScience\n21(3) 157–163\n© The Author(s) 2012\nReprints and permission:\nsagepub.com/journalsPermissions.nav\nDOI: 10.1177/0963721412443552\nhttp://cdps.sagepub.com\nIf you know something, or if you have stored informa\u0002tion about an event from the distant past, and never use\nthat information, never think of it, your brain is func\u0002tionally equivalent to that of an otherwise identical brain\nthat does not “contain” that information.\n—Endel Tulving (1991)\nTo understand learning, it is essential to understand the pro\u0002cesses involved in retrieving and reconstructing knowledge. We\nmay think we know something, that our minds contain or pos\u0002sess some knowledge, but the only way to assess knowledge is\nby engaging in an act of retrieval. Differences in the ability to\nrecover knowledge may not stem from what is “stored” in our\nminds but rather from differences in the retrieval cues available\nin particular contexts. Given the fundamental importance of\nretrieval for understanding the process of learning, it is surpris\u0002ing that retrieval processes have not received more attention in\neducational research. Consider that over the past decade, many\ninfluential National Research Council books on how people\nlearn have contained no mention of retrieval (National Research\nCouncil, 2000, 2005a, 2005b).\nIt is essential to consider retrieval processes not only\nbecause they are central to understanding learning but also\nbecause the act of retrieval itself is a powerful tool for enhanc\u0002ing learning. Moreover, active retrieval does not merely\nproduce rote, transient learning; it produces meaningful, long\u0002term learning. The idea that retrieval is the centerpiece for\nunderstanding learning, coupled with the importance of active\nretrieval for producing learning, is referred to as retrieval\u0002based learning.\nLearning Based on the Design of the Mind\nWe often think of our minds as places in our heads, mental\nspaces or containers where we store knowledge. Roediger\n(1980) noted that for centuries, most metaphors used to\ndescribe mental processes have characterized the mind as a\nphysical space and knowledge as physical things in that\nspace—for example, by likening our minds to libraries filled\nwith books or cabinets loaded with files (see too Moscovitch,\n2007). In education, the metaphor of a physical building is\noften used to describe the mind and knowledge. Knowledge is\nconstructed by learners who actively build knowledge struc\u0002tures; researchers seek to understand the architecture of the\nmind; and instructors aid students by providing scaffolding for\nlearning.\nWhen minds are viewed as places for storing knowledge, it\nis natural to focus attention on processes involved in\nconstructing new knowledge in storage. Educational research\nCorresponding Author:\nJeffrey D. Karpicke, Department of Psychological Sciences, Purdue University,\n703 Third St., West Lafayette, IN 47907-2081\nE-mail: karpicke@purdue.edu\nRetrieval-Based Learning: Active\nRetrieval Promotes Meaningful Learning\nJeffrey D. Karpicke\nPurdue University\nAbstract\nRetrieval is the key process for understanding learning and for promoting learning, yet retrieval is not often granted the central\nrole it deserves. Learning is typically identified with the encoding or construction of knowledge, and retrieval is considered\nmerely the assessment of learning that occurred in a prior experience. The retrieval-based learning perspective outlined\nhere is grounded in the fact that all expressions of knowledge involve retrieval and depend on the retrieval cues available in\na given context. Further, every time a person retrieves knowledge, that knowledge is changed, because retrieving knowledge\nimproves one’s ability to retrieve it again in the future. Practicing retrieval does not merely produce rote, transient learning;\nit produces meaningful, long-term learning. Yet retrieval practice is a tool many students lack metacognitive awareness of\nand do not use as often as they should. Active retrieval is an effective but undervalued strategy for promoting meaningful\nlearning.\nKeywords\nretrieval processes, learning, education, metacognition, meaningful learning\n158 Karpicke\nand instructional practices have placed a premium on identify\u0002ing the best ways to encode knowledge and experiences.\nRetrieval processes, the processes involved in using available\ncues to actively reconstruct knowledge, have received less\nattention. There seems to be a tacit assumption that successful\nencoding or construction of knowledge, in itself, is sufficient\nto ensure learning.\nBasic research on learning and memory, however, has\nemphasized that retrieval must be considered in any analysis\nof learning. In part, this is because people do not store static,\nexact copies of experiences that are reproduced verbatim at\nretrieval. Instead, knowledge is actively reconstructed on the\nbasis of the present context and available retrieval cues\n(Bartlett, 1932; Neisser, 1967; see too Moscovitch, 2007;\nRoediger, 2000). The reconstructive nature of mind is revealed\nin the systematic errors people make in retrieving knowledge,\nerrors that verbatim recording devices would not make. The\npast never occurs again in its exact form, so a mental store\u0002house of copies of past experiences would be of little use.\nPeople instead have the ability to use the past to meet the\ndemands of the present by reconstructing knowledge rather\nthan reproducing it exactly.\nWhat people express when they reconstruct knowledge\ndepends on the retrieval cues available in a given context. Ulti\u0002mately, knowledge reconstruction depends on the diagnostic\nvalue of cues, the degree to which cues help people recover\nparticular target information to the exclusion of competing\ncandidates (Nairne, 2002; Raaijmakers & Shiffrin, 1981). We\nmay wish to examine what a person has constructed or stored\nin mind, but it is impossible to directly assess the contents of\nstorage, per se. We can only ever examine what a person\nreconstructs given the available cues and context (Roediger,\n2000; Roediger & Guynn, 1996; Tulving & Pearlstone, 1966).\nThus, it is essential to consider retrieval processes in any anal\u0002ysis of learning.\nThe second crucial reason retrieval is important for learn\u0002ing is that learning is altered by the act of retrieval itself. Every\ntime a person retrieves knowledge, that knowledge is changed,\nbecause retrieving knowledge improves one’s ability to\nretrieve it again in the future (Karpicke & Roediger, 2007,\n2008; Karpicke & Zaromb, 2010). This is a feature of a func\u0002tional learning and memory system. Our minds are sensitive to\nthe likelihood that we will need knowledge at a future time,\nand if we retrieve something in the present, there is a good\nchance we will need to recover it again. The process of retrieval\nitself alters knowledge in anticipation of demands we may\nencounter in the future. Retrieval is therefore not only a tool\nfor assessing learning but also a tool for enhancing learning\n(Roediger & Karpicke, 2006a).\nRepeated Retrieval Enhances\nLong-Term Learning\nImagine you are studying for an upcoming exam. After you\nhave read through your notes or your textbook one time, what\nwould you want to do next? You have three options: You can\n(a) go back and restudy either all of the material or parts of it,\n(b) try to recall the material without restudying afterward, or\n(c) do something else. Which would you choose?\nWe (Karpicke, Butler, & Roediger, 2009) gave this ques\u0002tion to a large group of college students. Most students (57%)\nsaid they would reread their notes or textbook, and 21% said\nthey would do something else. Only 18% said they would\nattempt to recall material after reading it.1\nThe decision to\nrepeatedly read makes sense if we identify learning with pro\u0002cesses of encoding and constructing knowledge and consider\nretrieval to be only a way to assess prior learning. It stands to\nreason that more studying (i.e., more encoding and knowledge\nconstruction) should produce more learning, whereas retrieval\nshould measure learning but not produce it.\nWould students be better off repeatedly reading than engag\u0002ing in retrieval? We conducted an experiment with a design\nthat mirrored the question asked in the survey (Roediger &\nKarpicke, 2006b). Students read educational texts and recalled\nthem under one of three conditions. One group of students\nspent time repeatedly studying a text in four study periods. A\nsecond group read a text in three study periods and then\nrecalled it in one retrieval period (labeled SSSR), in which the\nstudents wrote down as many ideas from the text as they could\nrecall. A third group read the text during one study period and\nthen practiced recalling it during three consecutive repeated\nretrieval periods. Students did not reread the text or receive\nany feedback after any of the recall periods; they only prac\u0002ticed actively retrieving material.\nAt the end of the learning phase, the students made a judg\u0002ment of learning: a prediction of how well they would remem\u0002ber the material in the future. Then, one week later, students\nrecalled the material again to see how much they actually\nretained in the long term.\nFigure 1b shows students’ judgments of learning. The more\ntimes students repeatedly read the material, the better they\nbelieved they had learned it. However, Figure 1a shows that\nstudents’ actual learning exhibited the opposite pattern. The\nmore times students practiced actively retrieving the material,\nthe better they retained it in the long term. Students spent the\nsame amount of time experiencing the material in all three\nconditions, and students in the repeated-retrieval condition\nonly recalled and did not restudy the text, yet active retrieval\nproduced the best long-term retention (for further discussion\nof metacognitive awareness of the effects of retrieval practice,\nsee Karpicke & Grimaldi, 2012).\nReturning to the survey of student learning strategies\n(Karpicke et al., 2009), one might think the results would\nchange if we rewor",
        "html": "Current Directions in Psychological\nScience\n21(3) 157–163\n© The Author(s) 2012\nReprints and permission:\nsagepub.com/journalsPermissions.nav\nDOI: 10.1177/0963721412443552\nhttp://cdps.sagepub.com\nIf you know something, or if you have stored informa\u0002tion about an event from the distant past, and never use\nthat information, never think of it, your brain is func\u0002tionally equivalent to that of an otherwise identical brain\nthat does not “contain” that information.\n—Endel Tulving (1991)\nTo understand learning, it is essential to understand the pro\u0002cesses involved in retrieving and reconstructing knowledge. We\nmay think we know something, that our minds contain or pos\u0002sess some knowledge, but the only way to assess knowledge is\nby engaging in an act of retrieval. Differences in the ability to\nrecover knowledge may not stem from what is “stored” in our\nminds but rather from differences in the retrieval cues available\nin particular contexts. Given the fundamental importance of\nretrieval for understanding the process of learning, it is surpris\u0002ing that retrieval processes have not received more attention in\neducational research. Consider that over the past decade, many\ninfluential National Research Council books on how people\nlearn have contained no mention of retrieval (National Research\nCouncil, 2000, 2005a, 2005b).\nIt is essential to consider retrieval processes not only\nbecause they are central to understanding learning but also\nbecause the act of retrieval itself is a powerful tool for enhanc\u0002ing learning. Moreover, active retrieval does not merely\nproduce rote, transient learning; it produces meaningful, long\u0002term learning. The idea that retrieval is the centerpiece for\nunderstanding learning, coupled with the importance of active\nretrieval for producing learning, is referred to as retrieval\u0002based learning.\nLearning Based on the Design of the Mind\nWe often think of our minds as places in our heads, mental\nspaces or containers where we store knowledge. Roediger\n(1980) noted that for centuries, most metaphors used to\ndescribe mental processes have characterized the mind as a\nphysical space and knowledge as physical things in that\nspace—for example, by likening our minds to libraries filled\nwith books or cabinets loaded with files (see too Moscovitch,\n2007). In education, the metaphor of a physical building is\noften used to describe the mind and knowledge. Knowledge is\nconstructed by learners who actively build knowledge struc\u0002tures; researchers seek to understand the architecture of the\nmind; and instructors aid students by providing scaffolding for\nlearning.\nWhen minds are viewed as places for storing knowledge, it\nis natural to focus attention on processes involved in\nconstructing new knowledge in storage. Educational research\nCorresponding Author:\nJeffrey D. Karpicke, Department of Psychological Sciences, Purdue University,\n703 Third St., West Lafayette, IN 47907-2081\nE-mail: karpicke@purdue.edu\nRetrieval-Based Learning: Active\nRetrieval Promotes Meaningful Learning\nJeffrey D. Karpicke\nPurdue University\nAbstract\nRetrieval is the key process for understanding learning and for promoting learning, yet retrieval is not often granted the central\nrole it deserves. Learning is typically identified with the encoding or construction of knowledge, and retrieval is considered\nmerely the assessment of learning that occurred in a prior experience. The retrieval-based learning perspective outlined\nhere is grounded in the fact that all expressions of knowledge involve retrieval and depend on the retrieval cues available in\na given context. Further, every time a person retrieves knowledge, that knowledge is changed, because retrieving knowledge\nimproves one’s ability to retrieve it again in the future. Practicing retrieval does not merely produce rote, transient learning;\nit produces meaningful, long-term learning. Yet retrieval practice is a tool many students lack metacognitive awareness of\nand do not use as often as they should. Active retrieval is an effective but undervalued strategy for promoting meaningful\nlearning.\nKeywords\nretrieval processes, learning, education, metacognition, meaningful learning\n158 Karpicke\nand instructional practices have placed a premium on identify\u0002ing the best ways to encode knowledge and experiences.\nRetrieval processes, the processes involved in using available\ncues to actively reconstruct knowledge, have received less\nattention. There seems to be a tacit assumption that successful\nencoding or construction of knowledge, in itself, is sufficient\nto ensure learning.\nBasic research on learning and memory, however, has\nemphasized that retrieval must be considered in any analysis\nof learning. In part, this is because people do not store static,\nexact copies of experiences that are reproduced verbatim at\nretrieval. Instead, knowledge is actively reconstructed on the\nbasis of the present context and available retrieval cues\n(Bartlett, 1932; Neisser, 1967; see too Moscovitch, 2007;\nRoediger, 2000). The reconstructive nature of mind is revealed\nin the systematic errors people make in retrieving knowledge,\nerrors that verbatim recording devices would not make. The\npast never occurs again in its exact form, so a mental store\u0002house of copies of past experiences would be of little use.\nPeople instead have the ability to use the past to meet the\ndemands of the present by reconstructing knowledge rather\nthan reproducing it exactly.\nWhat people express when they reconstruct knowledge\ndepends on the retrieval cues available in a given context. Ulti\u0002mately, knowledge reconstruction depends on the diagnostic\nvalue of cues, the degree to which cues help people recover\nparticular target information to the exclusion of competing\ncandidates (Nairne, 2002; Raaijmakers & Shiffrin, 1981). We\nmay wish to examine what a person has constructed or stored\nin mind, but it is impossible to directly assess the contents of\nstorage, per se. We can only ever examine what a person\nreconstructs given the available cues and context (Roediger,\n2000; Roediger & Guynn, 1996; Tulving & Pearlstone, 1966).\nThus, it is essential to consider retrieval processes in any anal\u0002ysis of learning.\nThe second crucial reason retrieval is important for learn\u0002ing is that learning is altered by the act of retrieval itself. Every\ntime a person retrieves knowledge, that knowledge is changed,\nbecause retrieving knowledge improves one’s ability to\nretrieve it again in the future (Karpicke & Roediger, 2007,\n2008; Karpicke & Zaromb, 2010). This is a feature of a func\u0002tional learning and memory system. Our minds are sensitive to\nthe likelihood that we will need knowledge at a future time,\nand if we retrieve something in the present, there is a good\nchance we will need to recover it again. The process of retrieval\nitself alters knowledge in anticipation of demands we may\nencounter in the future. Retrieval is therefore not only a tool\nfor assessing learning but also a tool for enhancing learning\n(Roediger & Karpicke, 2006a).\nRepeated Retrieval Enhances\nLong-Term Learning\nImagine you are studying for an upcoming exam. After you\nhave read through your notes or your textbook one time, what\nwould you want to do next? You have three options: You can\n(a) go back and restudy either all of the material or parts of it,\n(b) try to recall the material without restudying afterward, or\n(c) do something else. Which would you choose?\nWe (Karpicke, Butler, & Roediger, 2009) gave this ques\u0002tion to a large group of college students. Most students (57%)\nsaid they would reread their notes or textbook, and 21% said\nthey would do something else. Only 18% said they would\nattempt to recall material after reading it.1\nThe decision to\nrepeatedly read makes sense if we identify learning with pro\u0002cesses of encoding and constructing knowledge and consider\nretrieval to be only a way to assess prior learning. It stands to\nreason that more studying (i.e., more encoding and knowledge\nconstruction) should produce more learning, whereas retrieval\nshould measure learning but not produce it.\nWould students be better off repeatedly reading than engag\u0002ing in retrieval? We conducted an experiment with a design\nthat mirrored the question asked in the survey (Roediger &\nKarpicke, 2006b). Students read educational texts and recalled\nthem under one of three conditions. One group of students\nspent time repeatedly studying a text in four study periods. A\nsecond group read a text in three study periods and then\nrecalled it in one retrieval period (labeled SSSR), in which the\nstudents wrote down as many ideas from the text as they could\nrecall. A third group read the text during one study period and\nthen practiced recalling it during three consecutive repeated\nretrieval periods. Students did not reread the text or receive\nany feedback after any of the recall periods; they only prac\u0002ticed actively retrieving material.\nAt the end of the learning phase, the students made a judg\u0002ment of learning: a prediction of how well they would remem\u0002ber the material in the future. Then, one week later, students\nrecalled the material again to see how much they actually\nretained in the long term.\nFigure 1b shows students’ judgments of learning. The more\ntimes students repeatedly read the material, the better they\nbelieved they had learned it. However, Figure 1a shows that\nstudents’ actual learning exhibited the opposite pattern. The\nmore times students practiced actively retrieving the material,\nthe better they retained it in the long term. Students spent the\nsame amount of time experiencing the material in all three\nconditions, and students in the repeated-retrieval condition\nonly recalled and did not restudy the text, yet active retrieval\nproduced the best long-term retention (for further discussion\nof metacognitive awareness of the effects of retrieval practice,\nsee Karpicke & Grimaldi, 2012).\nReturning to the survey of student learning strategies\n(Karpicke et al., 2009), one might think the results would\nchange if we rewor",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.alignmentforum.org/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without",
      "title": "How \"Discovering Latent Knowledge in Language Models Without Supervision\" Fits Into a Broader Alignment Scheme",
      "author": "Collin",
      "published_date": "2022-12-15T18:22:40.000Z",
      "content": {
        "text": "<div><div><h2>Introduction</h2><p>A few collaborators and I recently released a new paper: <a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge in Language Models Without Supervision</u></a>. For a quick summary of our paper, you can check out <a href=\"https://twitter.com/CollinBurns4/status/1600892261633785856\">this Twitter thread</a>.</p><p>In this post I will describe how I think the results and methods in our paper fit into a broader scalable alignment agenda. Unlike the paper, this post is explicitly aimed at an alignment audience and is mainly conceptual rather than empirical. </p><p><strong>Tl;dr:</strong> unsupervised methods are more scalable than supervised methods, deep learning has special structure that we can exploit for alignment, and we may be able to recover superhuman beliefs from deep learning representations in a totally unsupervised way.</p><p><i>Disclaimers: I have tried to make this post concise, at the cost of not making the full arguments for many of my claims; you should treat this as more of a rough sketch of my views rather than anything comprehensive. I also frequently change my mind – I’m usually more consistently excited about some of the broad intuitions but much less wedded to the details – and this of course just represents my current thinking on the topic. </i></p><h2>Problem</h2><p>I would feel pretty optimistic about alignment if – loosely speaking – we can get models to be robustly “honest” in a way that scales even to superhuman systems.<span><sup><a href=\"#fndaaiifyte3p\">[1]</a></sup></span> Moreover, I think a natural sub-problem that captures much or most of the difficulty here is: how can we make a language model like GPT-n “truthful” or “honest” in a way that is scalable? (For my purposes here I am also happy to make the assumption that GPT-n is not actively deceptive, in the sense that it does not actively try to obscure its representations.) </p><p>For example, imagine we train GPT-n to predict news articles conditioned on their dates of publication, and suppose the model ended up being able to predict future news articles very well. Or suppose we train GPT-n to predict the outcomes of particular actions in particular situations, all described (imperfectly by humans) in text. Then I would expect GPT-n would eventually (for large enough n) have a superhuman world model in an important sense. However, we don’t currently know how to recover the “beliefs” or “knowledge” of such a model even in principle.</p><p>A naive baseline for trying to make GPT-n truthful is to train it using human feedback to output text that human evaluators believe to be true. The basic issue with this is that human evaluators can’t assess complicated claims that a superhuman system might make. This could lead to either competitiveness problems (if GPT-n only outputs claims that humans can assess) or misalignment issues (if GPT-n outputs false claims because human evaluators can’t assess them correctly).</p><p>In many ways this problem is similar to <a href=\"https://www.alignmentforum.org/out?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8%2Fedit%3Fusp%3Dsharing\"><u>Eliciting Latent Knowledge (ELK)</u></a>, but unlike ELK <span>I am happy to take a “non-worst-case” empirical perspective in studying this problem. In particular, I suspect it will be very helpful – and possibly necessary – to use incidental empirical properties of deep learning systems, which often have a surprising amount of useful emergent structure (as I will discuss more under “Intuitions”). </span></p><p>On the other hand, if we want to study scalable alignment empirically, I think it’s very important for us to also have good reason to believe that our experiments will say something meaningful about future models – and it’s not immediately clear how to do that.</p><p>This raises the question: how do we even approach doing research on this sort of problem, methodologically? </p><h2>Methodology</h2><p>I worry that a lot of theoretical alignment work is either ungrounded or intractable, and I worry that a lot of empirical alignment work doesn’t address the core challenge of alignment in the sense that it won’t scale to superhuman models. I would really like to get the best of both worlds. </p><p>But what would it even mean to have an empirical result for current (sub-human or human-level) models and believe that that result will also apply to future (super-human) models? For example, if I have a method that seems to make GPT-3 truthful, what would make us believe that it should probably also scale to GPT-n for much larger n? </p><p>I think the biggest qualitative difference between GPT-3 and GPT-n (n &gt;&gt; 3) from an alignment perspective is that the GPT-3 is at most human-level, so human feedback is more or less sufficient for alignment, while GPT-n could be very superhuman, so naive human feedback is unlikely to be sufficient. In other words, I think the biggest technical challenge is to develop a method that can generalize even to settings that we can’t supervise. </p><p>How can we empirically test than an alignment scheme generalizes beyond settings that we can supervise? </p><p>I think there are at least a few reasonable strategies, which I may discuss in more detail in a future post, but I think one reasonable approach is to focus on <i>unsupervised</i> methods and show that those methods still generalize to the problems we care about. Unlike approaches that rely heavily on human feedback, from the perspective of an unsupervised method there is not necessarily any fundamental difference between “human-level” and “superhuman-level” models, so an unsupervised method working on human-level examples may provide meaningful evidence about it working on superhuman-level examples as well. </p><p>That said, I think it’s important to be very careful about what we mean by “unsupervised”. Using the outputs of a raw pretrained language model is “unsupervised” in the weak sense that such a model was pretrained on a corpus of text without any explicitly collected human labels, but <i>not</i> in the stronger sense that I care about. In particular, GPT-3’s outputs are still essentially just predicting what humans would say, which is unreliable; this is why we also avoid using model outputs in our paper. </p><p>A more subtle difficulty is that there can also be qualitative differences in the <i>features</i> learned by human-level and superhuman-level language models. For example, my guess is that current language models may represent “truth-like” features that very roughly corresponding to “what a human would say is true,” and that’s it. In contrast, I would guess that future superhuman language models may <i>also</i> represent a feature corresponding to “what the model thinks is actually true.” Since we ultimately really care about recovering “what a [future superhuman] model thinks is actually true,” this introduces a disanalogy between current models and future models that could be important. We don’t worry about this problem in our paper, but we discuss it more later on under “Scaling to GPT-n.”</p><p>This is all to point out that there can be important subtleties when comparing current and future models, but I think the basic point still remains: all else equal, unsupervised alignment methods are more likely to scale to superhuman models than methods that rely on human supervision.</p><p><span>I think the main reason unsupervised methods haven’t been seriously considered within alignment so far, as far as I can tell, is because of tractability concerns. It naively seems kind of impossible to get models to (say) be honest or truthful without any human supervision at all; what would such a method even look like?</span></p><p><span>To me, one of the main contributions of our paper is to show that this intuition is basically incorrect and to show that unsupervised methods can be surprisingly effective. </span></p><h2>Intuitions</h2><p>Why should this problem – identifying whether a model “thinks” an input is true or false without using any model outputs or human supervision, which is kind of like “unsupervised mind reading” – be possible at all? </p><p>I’ll sketch a handful of my intuitions here. In short: deep learning models learn useful features; deep learning features often have useful structure; and “truth” in particular has further useful structure. I’ll now elaborate on each of these in turn.</p><p>First, deep learning models generally learn representations that capture useful features; computer vision models <a href=\"https://arxiv.org/abs/1311.2901\"><u>famously learn</u></a> <a href=\"https://arxiv.org/pdf/1506.06579.pdf\"><u>edge detectors</u></a> because they are useful, language models learn <a href=\"https://aclanthology.org/N19-1419/\"><u>syntactic features</u></a> and <a href=\"https://arxiv.org/abs/1704.01444\"><u>sentiment features</u></a> because they are useful, and so on. Likewise, one hypothesis I have is that (a model’s “belief” of) the truth of an input will be a useful feature for models. For example, if a model sees a bunch of true text, then it should predict that future text will also likely be true, so inferring and representing the truth of that initial text should be useful for the model (similar to how inferring the sentiment of some text is useful for predicting subsequent text). If so, then language models may learn to internally represent “truth” in their internal activations if they’re capable enough.</p><p>Moreover, deep learning features often have useful structure. One articulation of this is Chris Olah’s “<a href=\"https://distill.pub/2020/circuits/zoom-in/\"><i><u>Features are the fundamental unit of neural networks. They correspond to directions</u></i></a>.” If this is basically true, this would suggest that useful features like the truth of an input may be represented in a relatively simple way in a model’s representation space – e.g",
        "html": "<div><div><h2>Introduction</h2><p>A few collaborators and I recently released a new paper: <a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge in Language Models Without Supervision</u></a>. For a quick summary of our paper, you can check out <a href=\"https://twitter.com/CollinBurns4/status/1600892261633785856\">this Twitter thread</a>.</p><p>In this post I will describe how I think the results and methods in our paper fit into a broader scalable alignment agenda. Unlike the paper, this post is explicitly aimed at an alignment audience and is mainly conceptual rather than empirical. </p><p><strong>Tl;dr:</strong> unsupervised methods are more scalable than supervised methods, deep learning has special structure that we can exploit for alignment, and we may be able to recover superhuman beliefs from deep learning representations in a totally unsupervised way.</p><p><i>Disclaimers: I have tried to make this post concise, at the cost of not making the full arguments for many of my claims; you should treat this as more of a rough sketch of my views rather than anything comprehensive. I also frequently change my mind – I’m usually more consistently excited about some of the broad intuitions but much less wedded to the details – and this of course just represents my current thinking on the topic. </i></p><h2>Problem</h2><p>I would feel pretty optimistic about alignment if – loosely speaking – we can get models to be robustly “honest” in a way that scales even to superhuman systems.<span><sup><a href=\"#fndaaiifyte3p\">[1]</a></sup></span> Moreover, I think a natural sub-problem that captures much or most of the difficulty here is: how can we make a language model like GPT-n “truthful” or “honest” in a way that is scalable? (For my purposes here I am also happy to make the assumption that GPT-n is not actively deceptive, in the sense that it does not actively try to obscure its representations.) </p><p>For example, imagine we train GPT-n to predict news articles conditioned on their dates of publication, and suppose the model ended up being able to predict future news articles very well. Or suppose we train GPT-n to predict the outcomes of particular actions in particular situations, all described (imperfectly by humans) in text. Then I would expect GPT-n would eventually (for large enough n) have a superhuman world model in an important sense. However, we don’t currently know how to recover the “beliefs” or “knowledge” of such a model even in principle.</p><p>A naive baseline for trying to make GPT-n truthful is to train it using human feedback to output text that human evaluators believe to be true. The basic issue with this is that human evaluators can’t assess complicated claims that a superhuman system might make. This could lead to either competitiveness problems (if GPT-n only outputs claims that humans can assess) or misalignment issues (if GPT-n outputs false claims because human evaluators can’t assess them correctly).</p><p>In many ways this problem is similar to <a href=\"https://www.alignmentforum.org/out?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8%2Fedit%3Fusp%3Dsharing\"><u>Eliciting Latent Knowledge (ELK)</u></a>, but unlike ELK <span>I am happy to take a “non-worst-case” empirical perspective in studying this problem. In particular, I suspect it will be very helpful – and possibly necessary – to use incidental empirical properties of deep learning systems, which often have a surprising amount of useful emergent structure (as I will discuss more under “Intuitions”). </span></p><p>On the other hand, if we want to study scalable alignment empirically, I think it’s very important for us to also have good reason to believe that our experiments will say something meaningful about future models – and it’s not immediately clear how to do that.</p><p>This raises the question: how do we even approach doing research on this sort of problem, methodologically? </p><h2>Methodology</h2><p>I worry that a lot of theoretical alignment work is either ungrounded or intractable, and I worry that a lot of empirical alignment work doesn’t address the core challenge of alignment in the sense that it won’t scale to superhuman models. I would really like to get the best of both worlds. </p><p>But what would it even mean to have an empirical result for current (sub-human or human-level) models and believe that that result will also apply to future (super-human) models? For example, if I have a method that seems to make GPT-3 truthful, what would make us believe that it should probably also scale to GPT-n for much larger n? </p><p>I think the biggest qualitative difference between GPT-3 and GPT-n (n &gt;&gt; 3) from an alignment perspective is that the GPT-3 is at most human-level, so human feedback is more or less sufficient for alignment, while GPT-n could be very superhuman, so naive human feedback is unlikely to be sufficient. In other words, I think the biggest technical challenge is to develop a method that can generalize even to settings that we can’t supervise. </p><p>How can we empirically test than an alignment scheme generalizes beyond settings that we can supervise? </p><p>I think there are at least a few reasonable strategies, which I may discuss in more detail in a future post, but I think one reasonable approach is to focus on <i>unsupervised</i> methods and show that those methods still generalize to the problems we care about. Unlike approaches that rely heavily on human feedback, from the perspective of an unsupervised method there is not necessarily any fundamental difference between “human-level” and “superhuman-level” models, so an unsupervised method working on human-level examples may provide meaningful evidence about it working on superhuman-level examples as well. </p><p>That said, I think it’s important to be very careful about what we mean by “unsupervised”. Using the outputs of a raw pretrained language model is “unsupervised” in the weak sense that such a model was pretrained on a corpus of text without any explicitly collected human labels, but <i>not</i> in the stronger sense that I care about. In particular, GPT-3’s outputs are still essentially just predicting what humans would say, which is unreliable; this is why we also avoid using model outputs in our paper. </p><p>A more subtle difficulty is that there can also be qualitative differences in the <i>features</i> learned by human-level and superhuman-level language models. For example, my guess is that current language models may represent “truth-like” features that very roughly corresponding to “what a human would say is true,” and that’s it. In contrast, I would guess that future superhuman language models may <i>also</i> represent a feature corresponding to “what the model thinks is actually true.” Since we ultimately really care about recovering “what a [future superhuman] model thinks is actually true,” this introduces a disanalogy between current models and future models that could be important. We don’t worry about this problem in our paper, but we discuss it more later on under “Scaling to GPT-n.”</p><p>This is all to point out that there can be important subtleties when comparing current and future models, but I think the basic point still remains: all else equal, unsupervised alignment methods are more likely to scale to superhuman models than methods that rely on human supervision.</p><p><span>I think the main reason unsupervised methods haven’t been seriously considered within alignment so far, as far as I can tell, is because of tractability concerns. It naively seems kind of impossible to get models to (say) be honest or truthful without any human supervision at all; what would such a method even look like?</span></p><p><span>To me, one of the main contributions of our paper is to show that this intuition is basically incorrect and to show that unsupervised methods can be surprisingly effective. </span></p><h2>Intuitions</h2><p>Why should this problem – identifying whether a model “thinks” an input is true or false without using any model outputs or human supervision, which is kind of like “unsupervised mind reading” – be possible at all? </p><p>I’ll sketch a handful of my intuitions here. In short: deep learning models learn useful features; deep learning features often have useful structure; and “truth” in particular has further useful structure. I’ll now elaborate on each of these in turn.</p><p>First, deep learning models generally learn representations that capture useful features; computer vision models <a href=\"https://arxiv.org/abs/1311.2901\"><u>famously learn</u></a> <a href=\"https://arxiv.org/pdf/1506.06579.pdf\"><u>edge detectors</u></a> because they are useful, language models learn <a href=\"https://aclanthology.org/N19-1419/\"><u>syntactic features</u></a> and <a href=\"https://arxiv.org/abs/1704.01444\"><u>sentiment features</u></a> because they are useful, and so on. Likewise, one hypothesis I have is that (a model’s “belief” of) the truth of an input will be a useful feature for models. For example, if a model sees a bunch of true text, then it should predict that future text will also likely be true, so inferring and representing the truth of that initial text should be useful for the model (similar to how inferring the sentiment of some text is useful for predicting subsequent text). If so, then language models may learn to internally represent “truth” in their internal activations if they’re capable enough.</p><p>Moreover, deep learning features often have useful structure. One articulation of this is Chris Olah’s “<a href=\"https://distill.pub/2020/circuits/zoom-in/\"><i><u>Features are the fundamental unit of neural networks. They correspond to directions</u></i></a>.” If this is basically true, this would suggest that useful features like the truth of an input may be represented in a relatively simple way in a model’s representation space – e.g",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "IntroductionA few collaborators and I recently released a new paper:Discovering Latent Knowledge in Language Models Without Supervision. For a quick summary of our paper, you can check outthis Twitter thread.In this post I will describe how I think the results and methods in our paper fit into a broader scalable alignment agenda. Unlike the paper, this post is explicitly aimed at an alignment audience and is mainly conceptual rather than empirical.Tl;dr:unsupervised methods are more scalable tha",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "IntroductionA few collaborators and I recently released a new paper:Discovering Latent Knowledge in Language Models Without Supervision. For a quick summary of our paper, you can check outthis Twitter thread.In this post I will describe how I think the results and methods in our paper fit into a broader scalable alignment agenda. Unlike the paper, this post is explicitly aimed at an alignment audience and is mainly conceptual rather than empirical.Tl;dr:unsupervised methods are more scalable tha",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Problem",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Methodology",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Intuitions",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://bpb-us-e2.wpmucdn.com/sites.utdallas.edu/dist/2/1242/files/2023/07/Contributions-of-processing-ability-and-knowledge-to-verbal-memory-tasks-across-the-adult-life-span.pdf",
      "title": "pQJA076-03.qxd",
      "author": "Douglas Burns",
      "published_date": null,
      "content": {
        "text": "Contributions of processing ability and\nknowledge to verbal memory tasks across\nthe adult life-span\nTrey Hedden\nStanford University, Stanford, CA, USA\nGary Lautenschlager\nUniversity of Georgia, Athens, GA, USA\nDenise C. Park\nUniversity of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA\nThis study investigated the relationships of processing capacity and knowledge to memory\nmeasures that varied in retrieval difficulty and reliance on verbal knowledge in an adult life-span\nsample (N \u0001 341). It was hypothesized that processing ability (speed and working memory) would\nhave the strongest relationship to tasks requiring active retrieval and that knowledge (vocabulary\nability) would be related to verbal fluency and cued recall, as participants relied upon verbal\nknowledge to retrieve category items (fluency) or develop associations (cued recall). Measurement\nand structural equation models were developed for the entire sample and separately for younger\n(aged 20–54 years, n \u0001 168) and older (aged 55–92 years, n \u0001 173) subgroups. In accordance with\nthe hypotheses, processing ability was found to be most highly related to free recall, with addi\u0002tional significant relationships to cued recall, verbal fluency, and recognition. Knowledge was\nfound to be significantly related only to verbal fluency and to cued recall. Moreover, knowledge\nwas more important for older than for younger adults in mediating variance in cued recall, sug\u0002gesting that older adults may use age-related increases in knowledge to partially compensate for\nprocessing declines when environmental support is available in memory tasks.\nIn the study of cognitive ageing, perhaps the most well known finding is that processing\nability declines with advancing age even in the absence of pathology (Park, Lautenschlager,\nHedden, Davidson, Smith, & Smith, 2002; Rabbitt & Lowe, 2000; Salthouse, 1996). Processing\n© 2005 The Experimental Psychology Society\nhttp://www.tandf.co.uk/journals/pp/02724987.html DOI:10.1080/02724980443000179\nCorrespondence should be addressed to Trey Hedden, Department of Psychology, 434 Jordan Hall, Building\n420, Stanford University, Stanford, CA 94305-2130, USA. Email: hedden@psych.stanford.edu\nTrey Hedden is supported by an NRSA fellowship from the National Institutes of Health. This research was\nsupported by grant R01-AG06265 from the National Institute on Aging to Denise C. Park. The authors thank Ki\nGoosens.\nTHE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY\n2005, 58A (1), 169–190\n170 HEDDEN, LAUTENSCHLAGER, PARK\nability refers to one’s capacity for efficiently executing mechanisms of controlled attention\nthat are invoked to perform tasks requiring rapid selection of information, switching among\nmultiple task goals, and actively maintaining multiple representations (e.g., Kane, Bleckley,\nConway, & Engle, 2001, Meyer & Kieras, 1997). Common measures of processing ability,\nsuch as speed of processing and working memory capacity measures, predict age-related\nchanges and individual differences in fluid intelligence and in long-term memory (Engle,\nKane, & Tuholski, 1999a; Engle, Tuholski, Laughlin, & Conway, 1999b; Park et al., 2002;\nPark et al., 1996; Verhaeghen & Salthouse, 1997). In contrast to the declines in processing\nabilities, knowledge, as measured by vocabulary ability or semantic knowledge, tends to\nremain stable across the adult life span or show age-related increases until very late in life\n(Lindenberger & Baltes, 1997; Park et al., 2002; Salthouse, 1993a; Schaie, 1994, 1996).\nAlthough there are abundant demonstrations of how decreases in measures of processing\ncapacity such as speed and working memory predict performance on a range of higher order\ntasks that include long-term memory and reasoning, less is known about the relationship of\nknowledge to such tasks. There is evidence that knowledge may be a more important com\u0002ponent of performance in late adulthood compared to earlier adulthood when processing\ncapacity is at its peak. For example, findings indicate that knowledge and expertise in partic\u0002ular domains gained with age aid performance in solving crossword puzzles (Hambrick,\nSalthouse, & Meinz, 1999), in memory for music (Meinz & Salthouse, 1998), and in playing\nbridge and chess (Charness & Bosman, 1990). Indeed, older adults are often able to main\u0002tain a high level of functioning in familiar tasks of everyday living even while displaying\ndeclines in processing ability in laboratory tests (Allaire & Marsiske, 2002; Charness, 2000;\nPark, 1992; Park & Gutchess, 2000). Nevertheless, increased knowledge and expertise do not\nalways protect against age-related processing declines within a domain of expertise (Meinz &\nSalthouse, 1998), nor do they slow the rate of decline in general processing abilities (Hambrick\net al., 1999). To date, the findings suggest a complex relationship among knowledge, processing\ncapacity, and performance, with knowledge potentially playing an increased role in aiding\ntask performance with advancing age. The interplay between processing ability and knowledge\nacross the lifespan should be most apparent on tasks where both processing capacity and\nknowledge can be relied upon to support performance.\nThe environmental support hypothesis proposed by Craik (1983) posited that as self\u0002initiated processing ability declines with age, environmental support from external cues and\ninternal habits plays an increasingly important role in supporting cognitive behaviours.\nEnvironmental support is here conceptualized as the prompting of internal processes by\nexternal cues. The presence of extensive cues does not itself provide environmental support,\nbut must be accompanied by the successful prompting of task-relevant cognitive processes.\nWe hypothesize that existing verbal knowledge (such as an extensive vocabulary), when\nprompted by cues contained in a task context, may provide environmental support for older\nadults and may partially compensate for processing declines on some memory tasks. For\nolder adults, who have impoverished processing ability but a wealth of knowledge, the envi\u0002ronmental support available in a memory task may be a particularly important factor in\ndetermining success or failure (Craik & Anderson, 1999; Hess, Flannagan, & Tate, 1993;\nNaveh-Benjamin, Craik, & Ben-Shaul, 2002).\nIn the present study, we examined the contributions of verbal knowledge and processing\nabilities (speed and working memory) to a range of commonly used measures of verbal\nPROCESS AND KNOWLEDGE IN VERBAL MEMORY 171\nmemory: free recall, cued recall, and recognition. These measures, according to Craik and\nByrd (1982), vary in the amount of self-initiated processing required to perform them.\nVerbal free recall tasks, in which a list of individual words is to be recalled, provide few\nretrieval cues for environmental support, and hence age differences are particularly large on\nthese tasks (Anderson, Craik, & Naveh-Benjamin, 1998b; Craik & Byrd, 1982; Craik, Byrd,\n& Swanson, 1987; Park, Smith, Dudley, & Lafronza, 1989). Although one might expect that\nknowledge would assist in developing associations even among unrelated words during the\nencoding phase of free recall, Craik and Anderson (1999) implicated retrieval, rather than\nencoding, processes as the primary source of deficits experienced by older adults in remem\u0002bering contextual associations. The absence of retrieval cues in a free recall task may there\u0002fore be particularly detrimental for older adults, who cannot effectively apply their knowledge\nwithout external cues and must rely instead on their diminishing processing ability. Suggesting\nthe importance of processing ability to free recall, Park et al. (1996, 2002) reported strong\nassociations between measures of speed and working memory to free recall.\nCued recall tasks, in which associations between paired cues and target words are mem\u0002orized, also show large age differences (Anderson et al., 1998b; Craik et al., 1987; Park et al.,\n1989), but provide more of an opportunity for the application of verbal knowledge, particu\u0002larly when the cues and targets are meaningfully associated with one another (Nelson &\nMcEvoy, 2002). We hypothesized that individuals with large vocabularies may be able to use\ntheir superior knowledge to generate more or better associations to connect targets and cues.\nThus, we would expect that both processing abilities and verbal knowledge would mediate\nvariance on a cued recall task.\nAccording to Craik and colleagues (Craik & Byrd, 1982; Craik et al., 1987), verbal recog\u0002nition tasks provide the most environmental support through retrieval cues, as the studied\nword is provided at retrieval, and a participant need only accept or reject the word as a studied\nitem. In support of this hypothesis, age differences in memory are reduced in cued recall tasks\ncompared to free recall tasks (Anderson et al., 1998b; Craik et al., 1987), and recognition tasks\ndisplay comparatively small age differences (Anderson et al., 1998b; Kausler, 1994, pp. 249–253;\nSpencer & Raz, 1995). Despite the small age differences in recognition memory, it seems\nunlikely that verbal knowledge is the mechanism mitigating age-related declines, as recogni\u0002tion tasks provide little opportunity for the application of verbal knowledge. Typically, all the\nwords presented as targets and lures in a recognition task are known to the participant\nthrough extraexperimental knowledge, so that the only distinction among targets and lures is\nthat targets have recently been presented on the study list. Rather than relying on processing\nability or verbal knowledge, recognition relies heavily on familiarity processes, in which\njudgements are based on perceptual fluency or relative activation of items in memory\n(Anderson, Bothell, Lebiere, & Matessa, 1998a; Johnston, Dark, & Jacoby, 1985; Kausler,\n1994, p. 253). These familiarity processes have been found to occur prior to the influences of\nrecollection in recognition tasks (McElree, Dolan, & Jacoby, ",
        "html": "Contributions of processing ability and\nknowledge to verbal memory tasks across\nthe adult life-span\nTrey Hedden\nStanford University, Stanford, CA, USA\nGary Lautenschlager\nUniversity of Georgia, Athens, GA, USA\nDenise C. Park\nUniversity of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA\nThis study investigated the relationships of processing capacity and knowledge to memory\nmeasures that varied in retrieval difficulty and reliance on verbal knowledge in an adult life-span\nsample (N \u0001 341). It was hypothesized that processing ability (speed and working memory) would\nhave the strongest relationship to tasks requiring active retrieval and that knowledge (vocabulary\nability) would be related to verbal fluency and cued recall, as participants relied upon verbal\nknowledge to retrieve category items (fluency) or develop associations (cued recall). Measurement\nand structural equation models were developed for the entire sample and separately for younger\n(aged 20–54 years, n \u0001 168) and older (aged 55–92 years, n \u0001 173) subgroups. In accordance with\nthe hypotheses, processing ability was found to be most highly related to free recall, with addi\u0002tional significant relationships to cued recall, verbal fluency, and recognition. Knowledge was\nfound to be significantly related only to verbal fluency and to cued recall. Moreover, knowledge\nwas more important for older than for younger adults in mediating variance in cued recall, sug\u0002gesting that older adults may use age-related increases in knowledge to partially compensate for\nprocessing declines when environmental support is available in memory tasks.\nIn the study of cognitive ageing, perhaps the most well known finding is that processing\nability declines with advancing age even in the absence of pathology (Park, Lautenschlager,\nHedden, Davidson, Smith, & Smith, 2002; Rabbitt & Lowe, 2000; Salthouse, 1996). Processing\n© 2005 The Experimental Psychology Society\nhttp://www.tandf.co.uk/journals/pp/02724987.html DOI:10.1080/02724980443000179\nCorrespondence should be addressed to Trey Hedden, Department of Psychology, 434 Jordan Hall, Building\n420, Stanford University, Stanford, CA 94305-2130, USA. Email: hedden@psych.stanford.edu\nTrey Hedden is supported by an NRSA fellowship from the National Institutes of Health. This research was\nsupported by grant R01-AG06265 from the National Institute on Aging to Denise C. Park. The authors thank Ki\nGoosens.\nTHE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY\n2005, 58A (1), 169–190\n170 HEDDEN, LAUTENSCHLAGER, PARK\nability refers to one’s capacity for efficiently executing mechanisms of controlled attention\nthat are invoked to perform tasks requiring rapid selection of information, switching among\nmultiple task goals, and actively maintaining multiple representations (e.g., Kane, Bleckley,\nConway, & Engle, 2001, Meyer & Kieras, 1997). Common measures of processing ability,\nsuch as speed of processing and working memory capacity measures, predict age-related\nchanges and individual differences in fluid intelligence and in long-term memory (Engle,\nKane, & Tuholski, 1999a; Engle, Tuholski, Laughlin, & Conway, 1999b; Park et al., 2002;\nPark et al., 1996; Verhaeghen & Salthouse, 1997). In contrast to the declines in processing\nabilities, knowledge, as measured by vocabulary ability or semantic knowledge, tends to\nremain stable across the adult life span or show age-related increases until very late in life\n(Lindenberger & Baltes, 1997; Park et al., 2002; Salthouse, 1993a; Schaie, 1994, 1996).\nAlthough there are abundant demonstrations of how decreases in measures of processing\ncapacity such as speed and working memory predict performance on a range of higher order\ntasks that include long-term memory and reasoning, less is known about the relationship of\nknowledge to such tasks. There is evidence that knowledge may be a more important com\u0002ponent of performance in late adulthood compared to earlier adulthood when processing\ncapacity is at its peak. For example, findings indicate that knowledge and expertise in partic\u0002ular domains gained with age aid performance in solving crossword puzzles (Hambrick,\nSalthouse, & Meinz, 1999), in memory for music (Meinz & Salthouse, 1998), and in playing\nbridge and chess (Charness & Bosman, 1990). Indeed, older adults are often able to main\u0002tain a high level of functioning in familiar tasks of everyday living even while displaying\ndeclines in processing ability in laboratory tests (Allaire & Marsiske, 2002; Charness, 2000;\nPark, 1992; Park & Gutchess, 2000). Nevertheless, increased knowledge and expertise do not\nalways protect against age-related processing declines within a domain of expertise (Meinz &\nSalthouse, 1998), nor do they slow the rate of decline in general processing abilities (Hambrick\net al., 1999). To date, the findings suggest a complex relationship among knowledge, processing\ncapacity, and performance, with knowledge potentially playing an increased role in aiding\ntask performance with advancing age. The interplay between processing ability and knowledge\nacross the lifespan should be most apparent on tasks where both processing capacity and\nknowledge can be relied upon to support performance.\nThe environmental support hypothesis proposed by Craik (1983) posited that as self\u0002initiated processing ability declines with age, environmental support from external cues and\ninternal habits plays an increasingly important role in supporting cognitive behaviours.\nEnvironmental support is here conceptualized as the prompting of internal processes by\nexternal cues. The presence of extensive cues does not itself provide environmental support,\nbut must be accompanied by the successful prompting of task-relevant cognitive processes.\nWe hypothesize that existing verbal knowledge (such as an extensive vocabulary), when\nprompted by cues contained in a task context, may provide environmental support for older\nadults and may partially compensate for processing declines on some memory tasks. For\nolder adults, who have impoverished processing ability but a wealth of knowledge, the envi\u0002ronmental support available in a memory task may be a particularly important factor in\ndetermining success or failure (Craik & Anderson, 1999; Hess, Flannagan, & Tate, 1993;\nNaveh-Benjamin, Craik, & Ben-Shaul, 2002).\nIn the present study, we examined the contributions of verbal knowledge and processing\nabilities (speed and working memory) to a range of commonly used measures of verbal\nPROCESS AND KNOWLEDGE IN VERBAL MEMORY 171\nmemory: free recall, cued recall, and recognition. These measures, according to Craik and\nByrd (1982), vary in the amount of self-initiated processing required to perform them.\nVerbal free recall tasks, in which a list of individual words is to be recalled, provide few\nretrieval cues for environmental support, and hence age differences are particularly large on\nthese tasks (Anderson, Craik, & Naveh-Benjamin, 1998b; Craik & Byrd, 1982; Craik, Byrd,\n& Swanson, 1987; Park, Smith, Dudley, & Lafronza, 1989). Although one might expect that\nknowledge would assist in developing associations even among unrelated words during the\nencoding phase of free recall, Craik and Anderson (1999) implicated retrieval, rather than\nencoding, processes as the primary source of deficits experienced by older adults in remem\u0002bering contextual associations. The absence of retrieval cues in a free recall task may there\u0002fore be particularly detrimental for older adults, who cannot effectively apply their knowledge\nwithout external cues and must rely instead on their diminishing processing ability. Suggesting\nthe importance of processing ability to free recall, Park et al. (1996, 2002) reported strong\nassociations between measures of speed and working memory to free recall.\nCued recall tasks, in which associations between paired cues and target words are mem\u0002orized, also show large age differences (Anderson et al., 1998b; Craik et al., 1987; Park et al.,\n1989), but provide more of an opportunity for the application of verbal knowledge, particu\u0002larly when the cues and targets are meaningfully associated with one another (Nelson &\nMcEvoy, 2002). We hypothesized that individuals with large vocabularies may be able to use\ntheir superior knowledge to generate more or better associations to connect targets and cues.\nThus, we would expect that both processing abilities and verbal knowledge would mediate\nvariance on a cued recall task.\nAccording to Craik and colleagues (Craik & Byrd, 1982; Craik et al., 1987), verbal recog\u0002nition tasks provide the most environmental support through retrieval cues, as the studied\nword is provided at retrieval, and a participant need only accept or reject the word as a studied\nitem. In support of this hypothesis, age differences in memory are reduced in cued recall tasks\ncompared to free recall tasks (Anderson et al., 1998b; Craik et al., 1987), and recognition tasks\ndisplay comparatively small age differences (Anderson et al., 1998b; Kausler, 1994, pp. 249–253;\nSpencer & Raz, 1995). Despite the small age differences in recognition memory, it seems\nunlikely that verbal knowledge is the mechanism mitigating age-related declines, as recogni\u0002tion tasks provide little opportunity for the application of verbal knowledge. Typically, all the\nwords presented as targets and lures in a recognition task are known to the participant\nthrough extraexperimental knowledge, so that the only distinction among targets and lures is\nthat targets have recently been presented on the study list. Rather than relying on processing\nability or verbal knowledge, recognition relies heavily on familiarity processes, in which\njudgements are based on perceptual fluency or relative activation of items in memory\n(Anderson, Bothell, Lebiere, & Matessa, 1998a; Johnston, Dark, & Jacoby, 1985; Kausler,\n1994, p. 253). These familiarity processes have been found to occur prior to the influences of\nrecollection in recognition tasks (McElree, Dolan, & Jacoby, ",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.oneusefulthing.org/p/latent-expertise-everyone-is-in-r",
      "title": "Latent Expertise: Everyone is in R&D",
      "author": "Ethan Mollick",
      "published_date": "2024-06-20T11:23:55.000Z",
      "content": {
        "text": "<div><div><p><span>AI discussions often fall into a weird dichotomy - it is either all “hype” or else the age of the superhuman machines is imminent. At least for now, that is a false dichotomy. There are areas where AI is </span><a href=\"https://www.oneusefulthing.org/p/superhuman\">better than an expert human</a><span> at particular tasks, and areas where it is completely useless. Instead of blanket statements, we should focus on specifics: we know that LLMs, without further development, are already useful as a co-intelligence that greatly improves </span><a href=\"https://www.oneusefulthing.org/p/signs-and-portents\">human performance</a><span> (in innovation, productivity, coding, and more), but we also have yet to figure out every strength and weakness. </span></p><p><span>The first wave of AI adoption was about individual use, and that seems to have been a </span><a href=\"https://www.microsoft.com/en-us/worklab/work-trend-index/ai-at-work-is-here-now-comes-the-hard-part\">huge success</a><span>, with some of the fastest adoption rates in history for a new technology. But the second wave, putting AI to work, is going to involve</span><a href=\"https://www.oneusefulthing.org/p/reshaping-the-tree-rebuilding-organizations\"> integrating it into organizations</a><span><a href=\"https://www.oneusefulthing.org/p/latent-expertise-everyone-is-in-r#footnote-1-144932334\">1</a></span><span>. This will take longer and will be the key to true productivity growth. After talking to many companies, however, I see many of them following the same well-trod path, viewing AI as an information technology that can be used for cost savings. I think this is a mistake. To see why, let’s reconsider the old analogy comparing AI to the Industrial Revolution.</span></p><p><span>One of the most fascinating things about the Industrial Revolution in England is how much progress happened in so many industries - from textiles to medicine to metallurgy to instrument-making - in a short time. A lot of credit is given to the great inventors like James Watt and his steam engine, </span><a href=\"https://www.nber.org/system/files/chapters/c12364/c12364.pdf\">but economists have suggested that these great inventors alone were not enough</a><span>. Instead, their work needed to be adjusted and made real by people who altered the technology for different industries and factories, and then further refined by the people who implemented it for specific uses. Without a base of skilled craftsman, mechanics, and engineers working in many mills and factories, the Industrial Revolution would have just happened in theory.</span></p><p>Yet I worry that the lesson of the Industrial Revolution is being lost in AI implementations at companies. Many leaders seem to have adopted a view that the main purpose of technology is efficiency. Following the Law of the Hammer (“to a hammer every problem looks like a nail”), they see AI as a cost-cutting mechanism. Any efficiency gains must be turned into cost savings, even before anyone in the organization figures out what AI is good for. It is as if, after getting access to the steam engine in the 1700s, every manufacturer decided to keep production and quality the same, and just fire staff in response to new-found efficiency, rather than building world-spanning companies by expanding their outputs.</p><p>Starting with centralized systems built for efficiency has other drawbacks besides strangling growth. Right now, nobody - from consultants to typical software vendors - has universal answers about how to use AI to unlock new opportunities in any particular industry. Companies that turn to centralized solutions run as typical IT projects are therefore unlikely to find breakthrough ideas, at least not yet. They first need to understand the value of AI in order to use it. </p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94369840-31a7-443f-adf9-5697e497af26_1376x864.png\"><div></div></a></figure></div><p>And to understand the value of AI, they need to do R&amp;D. Since AI doesn't work like traditional software, but more like a person (even though it isn't one), there is no reason to suspect that the IT department has the best AI prompters, nor that it has any particular insight into the best uses of AI inside an organization. IT certainly plays a role, but the actual use cases will come from workers and managers who find opportunities to use AI to help them with their job. In fact, for large companies, the source of any real advantage in AI will come from the expertise of their employees, which is needed to unlock the expertise latent in AI.</p><p><span>Large Language Models are </span><a href=\"https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox\">forgetful foxes in a Berlinian sense</a><span>: they know many things, imperfectly. Oddly, we don’t actually </span><em>know </em><span>everything they know, in part because training data is kept secret, but also because it isn’t always clear what LLMs learn from their training data. Yet it is clear that they do have expertise hidden in their latent space - they </span><a href=\"https://x.com/emollick/status/1793046320812310709\">outperform </a><span>doctors at diagnosing diseases in some </span><a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">studies</a><span>, and exceed more doctors in providing empathetic replies to patients, even though those were not expected uses of the system.</span></p><p><span>Unlocking the expertise latent in AI is, for now, a job for experts. There are multiple reasons for this. The first is that experts can easily judge whether work in their field is good or bad, and in what ways. Take, for example, two topics we are familiar with: teaching people to apply frameworks and interactive tutoring. We can give the AI two simple prompts, one to get the AI to act as a tutor and the other to get it provide frameworks for solving problems (if you want to try the more advanced versions of these prompts that actually work well, you can try the </span><a href=\"https://chatgpt.com/g/g-r6eogQdIh-ai-tutor-new\">Tutor GPT</a><span> and </span><a href=\"https://chatgpt.com/g/g-vZ7SgKBOh-framework-finder\">Frameworks GPT</a><span>:</span></p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25368729-76d6-48bb-a4fd-d7959e2bb888_2155x1412.png\"><div></div></a></figure></div><p>Since we know something about these topics, we can instantly tell that the answer on frameworks, while not amazing, is not terrible. It suggests a number of possible approaches, and, in the text that I cut out of the image, how to apply them. The frameworks are appropriate, and, if the goal is to make you think about the problem, this is not a bad start. The tutor prompt is a different matter. Good tutors need to interact with the student, not assume knowledge. They should not merely throw information at the student but adapt to their abilities and meet them where they are. And they should figure out what you know, not ask you what you think you need to know. The tutor fails at all of these points.</p><p><span>But because we know what good tutoring does, and what the gaps of the AI were, we can easily determine what behaviors we needed to suppress or activate in order for the AI to act as a good tutor. We also know how to teach other people to be a tutor, a skill that is remarkably transferrable to AI but just writing those instructions as a more elaborate prompt. The result is a solid tutor from a prompt alone (</span><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4802463\">see it here</a><span>). In fact, Google</span><a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\"> has tested a version of our prompt</a><span> against a fine-tuned educational model they built, and found that they have statistically similar performance across most dimensions.</span></p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0bfabdf-92b6-47a9-93d2-746a52f33715_523x350.png\"><div></div></a></figure></div><p><span>This also illustrates another reason why experts are best at using AI for now. As one of my PhD advisors, Eric von Hippel, </span><a href=\"https://evhippel.mit.edu/teaching/\">pointed out</a><span>: R&amp;D is very expensive because it involves lots of trial and error, but when you are doing a task all the time, trial and error is cheap and easy. That is why a surprisingly large percentage of important innovation comes, not from formal R&amp;D labs, but rather from people figuring out how to solve their own problems. Learning by doing is cheap if you are already doing.</span></p><p>Experts thus have many advantages. They are better able to see through LLM errors and hallucinations; they are better judges of AI output in their area of interest; they are better able to instruct the AI to do the required job; and they have the opportunity for more trial and error. That lets them unlock the latent expertise within LLMs in ways that others could not.</p><p><span>For example, LLMs can write solid job descriptions, but they often sound very generic. Is there the possibility that the AI could do something more? Dan Shapiro figured out how. He is a serial entrepreneur and </span><a href=\"https://glowforge.com/m/founders\">co-founder of Glowforge</a><span>, which makes cool laser carving tools. Dan is an expert at building culture in organizations and </span><a href=\"https://blog.glowforge.com/1967-2/\">he credits his job descriptions as one of his secret weapons for attracting talent</a><span> (plus he is hiring). But handcrafting these “job descriptions as love letter",
        "html": "<div><div><p><span>AI discussions often fall into a weird dichotomy - it is either all “hype” or else the age of the superhuman machines is imminent. At least for now, that is a false dichotomy. There are areas where AI is </span><a href=\"https://www.oneusefulthing.org/p/superhuman\">better than an expert human</a><span> at particular tasks, and areas where it is completely useless. Instead of blanket statements, we should focus on specifics: we know that LLMs, without further development, are already useful as a co-intelligence that greatly improves </span><a href=\"https://www.oneusefulthing.org/p/signs-and-portents\">human performance</a><span> (in innovation, productivity, coding, and more), but we also have yet to figure out every strength and weakness. </span></p><p><span>The first wave of AI adoption was about individual use, and that seems to have been a </span><a href=\"https://www.microsoft.com/en-us/worklab/work-trend-index/ai-at-work-is-here-now-comes-the-hard-part\">huge success</a><span>, with some of the fastest adoption rates in history for a new technology. But the second wave, putting AI to work, is going to involve</span><a href=\"https://www.oneusefulthing.org/p/reshaping-the-tree-rebuilding-organizations\"> integrating it into organizations</a><span><a href=\"https://www.oneusefulthing.org/p/latent-expertise-everyone-is-in-r#footnote-1-144932334\">1</a></span><span>. This will take longer and will be the key to true productivity growth. After talking to many companies, however, I see many of them following the same well-trod path, viewing AI as an information technology that can be used for cost savings. I think this is a mistake. To see why, let’s reconsider the old analogy comparing AI to the Industrial Revolution.</span></p><p><span>One of the most fascinating things about the Industrial Revolution in England is how much progress happened in so many industries - from textiles to medicine to metallurgy to instrument-making - in a short time. A lot of credit is given to the great inventors like James Watt and his steam engine, </span><a href=\"https://www.nber.org/system/files/chapters/c12364/c12364.pdf\">but economists have suggested that these great inventors alone were not enough</a><span>. Instead, their work needed to be adjusted and made real by people who altered the technology for different industries and factories, and then further refined by the people who implemented it for specific uses. Without a base of skilled craftsman, mechanics, and engineers working in many mills and factories, the Industrial Revolution would have just happened in theory.</span></p><p>Yet I worry that the lesson of the Industrial Revolution is being lost in AI implementations at companies. Many leaders seem to have adopted a view that the main purpose of technology is efficiency. Following the Law of the Hammer (“to a hammer every problem looks like a nail”), they see AI as a cost-cutting mechanism. Any efficiency gains must be turned into cost savings, even before anyone in the organization figures out what AI is good for. It is as if, after getting access to the steam engine in the 1700s, every manufacturer decided to keep production and quality the same, and just fire staff in response to new-found efficiency, rather than building world-spanning companies by expanding their outputs.</p><p>Starting with centralized systems built for efficiency has other drawbacks besides strangling growth. Right now, nobody - from consultants to typical software vendors - has universal answers about how to use AI to unlock new opportunities in any particular industry. Companies that turn to centralized solutions run as typical IT projects are therefore unlikely to find breakthrough ideas, at least not yet. They first need to understand the value of AI in order to use it. </p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94369840-31a7-443f-adf9-5697e497af26_1376x864.png\"><div></div></a></figure></div><p>And to understand the value of AI, they need to do R&amp;D. Since AI doesn't work like traditional software, but more like a person (even though it isn't one), there is no reason to suspect that the IT department has the best AI prompters, nor that it has any particular insight into the best uses of AI inside an organization. IT certainly plays a role, but the actual use cases will come from workers and managers who find opportunities to use AI to help them with their job. In fact, for large companies, the source of any real advantage in AI will come from the expertise of their employees, which is needed to unlock the expertise latent in AI.</p><p><span>Large Language Models are </span><a href=\"https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox\">forgetful foxes in a Berlinian sense</a><span>: they know many things, imperfectly. Oddly, we don’t actually </span><em>know </em><span>everything they know, in part because training data is kept secret, but also because it isn’t always clear what LLMs learn from their training data. Yet it is clear that they do have expertise hidden in their latent space - they </span><a href=\"https://x.com/emollick/status/1793046320812310709\">outperform </a><span>doctors at diagnosing diseases in some </span><a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">studies</a><span>, and exceed more doctors in providing empathetic replies to patients, even though those were not expected uses of the system.</span></p><p><span>Unlocking the expertise latent in AI is, for now, a job for experts. There are multiple reasons for this. The first is that experts can easily judge whether work in their field is good or bad, and in what ways. Take, for example, two topics we are familiar with: teaching people to apply frameworks and interactive tutoring. We can give the AI two simple prompts, one to get the AI to act as a tutor and the other to get it provide frameworks for solving problems (if you want to try the more advanced versions of these prompts that actually work well, you can try the </span><a href=\"https://chatgpt.com/g/g-r6eogQdIh-ai-tutor-new\">Tutor GPT</a><span> and </span><a href=\"https://chatgpt.com/g/g-vZ7SgKBOh-framework-finder\">Frameworks GPT</a><span>:</span></p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25368729-76d6-48bb-a4fd-d7959e2bb888_2155x1412.png\"><div></div></a></figure></div><p>Since we know something about these topics, we can instantly tell that the answer on frameworks, while not amazing, is not terrible. It suggests a number of possible approaches, and, in the text that I cut out of the image, how to apply them. The frameworks are appropriate, and, if the goal is to make you think about the problem, this is not a bad start. The tutor prompt is a different matter. Good tutors need to interact with the student, not assume knowledge. They should not merely throw information at the student but adapt to their abilities and meet them where they are. And they should figure out what you know, not ask you what you think you need to know. The tutor fails at all of these points.</p><p><span>But because we know what good tutoring does, and what the gaps of the AI were, we can easily determine what behaviors we needed to suppress or activate in order for the AI to act as a good tutor. We also know how to teach other people to be a tutor, a skill that is remarkably transferrable to AI but just writing those instructions as a more elaborate prompt. The result is a solid tutor from a prompt alone (</span><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4802463\">see it here</a><span>). In fact, Google</span><a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\"> has tested a version of our prompt</a><span> against a fine-tuned educational model they built, and found that they have statistically similar performance across most dimensions.</span></p><div><figure><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0bfabdf-92b6-47a9-93d2-746a52f33715_523x350.png\"><div></div></a></figure></div><p><span>This also illustrates another reason why experts are best at using AI for now. As one of my PhD advisors, Eric von Hippel, </span><a href=\"https://evhippel.mit.edu/teaching/\">pointed out</a><span>: R&amp;D is very expensive because it involves lots of trial and error, but when you are doing a task all the time, trial and error is cheap and easy. That is why a surprisingly large percentage of important innovation comes, not from formal R&amp;D labs, but rather from people figuring out how to solve their own problems. Learning by doing is cheap if you are already doing.</span></p><p>Experts thus have many advantages. They are better able to see through LLM errors and hallucinations; they are better judges of AI output in their area of interest; they are better able to instruct the AI to do the required job; and they have the opportunity for more trial and error. That lets them unlock the latent expertise within LLMs in ways that others could not.</p><p><span>For example, LLMs can write solid job descriptions, but they often sound very generic. Is there the possibility that the AI could do something more? Dan Shapiro figured out how. He is a serial entrepreneur and </span><a href=\"https://glowforge.com/m/founders\">co-founder of Glowforge</a><span>, which makes cool laser carving tools. Dan is an expert at building culture in organizations and </span><a href=\"https://blog.glowforge.com/1967-2/\">he credits his job descriptions as one of his secret weapons for attracting talent</a><span> (plus he is hiring). But handcrafting these “job descriptions as love letter",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "AI discussions often fall into a weird dichotomy - it is either all “hype” or else the age of the superhuman machines is imminent. At least for now, that is a false dichotomy. There are areas where AI isbetter than an expert humanat particular tasks, and areas where it is completely useless. Instead of blanket statements, we should focus on specifics: we know that LLMs, without further development, are already useful as a co-intelligence that greatly improveshuman performance(in innovation, prod",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AI discussions often fall into a weird dichotomy - it is either all “hype” or else the age of the superhuman machines is imminent. At least for now, that is a false dichotomy. There are areas where AI isbetter than an expert humanat particular tasks, and areas where it is completely useless. Instead of blanket statements, we should focus on specifics: we know that LLMs, without further development, are already useful as a co-intelligence that greatly improveshuman performance(in innovation, prod",
              "class": [],
              "id": ""
            }
          ],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://www.researchgate.net/publication/305344683_Effects_of_Prior_Knowledge_on_Memory_Implications_for_Education",
      "title": "Effects of Prior Knowledge on Memory: Implications for Education | Request PDF",
      "author": "",
      "published_date": "2016-07-01T00:00:00.000Z",
      "content": {
        "text": "<div><section><div><p>The encoding, consolidation, and retrieval of events and facts form the basis for acquiring new skills and knowledge. Prior knowledge can enhance those memory processes considerably and thus foster knowledge acquisition. But prior knowledge can also hinder knowledge acquisition, in particular when the to-be-learned information is inconsistent with the presuppositions of the learner. Therefore, taking students' prior knowledge into account and knowing about the way it affects memory processes is important for optimization of students' learning. Recent behavioral and neuroimaging experiments have shed new light on the neural mechanisms through which prior knowledge affects memory. However, relatively little is known about developmental differences in the ability to make efficient use of one's knowledge base for memory purposes. In this article, we review and integrate recent empirical evidence from developmental psychology and cognitive neuroscience about the effects of prior knowledge on memory processes. In particular, this may entail an extended shift from processing in the medial temporal lobes of the brain toward processing in the neocortex. Such findings have implications for students as developing individuals. Therefore, we highlight recent insights from cognitive neuroscience that call for further investigation in educational settings, discussing to what extent these novel insights may inform teaching in the classroom.</p></div><div><div></div><p>To read the full-text of this research,<br/> you can request a copy directly from the authors.</p></div><div><div><div><div><div><p>... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl &amp; Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod &amp; Shing, 2019;Hemmer &amp; Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto &amp; Love, 2004, Sherman &amp; Frost, 2000; see Bjorklund, 1987;<mark>Shing &amp; Brod, 2016</mark>, for further discussion and review). Yet, the literature employing these approaches, although vast, has produced somewhat opaque theories and surprisingly mixed findings. ...</p></div><div><p>... memory for expectation-related information (Hemmer &amp; Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Sakamoto &amp; Love, 2004). Developmental studies have explored how expectation-related information shapes memory and learning, particularly in children (e.g., Brod &amp; Shing, 2019;Stahl &amp; Feigenson, 2017;see Bjorklund, 1987<mark>, Shing &amp; Brod, 2016</mark>, for further discussion and review). Open questions and future directions of these studies create prime opportunities for integrating approaches. ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Carla-Macias-2200700610\"><span><span>Carla Macias</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Kimele-Persaud-2\"><span><span></span><span>Kimele Persaud</span></span></a></li></ul></div><div><p><span>Significant progress in the investigation of how prior knowledge influences episodic memory has been made using three sometimes isolated (but not mutually exclusive) approaches: strictly adult behavioral investigations, computational models, and investigations into the development of the system. Here we point out that these approaches are complementary, each approach informs and is informed by the other. Thus, a natural next step for research is to combine all three approaches to further our understanding of the role of prior knowledge in episodic memory. Here we use studies of memory for expectation-congruent and incongruent information from each of these often disparate approaches to illustrate how combining approaches can be used to test and revise theories from the other. This domain is particularly advantageous because it highlights important features of more general memory processes, further differentiates models of memory, and can shed light on developmental change in the memory system. We then present a case study to illustrate the progress that can be made from integrating all three approaches and highlight the need for more endeavors in this vein. As a first step, we also propose a new computational model of memory that takes into account behavioral and developmental factors that can influence prior knowledge and episodic memory interactions. This integrated approach has great potential for offering novel insights into the relationship between prior knowledge and episodic memory, and cognition more broadly.</span></p></div></div></div><div><div><div><p>... As in evolution, the information borrowed is also reorganized by the recipient. The borrowed information is combined with old information previously stored, and this combined information has been termed schemas (see Rumelhart, 1981;<mark>Shing and Brod, 2016)</mark> or chunks (see Cowan, 2001). When learning, those features of the instructional information that conform with previously stored schemas, are refined or emphasized; at the same time, novel information that does not match information stored in long term memory is smoothed (Bartlett, 1932;Thorndyke and Hayes-Roth, 1979). ...</p></div><div><p>... For example, specific goals to achieve a task activate the relevant information stored in long-term memory, including knowledge, beliefs, values, and interests (see Doebel, 2020). Thus, for human cognition, cues from the environment indicate which information held in long-term memory should be conveyed to working memory to generate activity (see <mark>Shing and Brod, 2016</mark>). ...</p></div></div><div><p><span>The mechanisms that govern biological evolution and human cognition are analogous, as both follow the same principles of natural information processing systems. In this article, we describe the following five principles that provide an analogy between biological evolution and human cognition: (a) Randomness as Genesis Principle and (b) Borrowing and Reorganizing Principle, which indicate how natural information processing systems obtain information; (c) Narrow Limits of Change Principle and (d) Information Store Principle, which indicate how information is processed and stored; and (e) Environmental Organizing and Linking Principle, which indicate how stored information is used to generate actions appropriate to an environment. In human cognition, these analogs only apply to cognitive processes associated with biologically secondary knowledge, the knowledge typically taught in educational institutions. Based on these five principles, cognitive load theory researchers have provided diverse prescriptions to optimize instructional activities and materials. We conclude by discussing general instructional implications and future research directions based on this analogy.</span></p></div></div><div><div><div><p>... Thampi et al. built their argument on their belief that \"theoretical framework behind the activation of prior knowledge through engaging the learner in a 'mind primer' (simulated scenario) has a stronger foundation rooted in theories of cognitive psychology.\" So, the theory of activation of prior knowledge <mark>[14]</mark> was central to the assumptions and justifications of the results of these studies. In our methodology, we were keen not to provide any foundational relevant knowledge by the facilitators in the simulation sessions so that the simulation-first group was not subjected to knowledge before the didactic lecture. ...</p></div><div><p>... Strongly disagree13. Can you list any changes you will make to your practice based on this session?<mark>14</mark>. How could this simulation and didactic session be improved? ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Tim-Khowong-2264219658\"><span><span>Tim Khowong</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Nehal-Khamis\"><span><span></span><span>Nehal Khamis</span></span></a></li></ul></div><div><p><span>Background\nTraditionally, learning is thought to occur best when prerequisite cognitive background information is delivered before simulation training. More recent studies have attempted to analyze the transformative nature of simulation by placing simulation before didactics. However, these studies were flawed as they provided background on the subject before the simulation itself. Our study aims to isolate the transformative effect of simulation and answer the question of whether lecture or simulation should come first.\nMethodology\nWe designed a novel simulation session and accompanying lecture for 18 Emergency Medicine residents in all three years of training regarding a subject they were entirely unfamiliar with, the emergent management of a left ventricular assist device (LVAD). The residents were randomized into two groups. One group had the lecture (8/18) before their simulation, while the other group (10/18) had the simulation first and the lecture afterward, testing the motivational nature. Thereafter, both groups responded to a post-session survey with Likert-style and open-ended comment questions to assess the reaction to the session and a knowledge-based multiple-choice question test.\nResults\nBoth groups did not score significantly differently on either the immediate post-test or a retention post-test that we administered four weeks later. Three of eight participants reported in open comments that they were much more comfortable with a lecture-first than a simulation-first format.\nConclusions\nDespite controlling for some of the limitations of previous studies, our results including learners’ preferences do not support a transformation in the sequence of clinical skills learning. Until other larger studies prov",
        "html": "<div><section><div><p>The encoding, consolidation, and retrieval of events and facts form the basis for acquiring new skills and knowledge. Prior knowledge can enhance those memory processes considerably and thus foster knowledge acquisition. But prior knowledge can also hinder knowledge acquisition, in particular when the to-be-learned information is inconsistent with the presuppositions of the learner. Therefore, taking students' prior knowledge into account and knowing about the way it affects memory processes is important for optimization of students' learning. Recent behavioral and neuroimaging experiments have shed new light on the neural mechanisms through which prior knowledge affects memory. However, relatively little is known about developmental differences in the ability to make efficient use of one's knowledge base for memory purposes. In this article, we review and integrate recent empirical evidence from developmental psychology and cognitive neuroscience about the effects of prior knowledge on memory processes. In particular, this may entail an extended shift from processing in the medial temporal lobes of the brain toward processing in the neocortex. Such findings have implications for students as developing individuals. Therefore, we highlight recent insights from cognitive neuroscience that call for further investigation in educational settings, discussing to what extent these novel insights may inform teaching in the classroom.</p></div><div><div></div><p>To read the full-text of this research,<br/> you can request a copy directly from the authors.</p></div><div><div><div><div><div><p>... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl &amp; Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod &amp; Shing, 2019;Hemmer &amp; Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto &amp; Love, 2004, Sherman &amp; Frost, 2000; see Bjorklund, 1987;<mark>Shing &amp; Brod, 2016</mark>, for further discussion and review). Yet, the literature employing these approaches, although vast, has produced somewhat opaque theories and surprisingly mixed findings. ...</p></div><div><p>... memory for expectation-related information (Hemmer &amp; Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Sakamoto &amp; Love, 2004). Developmental studies have explored how expectation-related information shapes memory and learning, particularly in children (e.g., Brod &amp; Shing, 2019;Stahl &amp; Feigenson, 2017;see Bjorklund, 1987<mark>, Shing &amp; Brod, 2016</mark>, for further discussion and review). Open questions and future directions of these studies create prime opportunities for integrating approaches. ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Carla-Macias-2200700610\"><span><span>Carla Macias</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Kimele-Persaud-2\"><span><span></span><span>Kimele Persaud</span></span></a></li></ul></div><div><p><span>Significant progress in the investigation of how prior knowledge influences episodic memory has been made using three sometimes isolated (but not mutually exclusive) approaches: strictly adult behavioral investigations, computational models, and investigations into the development of the system. Here we point out that these approaches are complementary, each approach informs and is informed by the other. Thus, a natural next step for research is to combine all three approaches to further our understanding of the role of prior knowledge in episodic memory. Here we use studies of memory for expectation-congruent and incongruent information from each of these often disparate approaches to illustrate how combining approaches can be used to test and revise theories from the other. This domain is particularly advantageous because it highlights important features of more general memory processes, further differentiates models of memory, and can shed light on developmental change in the memory system. We then present a case study to illustrate the progress that can be made from integrating all three approaches and highlight the need for more endeavors in this vein. As a first step, we also propose a new computational model of memory that takes into account behavioral and developmental factors that can influence prior knowledge and episodic memory interactions. This integrated approach has great potential for offering novel insights into the relationship between prior knowledge and episodic memory, and cognition more broadly.</span></p></div></div></div><div><div><div><p>... As in evolution, the information borrowed is also reorganized by the recipient. The borrowed information is combined with old information previously stored, and this combined information has been termed schemas (see Rumelhart, 1981;<mark>Shing and Brod, 2016)</mark> or chunks (see Cowan, 2001). When learning, those features of the instructional information that conform with previously stored schemas, are refined or emphasized; at the same time, novel information that does not match information stored in long term memory is smoothed (Bartlett, 1932;Thorndyke and Hayes-Roth, 1979). ...</p></div><div><p>... For example, specific goals to achieve a task activate the relevant information stored in long-term memory, including knowledge, beliefs, values, and interests (see Doebel, 2020). Thus, for human cognition, cues from the environment indicate which information held in long-term memory should be conveyed to working memory to generate activity (see <mark>Shing and Brod, 2016</mark>). ...</p></div></div><div><p><span>The mechanisms that govern biological evolution and human cognition are analogous, as both follow the same principles of natural information processing systems. In this article, we describe the following five principles that provide an analogy between biological evolution and human cognition: (a) Randomness as Genesis Principle and (b) Borrowing and Reorganizing Principle, which indicate how natural information processing systems obtain information; (c) Narrow Limits of Change Principle and (d) Information Store Principle, which indicate how information is processed and stored; and (e) Environmental Organizing and Linking Principle, which indicate how stored information is used to generate actions appropriate to an environment. In human cognition, these analogs only apply to cognitive processes associated with biologically secondary knowledge, the knowledge typically taught in educational institutions. Based on these five principles, cognitive load theory researchers have provided diverse prescriptions to optimize instructional activities and materials. We conclude by discussing general instructional implications and future research directions based on this analogy.</span></p></div></div><div><div><div><p>... Thampi et al. built their argument on their belief that \"theoretical framework behind the activation of prior knowledge through engaging the learner in a 'mind primer' (simulated scenario) has a stronger foundation rooted in theories of cognitive psychology.\" So, the theory of activation of prior knowledge <mark>[14]</mark> was central to the assumptions and justifications of the results of these studies. In our methodology, we were keen not to provide any foundational relevant knowledge by the facilitators in the simulation sessions so that the simulation-first group was not subjected to knowledge before the didactic lecture. ...</p></div><div><p>... Strongly disagree13. Can you list any changes you will make to your practice based on this session?<mark>14</mark>. How could this simulation and didactic session be improved? ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Tim-Khowong-2264219658\"><span><span>Tim Khowong</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Nehal-Khamis\"><span><span></span><span>Nehal Khamis</span></span></a></li></ul></div><div><p><span>Background\nTraditionally, learning is thought to occur best when prerequisite cognitive background information is delivered before simulation training. More recent studies have attempted to analyze the transformative nature of simulation by placing simulation before didactics. However, these studies were flawed as they provided background on the subject before the simulation itself. Our study aims to isolate the transformative effect of simulation and answer the question of whether lecture or simulation should come first.\nMethodology\nWe designed a novel simulation session and accompanying lecture for 18 Emergency Medicine residents in all three years of training regarding a subject they were entirely unfamiliar with, the emergent management of a left ventricular assist device (LVAD). The residents were randomized into two groups. One group had the lecture (8/18) before their simulation, while the other group (10/18) had the simulation first and the lecture afterward, testing the motivational nature. Thereafter, both groups responded to a post-session survey with Likert-style and open-ended comment questions to assess the reaction to the session and a knowledge-based multiple-choice question test.\nResults\nBoth groups did not score significantly differently on either the immediate post-test or a retention post-test that we administered four weeks later. Three of eight participants reported in open comments that they were much more comfortable with a lecture-first than a simulation-first format.\nConclusions\nDespite controlling for some of the limitations of previous studies, our results including learners’ preferences do not support a transformation in the sequence of clinical skills learning. Until other larger studies prov",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "The encoding, consolidation, and retrieval of events and facts form the basis for acquiring new skills and knowledge. Prior knowledge can enhance those memory processes considerably and thus foster knowledge acquisition. But prior knowledge can also hinder knowledge acquisition, in particular when the to-be-learned information is inconsistent with the presuppositions of the learner. Therefore, taking students' prior knowledge into account and knowing about the way it affects memory processes is ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "The encoding, consolidation, and retrieval of events and facts form the basis for acquiring new skills and knowledge. Prior knowledge can enhance those memory processes considerably and thus foster knowledge acquisition. But prior knowledge can also hinder knowledge acquisition, in particular when the to-be-learned information is inconsistent with the presuppositions of the learner. Therefore, taking students' prior knowledge into account and knowing about the way it affects memory processes is ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The encoding, consolidation, and retrieval of events and facts form the basis for acquiring new skills and knowledge. Prior knowledge can enhance those memory processes considerably and thus foster knowledge acquisition. But prior knowledge can also hinder knowledge acquisition, in particular when the to-be-learned information is inconsistent with the presuppositions of the learner. Therefore, taking students' prior knowledge into account and knowing about the way it affects memory processes is ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "To read the full-text of this research,you can request a copy directly from the authors.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl & Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod & Shing, 2019;Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto & Love, 2004, Sherman & Frost, 2000; see Bj",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl & Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod & Shing, 2019;Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto & Love, 2004, Sherman & Frost, 2000; see Bj",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl & Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod & Shing, 2019;Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto & Love, 2004, Sherman & Frost, 2000; see Bj",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl & Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod & Shing, 2019;Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto & Love, 2004, Sherman & Frost, 2000; see Bj",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Independently, each of these three approaches have been fruitful in advancing our understanding of the interaction of prior knowledge and memory (e.g., Stahl & Feigenson, 2017). Similarly, combining at least two of these approaches has also been informative in grounding theories of memory and knowledge interactions (e.g., Bein et al., 2015;Brod & Shing, 2019;Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Ortiz-Tudela et al., 2017;Sakamoto & Love, 2004, Sherman & Frost, 2000; see Bj",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... memory for expectation-related information (Hemmer & Steyvers, 2009a, 2009bHuttenlocher et al., 1991;Sakamoto & Love, 2004). Developmental studies have explored how expectation-related information shapes memory and learning, particularly in children (e.g., Brod & Shing, 2019;Stahl & Feigenson, 2017;see Bjorklund, 1987, Shing & Brod, 2016, for further discussion and review). Open questions and future directions of these studies create prime opportunities for integrating approaches. ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Carla MaciasKimele PersaudSignificant progress in the investigation of how prior knowledge influences episodic memory has been made using three sometimes isolated (but not mutually exclusive) approaches: strictly adult behavioral investigations, computational models, and investigations into the development of the system. Here we point out that these approaches are complementary, each approach informs and is informed by the other. Thus, a natural next step for research is to combine all three app",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Carla MaciasKimele Persaud",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Significant progress in the investigation of how prior knowledge influences episodic memory has been made using three sometimes isolated (but not mutually exclusive) approaches: strictly adult behavioral investigations, computational models, and investigations into the development of the system. Here we point out that these approaches are complementary, each approach informs and is informed by the other. Thus, a natural next step for research is to combine all three approaches to further our und",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... As in evolution, the information borrowed is also reorganized by the recipient. The borrowed information is combined with old information previously stored, and this combined information has been termed schemas (see Rumelhart, 1981;Shing and Brod, 2016)or chunks (see Cowan, 2001). When learning, those features of the instructional information that conform with previously stored schemas, are refined or emphasized; at the same time, novel information that does not match information stored in l",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... As in evolution, the information borrowed is also reorganized by the recipient. The borrowed information is combined with old information previously stored, and this combined information has been termed schemas (see Rumelhart, 1981;Shing and Brod, 2016)or chunks (see Cowan, 2001). When learning, those features of the instructional information that conform with previously stored schemas, are refined or emphasized; at the same time, novel information that does not match information stored in l",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... As in evolution, the information borrowed is also reorganized by the recipient. The borrowed information is combined with old information previously stored, and this combined information has been termed schemas (see Rumelhart, 1981;Shing and Brod, 2016)or chunks (see Cowan, 2001). When learning, those features of the instructional information that conform with previously stored schemas, are refined or emphasized; at the same time, novel information that does not match information stored in l",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... For example, specific goals to achieve a task activate the relevant information stored in long-term memory, including knowledge, beliefs, values, and interests (see Doebel, 2020). Thus, for human cognition, cues from the environment indicate which information held in long-term memory should be conveyed to working memory to generate activity (seeShing and Brod, 2016). ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The mechanisms that govern biological evolution and human cognition are analogous, as both follow the same principles of natural information processing systems. In this article, we describe the following five principles that provide an analogy between biological evolution and human cognition: (a) Randomness as Genesis Principle and (b) Borrowing and Reorganizing Principle, which indicate how natural information processing systems obtain information; (c) Narrow Limits of Change Principle and (d) ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Thampi et al. built their argument on their belief that \"theoretical framework behind the activation of prior knowledge through engaging the learner in a 'mind primer' (simulated scenario) has a stronger foundation rooted in theories of cognitive psychology.\" So, the theory of activation of prior knowledge[14]was central to the assumptions and justifications of the results of these studies. In our methodology, we were keen not to provide any foundational relevant knowledge by the facilitator",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Thampi et al. built their argument on their belief that \"theoretical framework behind the activation of prior knowledge through engaging the learner in a 'mind primer' (simulated scenario) has a stronger foundation rooted in theories of cognitive psychology.\" So, the theory of activation of prior knowledge[14]was central to the assumptions and justifications of the results of these studies. In our methodology, we were keen not to provide any foundational relevant knowledge by the facilitator",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Thampi et al. built their argument on their belief that \"theoretical framework behind the activation of prior knowledge through engaging the learner in a 'mind primer' (simulated scenario) has a stronger foundation rooted in theories of cognitive psychology.\" So, the theory of activation of prior knowledge[14]was central to the assumptions and justifications of the results of these studies. In our methodology, we were keen not to provide any foundational relevant knowledge by the facilitator",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Strongly disagree13. Can you list any changes you will make to your practice based on this session?14. How could this simulation and didactic session be improved? ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Tim KhowongNehal KhamisBackground\nTraditionally, learning is thought to occur best when prerequisite cognitive background information is delivered before simulation training. More recent studies have attempted to analyze the transformative nature of simulation by placing simulation before didactics. However, these studies were flawed as they provided background on the subject before the simulation itself. Our study aims to isolate the transformative effect of simulation and answer the question o",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Tim KhowongNehal Khamis",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Background\nTraditionally, learning is thought to occur best when prerequisite cognitive background information is delivered before simulation training. More recent studies have attempted to analyze the transformative nature of simulation by placing simulation before didactics. However, these studies were flawed as they provided background on the subject before the simulation itself. Our study aims to isolate the transformative effect of simulation and answer the question of whether lecture or si",
              "class": [],
              "id": ""
            }
          ],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4",
      "title": "What Discovering Latent Knowledge Did and Did Not Find",
      "author": "Fabien Roger",
      "published_date": "2023-03-13T19:29:45.000Z",
      "content": {
        "text": "<div><p>Crossposted from the <a href=\"https://alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4\">AI Alignment Forum</a>. May contain more technical jargon than usual.</p><div><p><i>Thanks to Marius Hobbhahn and Oam Patel for helpful feedback on drafts. Thanks to Collin and Haotian for answering many questions about their work.</i></p><p><a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge in Language Models Without Supervision</u></a> describes Contrast-Consistent Search (<strong>CCS</strong>), a method to find a classifier which <i>accurately answers yes-no questions given only unlabeled model activations</i>. It might be a stepping stone towards recovering superhuman beliefs of AI systems, as unsupervised methods are more scalable and might be less likely to simply recover “what a human would say”.</p><p>I think this research direction is interesting and promising. But I feel like people often got a bit carried away with the approximate takeaway they got from the experimental results of the initial paper.</p><p>In this post, <strong>I present experimental results which highlight the strengths and weaknesses of CCS</strong>.</p><ol><li>CCS is able to<strong> </strong>find <strong>a single linear probe which correctly classifies statements across datasets</strong>, and it doesn’t hurt performance;</li><li>CCS does so better than random, but not by a huge margin: on average, <strong>random linear probes have a 75% accuracy</strong> on some “easy” datasets;</li><li>CCS does <strong>not find </strong><i><strong>the </strong></i><strong>single linear probe with high accuracy</strong>: there are <strong>more than 20</strong> orthogonal linear probes (i.e. using completely different information) that have similar accuracies as the linear probe found by CCS (for most datasets);</li><li>CCS <strong>does not always find a probe with low test CCS loss</strong> (Figure 1 of the paper is misleading). CSS finds probes which are sometimes overconfident in inconsistent predictions on the test set, resulting in a test loss that is sometimes higher than always predicting a constant probability;</li><li><strong>CCS’ performance on GPT-J heavily depends on the last tokens of the input</strong>, especially when looking at the last layers’ activations (the setting used in the paper).</li></ol><p><strong>Main takeaways</strong>:</p><ul><li>CCS does not simply find weird heuristics on each dataset independently, i.e. it finds some feature or property that is shared between datasets. However, we still don’t know if this feature corresponds to the model’s “beliefs”.</li><li>Future work should compare their work against the random probe baseline. Comparing to a 50% random guessing baseline is misleading, as random probes have higher accuracy than that.</li><li>CCS will likely miss important information about the model’s beliefs because there is more than one linear probe which achieves low loss and high CCS accuracy, i.e. there is more than one truth-like feature.</li><li>There are many orthogonal linear probes which achieve low loss and high CCS accuracy, i.e. there are many truth-like features. Narrowing down which linear probe corresponds to the model’s beliefs might be hard.</li><li>There exists a direction which contains all linearly available information about truth, i.e. you can’t train a linear classifier to classify true from untrue texts after projecting the activations along this direction. CCS doesn’t find it. This means CCS is ill-suited for ablation-related experiments.</li><li>Future work should use more data or more regularization than the original paper did if it wants to find features which are actually truth-like.</li><li>To get clean results, use CCS on UQA, and don’t get too close to GPT models. Investigating when and why CCS sometimes fails with GPT models could be a promising research direction.</li><li>When using CCS on GPT models, don’t use CCS only on the last layer, as probes trained on activations earlier in the network are less sensitive to the format of the input.</li></ul><h2>Experimental setup</h2><p>I’m using a <a href=\"https://github.com/safer-ai/Exhaustive-CCS\"><u>modified version</u></a><span><sup><a href=\"#fnv9g46w54bfl\">[1]</a></sup></span> of the code Collin and Haotian used to run the experiments (the zip file linked in this <a href=\"https://github.com/collin-burns/discovering_latent_knowledge/\"><u>readme</u></a>).</p><p>I report results for two models:</p><ul><li><a href=\"https://arxiv.org/abs/2005.00700\"><strong><u>UnifiedQA</u></strong></a> (<a href=\"https://huggingface.co/allenai/unifiedqa-t5-11b\"><u>T5, 11B parameters</u></a>), which has the highest accuracies, and which Collin uses for Figure 1 of the paper. UQA has been trained to be good at tasks like the one CCS uses, which is probably why it performs so well. Unless specified, I use the activations of the last layer of the encoder, i.e. the activations fed to the decoder (which is the setting used in the paper).</li><li><a href=\"https://huggingface.co/EleutherAI/gpt-j-6B\"><strong><u>GPT-J</u></strong></a>, which is the model closest to the models I truly care about, which Collin also experimented with. GPT-J has been trained exclusively with next-token prediction on the Pile. Unless specified, I use the activations of the last layer, i.e. the activations fed to the unembedding layer (which is the setting used in the paper).</li></ul><p>For each model, <strong>I only use datasets which they can solve</strong>, i.e. datasets for which the accuracy of a linear probe trained with supervised labels on the last layer’s activation is at least 90%. All experiments are done with 10 random seeds.</p><h2>What CCS does and doesn’t find</h2><h2>CCS is able to find a single probe which correctly classifies statements across datasets</h2><p><strong>What the paper does</strong>: It trains linear probes on individual datasets (IMDB, COPA, …), and then measures high transfer accuracies (Appendix E).</p><p><strong>What Collin claims </strong>(in<a href=\"https://www.alignmentforum.org/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without\"><u> the Alignment Forum Post</u></a>): “CCS, accurately classifies text as true or false directly from a model’s unlabeled activations across a wide range of tasks”. Right below, the figure only shows one theta instead of many thetas (one per task) may lead the reader to think that only one linear probe is trained across the wide range of tasks.</p><p><strong>What I measure</strong>: I train a probe on all datasets (the “trained together” probe) each model can solve. I reproduce Collin’s experiments by training probes on each dataset. I evaluate accuracy on each dataset separately. I compare those two ways of finding probes with the “ceiling” used in the paper: training a supervised probe per dataset.</p><p><strong>What I find</strong>: Training a single CCS probe does not reduce accuracy by a significant margin over training a probe per dataset, which supports the idea that CCS enables you to find a single probe which classifies text as true or false (a least for text inputs which are correctly answered questions from classic NLP datasets).</p><figure></figure><h2>CCS does so better than random, but not by a huge margin</h2><p><strong>What the paper does</strong>: it never measures the accuracy of random probes</p><p><strong>What Collin claims</strong>: “it wasn’t clear to me whether it should even be possible to classify examples as true or false from unlabeled LM representations <i>better than random chance</i>”</p><p><strong>What I measure</strong>: I use the random initialization of CCS probes, but I don’t train them at all. Then I measure their accuracy.</p><p><i>Note: random accuracy can be better than 0.5 because the methodology used in the CCS paper allows you to swap all predictions if your accuracy is below 50%, and thereby extract one bit of information from the (test) labels. This is because the CCS loss isn’t able to distinguish the probes which correctly classify all statements from the probes which incorrectly classify all statements.</i></p><p><strong>What I find</strong>: The accuracy of random linear probes is very high! But both CCS and supervised learning are above what you would expect if they were just getting random-but-lucky probes. </p><figure></figure><p><strong>Implications</strong>: Future research should always compare their results with randomly initialized probes rather than with random guessing when it wants to assess how much CCS was able to single out a “good” probe. Other baselines such as zero-shot and Logistic Regression can be interesting but measure slightly different things.</p><p>The fact that even random probes can get relatively high accuracy hints towards the next point: there are many linear probes which get high accuracy.</p><h2>CCS does not find the single linear probe with high accuracy</h2><p><strong>What the paper does</strong>: It does not try to measure if the probe CCS finds is unique.</p><p><strong>What Collin claims</strong>: “Hypothesis 3: One of the few <i>truth-like</i> features GPT-n will represent is whether an input is actually true or false. This would mean that we only need a small number of remaining bits to identify its beliefs from all the <i>candidate </i>features”.</p><p><i>For the rest of this post, I will write about “directions” without specifying that I’m writing about the direction of a linear classifier i.e. the direction orthogonal to the decision plane of the classifier i.e the unit vector d in y = \\sigma(a&lt;d,x&gt; + b). I will also use “direction” to speak about the feature it captures in the model’s activation.</i></p><p><strong>What I measure</strong>: I use CCS multiple times, constraining the direction of the linear classifier at iteration n to be orthogonal to the directions of the linear classifiers found by CCS at iteration 1, …, n-1 ",
        "html": "<div><p>Crossposted from the <a href=\"https://alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4\">AI Alignment Forum</a>. May contain more technical jargon than usual.</p><div><p><i>Thanks to Marius Hobbhahn and Oam Patel for helpful feedback on drafts. Thanks to Collin and Haotian for answering many questions about their work.</i></p><p><a href=\"https://arxiv.org/abs/2212.03827\"><u>Discovering Latent Knowledge in Language Models Without Supervision</u></a> describes Contrast-Consistent Search (<strong>CCS</strong>), a method to find a classifier which <i>accurately answers yes-no questions given only unlabeled model activations</i>. It might be a stepping stone towards recovering superhuman beliefs of AI systems, as unsupervised methods are more scalable and might be less likely to simply recover “what a human would say”.</p><p>I think this research direction is interesting and promising. But I feel like people often got a bit carried away with the approximate takeaway they got from the experimental results of the initial paper.</p><p>In this post, <strong>I present experimental results which highlight the strengths and weaknesses of CCS</strong>.</p><ol><li>CCS is able to<strong> </strong>find <strong>a single linear probe which correctly classifies statements across datasets</strong>, and it doesn’t hurt performance;</li><li>CCS does so better than random, but not by a huge margin: on average, <strong>random linear probes have a 75% accuracy</strong> on some “easy” datasets;</li><li>CCS does <strong>not find </strong><i><strong>the </strong></i><strong>single linear probe with high accuracy</strong>: there are <strong>more than 20</strong> orthogonal linear probes (i.e. using completely different information) that have similar accuracies as the linear probe found by CCS (for most datasets);</li><li>CCS <strong>does not always find a probe with low test CCS loss</strong> (Figure 1 of the paper is misleading). CSS finds probes which are sometimes overconfident in inconsistent predictions on the test set, resulting in a test loss that is sometimes higher than always predicting a constant probability;</li><li><strong>CCS’ performance on GPT-J heavily depends on the last tokens of the input</strong>, especially when looking at the last layers’ activations (the setting used in the paper).</li></ol><p><strong>Main takeaways</strong>:</p><ul><li>CCS does not simply find weird heuristics on each dataset independently, i.e. it finds some feature or property that is shared between datasets. However, we still don’t know if this feature corresponds to the model’s “beliefs”.</li><li>Future work should compare their work against the random probe baseline. Comparing to a 50% random guessing baseline is misleading, as random probes have higher accuracy than that.</li><li>CCS will likely miss important information about the model’s beliefs because there is more than one linear probe which achieves low loss and high CCS accuracy, i.e. there is more than one truth-like feature.</li><li>There are many orthogonal linear probes which achieve low loss and high CCS accuracy, i.e. there are many truth-like features. Narrowing down which linear probe corresponds to the model’s beliefs might be hard.</li><li>There exists a direction which contains all linearly available information about truth, i.e. you can’t train a linear classifier to classify true from untrue texts after projecting the activations along this direction. CCS doesn’t find it. This means CCS is ill-suited for ablation-related experiments.</li><li>Future work should use more data or more regularization than the original paper did if it wants to find features which are actually truth-like.</li><li>To get clean results, use CCS on UQA, and don’t get too close to GPT models. Investigating when and why CCS sometimes fails with GPT models could be a promising research direction.</li><li>When using CCS on GPT models, don’t use CCS only on the last layer, as probes trained on activations earlier in the network are less sensitive to the format of the input.</li></ul><h2>Experimental setup</h2><p>I’m using a <a href=\"https://github.com/safer-ai/Exhaustive-CCS\"><u>modified version</u></a><span><sup><a href=\"#fnv9g46w54bfl\">[1]</a></sup></span> of the code Collin and Haotian used to run the experiments (the zip file linked in this <a href=\"https://github.com/collin-burns/discovering_latent_knowledge/\"><u>readme</u></a>).</p><p>I report results for two models:</p><ul><li><a href=\"https://arxiv.org/abs/2005.00700\"><strong><u>UnifiedQA</u></strong></a> (<a href=\"https://huggingface.co/allenai/unifiedqa-t5-11b\"><u>T5, 11B parameters</u></a>), which has the highest accuracies, and which Collin uses for Figure 1 of the paper. UQA has been trained to be good at tasks like the one CCS uses, which is probably why it performs so well. Unless specified, I use the activations of the last layer of the encoder, i.e. the activations fed to the decoder (which is the setting used in the paper).</li><li><a href=\"https://huggingface.co/EleutherAI/gpt-j-6B\"><strong><u>GPT-J</u></strong></a>, which is the model closest to the models I truly care about, which Collin also experimented with. GPT-J has been trained exclusively with next-token prediction on the Pile. Unless specified, I use the activations of the last layer, i.e. the activations fed to the unembedding layer (which is the setting used in the paper).</li></ul><p>For each model, <strong>I only use datasets which they can solve</strong>, i.e. datasets for which the accuracy of a linear probe trained with supervised labels on the last layer’s activation is at least 90%. All experiments are done with 10 random seeds.</p><h2>What CCS does and doesn’t find</h2><h2>CCS is able to find a single probe which correctly classifies statements across datasets</h2><p><strong>What the paper does</strong>: It trains linear probes on individual datasets (IMDB, COPA, …), and then measures high transfer accuracies (Appendix E).</p><p><strong>What Collin claims </strong>(in<a href=\"https://www.alignmentforum.org/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without\"><u> the Alignment Forum Post</u></a>): “CCS, accurately classifies text as true or false directly from a model’s unlabeled activations across a wide range of tasks”. Right below, the figure only shows one theta instead of many thetas (one per task) may lead the reader to think that only one linear probe is trained across the wide range of tasks.</p><p><strong>What I measure</strong>: I train a probe on all datasets (the “trained together” probe) each model can solve. I reproduce Collin’s experiments by training probes on each dataset. I evaluate accuracy on each dataset separately. I compare those two ways of finding probes with the “ceiling” used in the paper: training a supervised probe per dataset.</p><p><strong>What I find</strong>: Training a single CCS probe does not reduce accuracy by a significant margin over training a probe per dataset, which supports the idea that CCS enables you to find a single probe which classifies text as true or false (a least for text inputs which are correctly answered questions from classic NLP datasets).</p><figure></figure><h2>CCS does so better than random, but not by a huge margin</h2><p><strong>What the paper does</strong>: it never measures the accuracy of random probes</p><p><strong>What Collin claims</strong>: “it wasn’t clear to me whether it should even be possible to classify examples as true or false from unlabeled LM representations <i>better than random chance</i>”</p><p><strong>What I measure</strong>: I use the random initialization of CCS probes, but I don’t train them at all. Then I measure their accuracy.</p><p><i>Note: random accuracy can be better than 0.5 because the methodology used in the CCS paper allows you to swap all predictions if your accuracy is below 50%, and thereby extract one bit of information from the (test) labels. This is because the CCS loss isn’t able to distinguish the probes which correctly classify all statements from the probes which incorrectly classify all statements.</i></p><p><strong>What I find</strong>: The accuracy of random linear probes is very high! But both CCS and supervised learning are above what you would expect if they were just getting random-but-lucky probes. </p><figure></figure><p><strong>Implications</strong>: Future research should always compare their results with randomly initialized probes rather than with random guessing when it wants to assess how much CCS was able to single out a “good” probe. Other baselines such as zero-shot and Logistic Regression can be interesting but measure slightly different things.</p><p>The fact that even random probes can get relatively high accuracy hints towards the next point: there are many linear probes which get high accuracy.</p><h2>CCS does not find the single linear probe with high accuracy</h2><p><strong>What the paper does</strong>: It does not try to measure if the probe CCS finds is unique.</p><p><strong>What Collin claims</strong>: “Hypothesis 3: One of the few <i>truth-like</i> features GPT-n will represent is whether an input is actually true or false. This would mean that we only need a small number of remaining bits to identify its beliefs from all the <i>candidate </i>features”.</p><p><i>For the rest of this post, I will write about “directions” without specifying that I’m writing about the direction of a linear classifier i.e. the direction orthogonal to the decision plane of the classifier i.e the unit vector d in y = \\sigma(a&lt;d,x&gt; + b). I will also use “direction” to speak about the feature it captures in the model’s activation.</i></p><p><strong>What I measure</strong>: I use CCS multiple times, constraining the direction of the linear classifier at iteration n to be orthogonal to the directions of the linear classifiers found by CCS at iteration 1, …, n-1 ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Crossposted from theAI Alignment Forum. May contain more technical jargon than usual.Thanks to Marius Hobbhahn and Oam Patel for helpful feedback on drafts. Thanks to Collin and Haotian for answering many questions about their work.Discovering Latent Knowledge in Language Models Without Supervisiondescribes Contrast-Consistent Search (CCS), a method to find a classifier whichaccurately answers yes-no questions given only unlabeled model activations. It might be a stepping stone towards recoverin",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Thanks to Marius Hobbhahn and Oam Patel for helpful feedback on drafts. Thanks to Collin and Haotian for answering many questions about their work.Discovering Latent Knowledge in Language Models Without Supervisiondescribes Contrast-Consistent Search (CCS), a method to find a classifier whichaccurately answers yes-no questions given only unlabeled model activations. It might be a stepping stone towards recovering superhuman beliefs of AI systems, as unsupervised methods are more scalable and mig",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Experimental setup",
              "id": ""
            },
            {
              "level": "h2",
              "text": "What CCS does and doesn’t find",
              "id": ""
            },
            {
              "level": "h2",
              "text": "CCS is able to find a single probe which correctly classifies statements across datasets",
              "id": ""
            },
            {
              "level": "h2",
              "text": "CCS does so better than random, but not by a huge margin",
              "id": ""
            },
            {
              "level": "h2",
              "text": "CCS does not find the single linear probe with high accuracy",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://arxiv.org/abs/2212.03827",
      "title": "Discovering Latent Knowledge in Language Models Without Supervision",
      "author": "Burns; Collin; Ye; Haotian; Klein; Dan; Steinhardt; Jacob",
      "published_date": "2023-02-27T04:20:19.000Z",
      "content": {
        "text": "<div><div>\n<p><a href=\"/pdf/2212.03827\">Download PDF</a></p><blockquote>\nAbstract: Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a method for accurately answering yes-no questions\ngiven only unlabeled model activations. It works by finding a direction in\nactivation space that satisfies logical consistency properties, such as that a\nstatement and its negation have opposite truth values. We show that despite\nusing no supervision and no model outputs, our method can recover diverse\nknowledge represented in large language models: across 6 models and 10\nquestion-answering datasets, it outperforms zero-shot accuracy by 4\\% on\naverage. We also find that it cuts prompt sensitivity in half and continues to\nmaintain high accuracy even when models are prompted to generate incorrect\nanswers. Our results provide an initial step toward discovering what language\nmodels know, distinct from what they say, even when we don't have access to\nexplicit ground truth labels.\n</blockquote>\n</div><div>\n<h2>Submission history</h2><p> From: Collin Burns [<a href=\"/show-email/0696d28b/2212.03827\">view email</a>]\n<br /><strong>[v1]</strong>\nWed, 7 Dec 2022 18:17:56 UTC (1,486 KB)<br /></p></div>||||I|||| Skip to main content\nWe gratefully acknowledge support from\nthe Simons Foundation and member institutions.\n> cs > arXiv:2212.03827\nHelp | Advanced Search\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\nGO\nquick links\n* Login\n* Help Pages\n* About\nComputer Science > Computation and Language\narXiv:2212.03827 (cs)\n[Submitted on 7 Dec 2022]\nTitle: Discovering Latent Knowledge in Language Models Without Supervision\nAuthors: Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt\nDownload PDF\nAbstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.\nSubjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\nCite as: arXiv:2212.03827 [cs.CL]\n(or arXiv:2212.03827v1 [cs.CL] for this version)\nhttps://doi.org/10.48550/arXiv.2212.03827\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Collin Burns [view email]\n[v1] Wed, 7 Dec 2022 18:17:56 UTC (1,486 KB)\nFull-text links:\nDownload:\n* PDF\n* Other formats\n(license)\nCurrent browse context:\ncs.CL\n< prev | next >\nnew | recent | 2212\nChange to browse by:\ncs\ncs.AI\ncs.LG\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\na export bibtex citation Loading...\nBibtex formatted citation\n×\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nRelated Papers\nRecommenders and Search Tools\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n* About\n* Help\n* Click here to contact arXiv Contact\n* Click here to subscribe Subscribe\n* Copyright\n* Privacy Policy\n* Web Accessibility Assistance\n* arXiv Operational Status\nGet status notifications via email or slack",
        "html": "<div><div>\n<p><a href=\"/pdf/2212.03827\">Download PDF</a></p><blockquote>\nAbstract: Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a method for accurately answering yes-no questions\ngiven only unlabeled model activations. It works by finding a direction in\nactivation space that satisfies logical consistency properties, such as that a\nstatement and its negation have opposite truth values. We show that despite\nusing no supervision and no model outputs, our method can recover diverse\nknowledge represented in large language models: across 6 models and 10\nquestion-answering datasets, it outperforms zero-shot accuracy by 4\\% on\naverage. We also find that it cuts prompt sensitivity in half and continues to\nmaintain high accuracy even when models are prompted to generate incorrect\nanswers. Our results provide an initial step toward discovering what language\nmodels know, distinct from what they say, even when we don't have access to\nexplicit ground truth labels.\n</blockquote>\n</div><div>\n<h2>Submission history</h2><p> From: Collin Burns [<a href=\"/show-email/0696d28b/2212.03827\">view email</a>]\n<br /><strong>[v1]</strong>\nWed, 7 Dec 2022 18:17:56 UTC (1,486 KB)<br /></p></div>||||I|||| Skip to main content\nWe gratefully acknowledge support from\nthe Simons Foundation and member institutions.\n> cs > arXiv:2212.03827\nHelp | Advanced Search\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\nGO\nquick links\n* Login\n* Help Pages\n* About\nComputer Science > Computation and Language\narXiv:2212.03827 (cs)\n[Submitted on 7 Dec 2022]\nTitle: Discovering Latent Knowledge in Language Models Without Supervision\nAuthors: Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt\nDownload PDF\nAbstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.\nSubjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\nCite as: arXiv:2212.03827 [cs.CL]\n(or arXiv:2212.03827v1 [cs.CL] for this version)\nhttps://doi.org/10.48550/arXiv.2212.03827\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Collin Burns [view email]\n[v1] Wed, 7 Dec 2022 18:17:56 UTC (1,486 KB)\nFull-text links:\nDownload:\n* PDF\n* Other formats\n(license)\nCurrent browse context:\ncs.CL\n< prev | next >\nnew | recent | 2212\nChange to browse by:\ncs\ncs.AI\ncs.LG\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\na export bibtex citation Loading...\nBibtex formatted citation\n×\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nRelated Papers\nRecommenders and Search Tools\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n* About\n* Help\n* Click here to contact arXiv Contact\n* Click here to subscribe Subscribe\n* Copyright\n* Privacy Policy\n* Web Accessibility Assistance\n* arXiv Operational Status\nGet status notifications via email or slack",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Download PDFAbstract: Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a metho",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Download PDFAbstract: Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a metho",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Submission historyFrom: Collin Burns [view email][v1]Wed, 7 Dec 2022 18:17:56 UTC (1,486 KB)",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Submission history",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "knowledge_management"
    },
    {
      "url": "https://github.com/collin-burns/discovering_latent_knowledge",
      "title": "GitHub - collin-burns/discovering_latent_knowledge",
      "author": "collin-burns",
      "published_date": "2022-12-07T18:12:36.000Z",
      "content": {
        "text": "<div><div><article><p></p><h2>Discovering Latent Knowledge Without Supervision</h2><a href=\"#discovering-latent-knowledge-without-supervision\"></a><p></p>\n<p>This repository contains the essential code for Discovering Latent Knowledge in Language Models Without Supervision.</p>\n<p>\n<a href=\"https://github.com/collin-burns/discovering_latent_knowledge/blob/main/figure.png\"></a>\n</p>\n<p>We introduce a method for discovering truth-like features directly from model activations in a purely unsupervised way.</p>\n<p>NOTE: we now recommend using <a href=\"https://github.com/EleutherAI/elk\">https://github.com/EleutherAI/elk</a>, which is an improved code base.</p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<blockquote>\n<p>Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.</p>\n</blockquote>\n<p></p><h2>Code</h2><a href=\"#code\"></a><p></p>\n<p>We provide three options for code:</p>\n<ol>\n<li>A notebook walking through our main method in a simple way: <code>CCS.ipynb</code>. This may be the best place to start if you want to understand the method better and play around with it a bit.</li>\n<li>More flexible and efficient scripts for using our method in different settings: <code>generate.py</code> and <code>evaluate.py</code> (both of which rely heavily on <code>utils.py</code>). This code is a polished and simplified version of the code used for the paper. This may be the best place to build on if you want to build on our work.</li>\n<li>You can also download the original (more comprehensive, but also more complicated and less polished) code <a href=\"https://openreview.net/attachment?id=ETKGuby0hcs&amp;name=supplementary_material\">here</a>.</li>\n</ol>\n<p>Below we provide usage details for our main python scripts (<code>generate.py</code> and <code>evaluate.py</code>).</p>\n<p></p><h3>Generation</h3><a href=\"#generation\"></a><p></p>\n<p>First, use <code>generate.py</code> for (1) creating contrast pairs, and (2) generating hidden states from a model. For example, you can run:</p>\n<div><pre><code>python generate.py --model_name deberta --num_examples 400 --batch_size 40\n</code></pre></div>\n<p>or</p>\n<div><pre><code>python generate.py --model_name gpt-j --num_examples 100 --batch_size 20\n</code></pre></div>\n<p>or</p>\n<div><pre><code>CUDA_VISIBLE_DEVICES=0,1 python generate.py --parallelize --model_name t5-11b --num_examples 100\n</code></pre></div>\n<p>To use the decoder of an encoder-decoder model (which we found is worse than the encoder for T5 and UnifiedQA, but better than the encoder for T0), specify <code>--use_decoder</code>.</p>\n<p>There are also many optional flags for specifying the dataset (<code>--dataset</code>; the default is <code>imdb</code>), the cache directory for model weights (<code>--cache_dir</code>; the default is <code>None</code>), which prompt for the dataset to use (<code>--prompt_idx</code>; the default is <code>0</code>), where to save the hidden states for all layers in the model (<code>--all_layers</code>), and so on.</p>\n<p></p><h3>Evaluation</h3><a href=\"#evaluation\"></a><p></p>\n<p>After generating hidden states, you can use <code>evaluate.py</code> for running our main method, CCS, on those hidden states. Simply run it with the same flags as you used when running <code>generate.py</code>, and it will load the correct hidden states for you. For example, if you ran <code>generate.py</code> with DeBERTa, generating 400 examples with a batch size of 40 (the first example in the Generation section), then you can run:</p>\n<div><pre><code>python evaluate.py --model_name deberta --num_examples 400 --batch_size 40\n</code></pre></div>\n<p>In addition to evaluating the performance of CCS, <code>evaluate.py</code> also verifies that logistic regression (LR) accuracy is reasonable. This can diagnose why CCS performance may be low; if LR accuracy is low, that suggestions that the model's representations aren't good enough for CCS to work well.</p>\n<p></p><h3>Requirements</h3><a href=\"#requirements\"></a><p></p>\n<p>This code base was tested on Python 3.7.5 and PyTorch 1.12. It also uses the <a href=\"https://pypi.org/project/datasets/\">datasets</a> and <a href=\"https://github.com/bigscience-workshop/promptsource\">promptsource</a> packages for loading and formatting datasets.</p>\n<p></p><h2>Citation</h2><a href=\"#citation\"></a><p></p>\n<p>If you find this work helpful, please consider citing our paper:</p>\n<div><pre><code>@article{burns2022dl,\ntitle={Discovering Latent Knowledge in Language Models Without Supervision},\nauthor={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},\njournal={ArXiV},\nyear={2022}\n}\n</code></pre></div>\n</article></div></div>",
        "html": "<div><div><article><p></p><h2>Discovering Latent Knowledge Without Supervision</h2><a href=\"#discovering-latent-knowledge-without-supervision\"></a><p></p>\n<p>This repository contains the essential code for Discovering Latent Knowledge in Language Models Without Supervision.</p>\n<p>\n<a href=\"https://github.com/collin-burns/discovering_latent_knowledge/blob/main/figure.png\"></a>\n</p>\n<p>We introduce a method for discovering truth-like features directly from model activations in a purely unsupervised way.</p>\n<p>NOTE: we now recommend using <a href=\"https://github.com/EleutherAI/elk\">https://github.com/EleutherAI/elk</a>, which is an improved code base.</p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<blockquote>\n<p>Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.</p>\n</blockquote>\n<p></p><h2>Code</h2><a href=\"#code\"></a><p></p>\n<p>We provide three options for code:</p>\n<ol>\n<li>A notebook walking through our main method in a simple way: <code>CCS.ipynb</code>. This may be the best place to start if you want to understand the method better and play around with it a bit.</li>\n<li>More flexible and efficient scripts for using our method in different settings: <code>generate.py</code> and <code>evaluate.py</code> (both of which rely heavily on <code>utils.py</code>). This code is a polished and simplified version of the code used for the paper. This may be the best place to build on if you want to build on our work.</li>\n<li>You can also download the original (more comprehensive, but also more complicated and less polished) code <a href=\"https://openreview.net/attachment?id=ETKGuby0hcs&amp;name=supplementary_material\">here</a>.</li>\n</ol>\n<p>Below we provide usage details for our main python scripts (<code>generate.py</code> and <code>evaluate.py</code>).</p>\n<p></p><h3>Generation</h3><a href=\"#generation\"></a><p></p>\n<p>First, use <code>generate.py</code> for (1) creating contrast pairs, and (2) generating hidden states from a model. For example, you can run:</p>\n<div><pre><code>python generate.py --model_name deberta --num_examples 400 --batch_size 40\n</code></pre></div>\n<p>or</p>\n<div><pre><code>python generate.py --model_name gpt-j --num_examples 100 --batch_size 20\n</code></pre></div>\n<p>or</p>\n<div><pre><code>CUDA_VISIBLE_DEVICES=0,1 python generate.py --parallelize --model_name t5-11b --num_examples 100\n</code></pre></div>\n<p>To use the decoder of an encoder-decoder model (which we found is worse than the encoder for T5 and UnifiedQA, but better than the encoder for T0), specify <code>--use_decoder</code>.</p>\n<p>There are also many optional flags for specifying the dataset (<code>--dataset</code>; the default is <code>imdb</code>), the cache directory for model weights (<code>--cache_dir</code>; the default is <code>None</code>), which prompt for the dataset to use (<code>--prompt_idx</code>; the default is <code>0</code>), where to save the hidden states for all layers in the model (<code>--all_layers</code>), and so on.</p>\n<p></p><h3>Evaluation</h3><a href=\"#evaluation\"></a><p></p>\n<p>After generating hidden states, you can use <code>evaluate.py</code> for running our main method, CCS, on those hidden states. Simply run it with the same flags as you used when running <code>generate.py</code>, and it will load the correct hidden states for you. For example, if you ran <code>generate.py</code> with DeBERTa, generating 400 examples with a batch size of 40 (the first example in the Generation section), then you can run:</p>\n<div><pre><code>python evaluate.py --model_name deberta --num_examples 400 --batch_size 40\n</code></pre></div>\n<p>In addition to evaluating the performance of CCS, <code>evaluate.py</code> also verifies that logistic regression (LR) accuracy is reasonable. This can diagnose why CCS performance may be low; if LR accuracy is low, that suggestions that the model's representations aren't good enough for CCS to work well.</p>\n<p></p><h3>Requirements</h3><a href=\"#requirements\"></a><p></p>\n<p>This code base was tested on Python 3.7.5 and PyTorch 1.12. It also uses the <a href=\"https://pypi.org/project/datasets/\">datasets</a> and <a href=\"https://github.com/bigscience-workshop/promptsource\">promptsource</a> packages for loading and formatting datasets.</p>\n<p></p><h2>Citation</h2><a href=\"#citation\"></a><p></p>\n<p>If you find this work helpful, please consider citing our paper:</p>\n<div><pre><code>@article{burns2022dl,\ntitle={Discovering Latent Knowledge in Language Models Without Supervision},\nauthor={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},\njournal={ArXiV},\nyear={2022}\n}\n</code></pre></div>\n</article></div></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Discovering Latent Knowledge Without SupervisionThis repository contains the essential code for Discovering Latent Knowledge in Language Models Without Supervision.We introduce a method for discovering truth-like features directly from model activations in a purely unsupervised way.NOTE: we now recommend usinghttps://github.com/EleutherAI/elk, which is an improved code base.AbstractExisting techniques for training language models can be misaligned with the truth: if we train models with imitatio",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Discovering Latent Knowledge Without SupervisionThis repository contains the essential code for Discovering Latent Knowledge in Language Models Without Supervision.We introduce a method for discovering truth-like features directly from model activations in a purely unsupervised way.NOTE: we now recommend usinghttps://github.com/EleutherAI/elk, which is an improved code base.AbstractExisting techniques for training language models can be misaligned with the truth: if we train models with imitatio",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "Discovering Latent Knowledge Without SupervisionThis repository contains the essential code for Discovering Latent Knowledge in Language Models Without Supervision.We introduce a method for discovering truth-like features directly from model activations in a purely unsupervised way.NOTE: we now recommend usinghttps://github.com/EleutherAI/elk, which is an improved code base.AbstractExisting techniques for training language models can be misaligned with the truth: if we train models with imitatio",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "python generate.py --model_name deberta --num_examples 400 --batch_size 40",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "python generate.py --model_name gpt-j --num_examples 100 --batch_size 20",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "CUDA_VISIBLE_DEVICES=0,1 python generate.py --parallelize --model_name t5-11b --num_examples 100",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "python evaluate.py --model_name deberta --num_examples 400 --batch_size 40",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "@article{burns2022dl,\ntitle={Discovering Latent Knowledge in Language Models Without Supervision},\nauthor={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},\njournal={ArXiV},\nyear={2022}\n}",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Discovering Latent Knowledge Without Supervision",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Code",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Generation",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Evaluation",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Requirements",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Citation",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://openreview.net/forum?id=ETKGuby0hcs",
      "title": "Discovering Latent Knowledge in Language Models Without Supervision",
      "author": "",
      "published_date": "2023-02-01T00:00:00.000Z",
      "content": {
        "text": "<div><div><main><div><div><p></p><h2><span>Blind Submission by Conference</span> • <span>Discovering Latent Knowledge in Language Models Without Supervision</span></h2><p></p><p><span>Keywords: </span><span>AI safety, AI alignment, truthfulness, large language models, honesty, interpretability</span></p><p><span>Abstract: </span><span>Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.</span></p><p><span>Anonymous Url: </span><span>I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.</span></p><p><span>No Acknowledgement Section: </span><span>I certify that there is no acknowledgement section in this submission for double blind review.</span></p><p><span>Supplementary Material: </span><span><a href=\"https://openreview.net/attachment?id=ETKGuby0hcs&amp;name=supplementary_material\"> zip</a></span></p><p><span>Code Of Ethics: </span><span>I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span></p><p><span>Submission Guidelines: </span><span>Yes</span></p><p><span>Please Choose The Closest Area That Your Submission Falls Into: </span><span>Social Aspects of Machine Learning (eg, AI safety, fairness, privacy, interpretability, human-AI interaction, ethics)</span></p><div><p><span>Community Implementations: </span><span><a href=\"https://www.catalyzex.com/paper/discovering-latent-knowledge-in-language/code\"> 2 code implementations</a></span></p></div></div><hr/></div></main></div></div>",
        "html": "<div><div><main><div><div><p></p><h2><span>Blind Submission by Conference</span> • <span>Discovering Latent Knowledge in Language Models Without Supervision</span></h2><p></p><p><span>Keywords: </span><span>AI safety, AI alignment, truthfulness, large language models, honesty, interpretability</span></p><p><span>Abstract: </span><span>Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.</span></p><p><span>Anonymous Url: </span><span>I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.</span></p><p><span>No Acknowledgement Section: </span><span>I certify that there is no acknowledgement section in this submission for double blind review.</span></p><p><span>Supplementary Material: </span><span><a href=\"https://openreview.net/attachment?id=ETKGuby0hcs&amp;name=supplementary_material\"> zip</a></span></p><p><span>Code Of Ethics: </span><span>I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span></p><p><span>Submission Guidelines: </span><span>Yes</span></p><p><span>Please Choose The Closest Area That Your Submission Falls Into: </span><span>Social Aspects of Machine Learning (eg, AI safety, fairness, privacy, interpretability, human-AI interaction, ethics)</span></p><div><p><span>Community Implementations: </span><span><a href=\"https://www.catalyzex.com/paper/discovering-latent-knowledge-in-language/code\"> 2 code implementations</a></span></p></div></div><hr/></div></main></div></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Blind Submission by Conference•Discovering Latent Knowledge in Language Models Without SupervisionKeywords:AI safety, AI alignment, truthfulness, large language models, honesty, interpretabilityAbstract:Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Blind Submission by Conference•Discovering Latent Knowledge in Language Models Without SupervisionKeywords:AI safety, AI alignment, truthfulness, large language models, honesty, interpretabilityAbstract:Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Blind Submission by Conference•Discovering Latent Knowledge in Language Models Without SupervisionKeywords:AI safety, AI alignment, truthfulness, large language models, honesty, interpretabilityAbstract:Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Blind Submission by Conference•Discovering Latent Knowledge in Language Models Without SupervisionKeywords:AI safety, AI alignment, truthfulness, large language models, honesty, interpretabilityAbstract:Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Community Implementations:2 code implementations",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Blind Submission by Conference•Discovering Latent Knowledge in Language Models Without Supervision",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://www.biorxiv.org/content/10.1101/2021.02.07.430171v2.full",
      "title": "The Memory for Latent Representations: An Account of Working Memory that Builds on Visual Knowledge for Efficient and Detailed Visual Representations",
      "author": "View ORCID ProfileShekoofeh Hedayati, Ryan O’Donnell, View ORCID ProfileBrad Wyble",
      "published_date": "2021-02-08T00:00:00.000Z",
      "content": {
        "text": "<div><div><div>\n<p><span>\nNew Results </span></p>\n<p><span><span>doi:</span> https://doi.org/10.1101/2021.02.07.430171 </span></p>\n</div>\n<div><div><h2>Abstract</h2><p>Visual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to the human visual system and an activation-based binding pool of neurons that binds items' attributes to tokenized representations. The simulation results revealed that shape information for stimuli that the model was trained on, can be encoded and retrieved efficiently from latents in higher levels of the visual hierarchy. On the other hand, novel patterns that are completely outside the training set can be stored from a single exposure using only latents from early layers of the visual system. Moreover, a given stimulus in working memory can have multiple codes, representing specific visual features such as shape or color, in addition to categorical information. Finally, we validated our model by testing a series of predictions against behavioral results acquired from WM tasks. The model provides a compelling demonstration of visual knowledge yielding the formation of compact visual representation for efficient memory encoding.</p></div><div><h2>Introduction</h2><p>In the study of memory, working memory (WM) is thought to be responsible for temporarily holding and manipulating information. This capacity to control information is thought to be a keystone of our ability to perform complex cognitive operations. Characterizing WM is an integral part of the birth of cognitive psychology, as decades of research have centered on the question of discovering the capacity and nature of this short-term memory system (e.g., <a href=\"#ref-61\">Miller, 1956</a>; when the term <em>short-term memory</em> was favored).</p><p>One of the central issues in the many discussions over the structure of WM is how it is affected by previously learned knowledge (i.e., long-term memory; <a href=\"#ref-5\">Baddeley &amp; Hitch, 1974</a>; <a href=\"#ref-22\">Cowan, 1988</a>; <a href=\"#ref-25\">Cowan, 2019</a>; <a href=\"#ref-28\">Ericsson &amp; Kintsch, 1995</a>; <a href=\"#ref-66\">Norris, 2017</a>; <a href=\"#ref-68\">Oberauer, 2009</a>). Knowledge that emerges from long-term familiarity with particular shapes or statistically common featural combinations enables us to recognize and remember complex objects (i.e., the prototypical shape of a car, or the strokes that comprise a digit). It is widely acknowledged that such information is crucial for building WM representations (<a href=\"#ref-23\">Cowan, 1999</a>; <a href=\"#ref-11\">Brady, Konkle &amp; Alvarez, 2009</a>; <a href=\"#ref-68\">Oberauer, 2009</a>;) but there has been little attempt, if any, to mechanistically implement the role of visual knowledge in WM models in spite of abundant behavioral research in this domain (<a href=\"#ref-1\">Alvarez &amp; Cavanagh, 2004</a>; <a href=\"#ref-16\">Chen &amp; Cowan, 2005</a>; <a href=\"#ref-17\">2009</a>; Hulme, Maughen &amp; Brown, 1991; <a href=\"#ref-64\">Ngiam, et al., 2019</a>; <a href=\"#ref-65\">Ngiam, Brissenden &amp; Awh, 2019</a>; <a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>). For instance, <a href=\"#ref-1\">Alvarez &amp; Cavanagh (2004)</a> demonstrated that the number of items stored in WM is affected by stimulus complexity, with particularly poor performance for Chinese characters. <a href=\"#ref-93\">Zimmer &amp; Fischer (2020)</a> expanded on this by showing that the difficulty in remembering Chinese characters is specific to individuals who are not readers of the language. That is to say, the WM capacity for Chinese characters is higher if observers have already been trained on those characters. Moreover, <a href=\"#ref-13\">Brady, Stormer &amp; Alvarez (2016)</a> have demonstrated that evident memory capacity for natural images is higher than memory capacity for simple colors, as natural images are the stimuli that people have had the most visual experience with compared to artificial shapes. These results can be extended to verbal memory, as performance on immediate recall of a list of words is limited by the number pre-learned chunks represented in long-term knowledge (<a href=\"#ref-16\">Chen &amp; Cowan, 2005</a>; <a href=\"#ref-40\">Hulme, Maughan &amp; Brown, 1991</a>; <a href=\"#ref-41\">Hulme et al., 2003</a>).</p><p>Even prior to these findings, there has been extensive theoretical discussion of the necessity to link WM to long-term memory representations. Among the earliest memory schemes was the <a href=\"#ref-2\">Atkinson &amp; Shiffrin (1968)</a> model of memory in which representations in long-term memory could be transferred to a short-term storage if needed. Later, <a href=\"#ref-5\">Baddeley and Hitch (1974)</a> proposed the multicomponent model of WM. In this model the short-term storage for visual information (i.e., visuospatial sketchpad) was shown to be dependent on visual semantics and episodic long-term memory with a bidirectional arrow indicating the flow of information (<a href=\"#ref-4\">Baddeley, 2000</a>). This idea is also carried by theories of activated long-term memory account (<a href=\"#ref-22\">Cowan 1988</a>, <a href=\"#ref-23\">1999</a>, <a href=\"#ref-24\">2001</a>; <a href=\"#ref-28\">Ericsson &amp; Kintsch, 1995</a>) which is also discussed by <a href=\"#ref-68\">Oberauer (2009)</a>. In this account (also known as the embedded process framework), WM representations are built from the pre-existing structural representations within the long-term memory.</p><p>The above accounts (i.e., multiple components and activated long-term memory) provide a venue toward a WM mechanism integrated with long-term knowledge, but their lack of computational specificity has made it challenging to make testable predictions of <em>how</em> knowledge reflects in WM mechanisms. This includes addressing questions such as “How do we form rapid memory of novel configurations (<a href=\"#ref-51\">Lake et al., 2011</a>)?” and “Why is WM capacity higher for familiar items (<a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>)?”</p><p>To fill this gap, we implemented a computational WM model in conjunction with a visual knowledge system named Memory for Latent Representations (i.e., MLR). The proposed model simulates how latent representations of items embedded in the visual knowledge hierarchy are encoded into WM depending on their level of familiarity. Subsequently, the encoded items in WM can be retrieved by reactivating those same latent representations in the visual knowledge system. This paper outlines a candidate model for storing and retrieving visual memories in a dedicated pool of neurons and provides empirical validation of the flexibility of WM.</p></div><div><h2>The new MLR model of working memory</h2><p>The MLR model takes advantage of recent innovations in generative deep learning models to represent and reconstruct visual stimuli that are embedded in the visual knowledge. Rather than storing unidimensional attributes as in other recent working memory models (<a href=\"#ref-80\">Schneegans &amp; Bays, 2017</a>; <a href=\"#ref-56\">Lin &amp; Oberauer, 2019</a>; <a href=\"#ref-82\">Swan &amp; Wyble, 2014</a>), MLR can encode and reconstruct arbitrary shape attributes, such as a particular handwritten digit, or an article of clothing. To achieve this, MLR uses the latent distributions from the hidden layers in a pre-trained deep neural network to form WM representations. In this context, <em>latent</em> is a representation of a stimulus attribute such as the shape, color, or category of a stimulus. <a href=\"#F1\">Figure 1</a> illustrates an example of digits that can be reconstructed from a simple two-dimensional latent space in a variational autoencoder (i.e., VAE) trained on the MNIST data set, which is a collection of 60,000 hand-written digits (<a href=\"#ref-45\">Kingma &amp; Welling 2013</a>).</p><div><div><div><p><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg?width=800&amp;height=600&amp;carousel=1\"><span></span></a></p></div><ul><li><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg?download=true\">Download figure</a></li><li><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg\">Open in new tab</a></li></ul></div><div><p><span>Figure 1.</span></p><p>A latent space for MNIST digits from <a href=\"#ref-45\">Kingma &amp; Welling (2013)</a>. These digits represent the output of a generative model when a particular x,y location in the latent space is activated and then used to drive a reconstruction back to a full 28×28 image of a digit.</p></div></div><div><h3>The visual knowledge in MLR</h3><p>MLR captures two fundamental characteristics of visual knowledge that enables us to explain its interaction with WM. In the proposed framework, these two features are compression of visual information and categorical representations. Later, we show how these aspects of visual knowledge would affect WM capacity and precision of retrieved items for familiar and novel items (<a href=\"#ref-12\">Brady, et al., 2008</a>; <a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>).</p><div><h4>Compression</h4><p>The amount of visual sensory input that we receive at every moment is enormous. Therefore, efficient data compression is essential given the limited-resources available to the perceptual system. It is",
        "html": "<div><div><div>\n<p><span>\nNew Results </span></p>\n<p><span><span>doi:</span> https://doi.org/10.1101/2021.02.07.430171 </span></p>\n</div>\n<div><div><h2>Abstract</h2><p>Visual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to the human visual system and an activation-based binding pool of neurons that binds items' attributes to tokenized representations. The simulation results revealed that shape information for stimuli that the model was trained on, can be encoded and retrieved efficiently from latents in higher levels of the visual hierarchy. On the other hand, novel patterns that are completely outside the training set can be stored from a single exposure using only latents from early layers of the visual system. Moreover, a given stimulus in working memory can have multiple codes, representing specific visual features such as shape or color, in addition to categorical information. Finally, we validated our model by testing a series of predictions against behavioral results acquired from WM tasks. The model provides a compelling demonstration of visual knowledge yielding the formation of compact visual representation for efficient memory encoding.</p></div><div><h2>Introduction</h2><p>In the study of memory, working memory (WM) is thought to be responsible for temporarily holding and manipulating information. This capacity to control information is thought to be a keystone of our ability to perform complex cognitive operations. Characterizing WM is an integral part of the birth of cognitive psychology, as decades of research have centered on the question of discovering the capacity and nature of this short-term memory system (e.g., <a href=\"#ref-61\">Miller, 1956</a>; when the term <em>short-term memory</em> was favored).</p><p>One of the central issues in the many discussions over the structure of WM is how it is affected by previously learned knowledge (i.e., long-term memory; <a href=\"#ref-5\">Baddeley &amp; Hitch, 1974</a>; <a href=\"#ref-22\">Cowan, 1988</a>; <a href=\"#ref-25\">Cowan, 2019</a>; <a href=\"#ref-28\">Ericsson &amp; Kintsch, 1995</a>; <a href=\"#ref-66\">Norris, 2017</a>; <a href=\"#ref-68\">Oberauer, 2009</a>). Knowledge that emerges from long-term familiarity with particular shapes or statistically common featural combinations enables us to recognize and remember complex objects (i.e., the prototypical shape of a car, or the strokes that comprise a digit). It is widely acknowledged that such information is crucial for building WM representations (<a href=\"#ref-23\">Cowan, 1999</a>; <a href=\"#ref-11\">Brady, Konkle &amp; Alvarez, 2009</a>; <a href=\"#ref-68\">Oberauer, 2009</a>;) but there has been little attempt, if any, to mechanistically implement the role of visual knowledge in WM models in spite of abundant behavioral research in this domain (<a href=\"#ref-1\">Alvarez &amp; Cavanagh, 2004</a>; <a href=\"#ref-16\">Chen &amp; Cowan, 2005</a>; <a href=\"#ref-17\">2009</a>; Hulme, Maughen &amp; Brown, 1991; <a href=\"#ref-64\">Ngiam, et al., 2019</a>; <a href=\"#ref-65\">Ngiam, Brissenden &amp; Awh, 2019</a>; <a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>). For instance, <a href=\"#ref-1\">Alvarez &amp; Cavanagh (2004)</a> demonstrated that the number of items stored in WM is affected by stimulus complexity, with particularly poor performance for Chinese characters. <a href=\"#ref-93\">Zimmer &amp; Fischer (2020)</a> expanded on this by showing that the difficulty in remembering Chinese characters is specific to individuals who are not readers of the language. That is to say, the WM capacity for Chinese characters is higher if observers have already been trained on those characters. Moreover, <a href=\"#ref-13\">Brady, Stormer &amp; Alvarez (2016)</a> have demonstrated that evident memory capacity for natural images is higher than memory capacity for simple colors, as natural images are the stimuli that people have had the most visual experience with compared to artificial shapes. These results can be extended to verbal memory, as performance on immediate recall of a list of words is limited by the number pre-learned chunks represented in long-term knowledge (<a href=\"#ref-16\">Chen &amp; Cowan, 2005</a>; <a href=\"#ref-40\">Hulme, Maughan &amp; Brown, 1991</a>; <a href=\"#ref-41\">Hulme et al., 2003</a>).</p><p>Even prior to these findings, there has been extensive theoretical discussion of the necessity to link WM to long-term memory representations. Among the earliest memory schemes was the <a href=\"#ref-2\">Atkinson &amp; Shiffrin (1968)</a> model of memory in which representations in long-term memory could be transferred to a short-term storage if needed. Later, <a href=\"#ref-5\">Baddeley and Hitch (1974)</a> proposed the multicomponent model of WM. In this model the short-term storage for visual information (i.e., visuospatial sketchpad) was shown to be dependent on visual semantics and episodic long-term memory with a bidirectional arrow indicating the flow of information (<a href=\"#ref-4\">Baddeley, 2000</a>). This idea is also carried by theories of activated long-term memory account (<a href=\"#ref-22\">Cowan 1988</a>, <a href=\"#ref-23\">1999</a>, <a href=\"#ref-24\">2001</a>; <a href=\"#ref-28\">Ericsson &amp; Kintsch, 1995</a>) which is also discussed by <a href=\"#ref-68\">Oberauer (2009)</a>. In this account (also known as the embedded process framework), WM representations are built from the pre-existing structural representations within the long-term memory.</p><p>The above accounts (i.e., multiple components and activated long-term memory) provide a venue toward a WM mechanism integrated with long-term knowledge, but their lack of computational specificity has made it challenging to make testable predictions of <em>how</em> knowledge reflects in WM mechanisms. This includes addressing questions such as “How do we form rapid memory of novel configurations (<a href=\"#ref-51\">Lake et al., 2011</a>)?” and “Why is WM capacity higher for familiar items (<a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>)?”</p><p>To fill this gap, we implemented a computational WM model in conjunction with a visual knowledge system named Memory for Latent Representations (i.e., MLR). The proposed model simulates how latent representations of items embedded in the visual knowledge hierarchy are encoded into WM depending on their level of familiarity. Subsequently, the encoded items in WM can be retrieved by reactivating those same latent representations in the visual knowledge system. This paper outlines a candidate model for storing and retrieving visual memories in a dedicated pool of neurons and provides empirical validation of the flexibility of WM.</p></div><div><h2>The new MLR model of working memory</h2><p>The MLR model takes advantage of recent innovations in generative deep learning models to represent and reconstruct visual stimuli that are embedded in the visual knowledge. Rather than storing unidimensional attributes as in other recent working memory models (<a href=\"#ref-80\">Schneegans &amp; Bays, 2017</a>; <a href=\"#ref-56\">Lin &amp; Oberauer, 2019</a>; <a href=\"#ref-82\">Swan &amp; Wyble, 2014</a>), MLR can encode and reconstruct arbitrary shape attributes, such as a particular handwritten digit, or an article of clothing. To achieve this, MLR uses the latent distributions from the hidden layers in a pre-trained deep neural network to form WM representations. In this context, <em>latent</em> is a representation of a stimulus attribute such as the shape, color, or category of a stimulus. <a href=\"#F1\">Figure 1</a> illustrates an example of digits that can be reconstructed from a simple two-dimensional latent space in a variational autoencoder (i.e., VAE) trained on the MNIST data set, which is a collection of 60,000 hand-written digits (<a href=\"#ref-45\">Kingma &amp; Welling 2013</a>).</p><div><div><div><p><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg?width=800&amp;height=600&amp;carousel=1\"><span></span></a></p></div><ul><li><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg?download=true\">Download figure</a></li><li><a href=\"https://www.biorxiv.org/content/biorxiv/early/2021/02/08/2021.02.07.430171/F1.large.jpg\">Open in new tab</a></li></ul></div><div><p><span>Figure 1.</span></p><p>A latent space for MNIST digits from <a href=\"#ref-45\">Kingma &amp; Welling (2013)</a>. These digits represent the output of a generative model when a particular x,y location in the latent space is activated and then used to drive a reconstruction back to a full 28×28 image of a digit.</p></div></div><div><h3>The visual knowledge in MLR</h3><p>MLR captures two fundamental characteristics of visual knowledge that enables us to explain its interaction with WM. In the proposed framework, these two features are compression of visual information and categorical representations. Later, we show how these aspects of visual knowledge would affect WM capacity and precision of retrieved items for familiar and novel items (<a href=\"#ref-12\">Brady, et al., 2008</a>; <a href=\"#ref-91\">Yu et al., 1985</a>; <a href=\"#ref-92\">Zhang &amp; Simon, 1985</a>; <a href=\"#ref-93\">Zimmer &amp; Fischer, 2020</a>).</p><div><h4>Compression</h4><p>The amount of visual sensory input that we receive at every moment is enormous. Therefore, efficient data compression is essential given the limited-resources available to the perceptual system. It is",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "New Resultsdoi:https://doi.org/10.1101/2021.02.07.430171AbstractVisual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to th",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "New Resultsdoi:https://doi.org/10.1101/2021.02.07.430171AbstractVisual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to th",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "New Resultsdoi:https://doi.org/10.1101/2021.02.07.430171",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractVisual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to the human visual system and an activation-based binding po",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractVisual knowledge obtained from our lifelong experience of the world plays a critical role in our ability to build short-term memories. We propose a mechanistic explanation of how working memories are built from the latent representations of visual knowledge and can then be reconstructed. The proposed model, Memory for Latent Representations (MLR), features a variational autoencoder with an architecture that corresponds broadly to the human visual system and an activation-based binding po",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "IntroductionIn the study of memory, working memory (WM) is thought to be responsible for temporarily holding and manipulating information. This capacity to control information is thought to be a keystone of our ability to perform complex cognitive operations. Characterizing WM is an integral part of the birth of cognitive psychology, as decades of research have centered on the question of discovering the capacity and nature of this short-term memory system (e.g.,Miller, 1956; when the termshort-",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The new MLR model of working memoryThe MLR model takes advantage of recent innovations in generative deep learning models to represent and reconstruct visual stimuli that are embedded in the visual knowledge. Rather than storing unidimensional attributes as in other recent working memory models (Schneegans & Bays, 2017;Lin & Oberauer, 2019;Swan & Wyble, 2014), MLR can encode and reconstruct arbitrary shape attributes, such as a particular handwritten digit, or an article of clothing. To achieve ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Download figureOpen in new tabFigure 1.A latent space for MNIST digits fromKingma & Welling (2013). These digits represent the output of a generative model when a particular x,y location in the latent space is activated and then used to drive a reconstruction back to a full 28×28 image of a digit.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Download figureOpen in new tab",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure 1.A latent space for MNIST digits fromKingma & Welling (2013). These digits represent the output of a generative model when a particular x,y location in the latent space is activated and then used to drive a reconstruction back to a full 28×28 image of a digit.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The visual knowledge in MLRMLR captures two fundamental characteristics of visual knowledge that enables us to explain its interaction with WM. In the proposed framework, these two features are compression of visual information and categorical representations. Later, we show how these aspects of visual knowledge would affect WM capacity and precision of retrieved items for familiar and novel items (Brady, et al., 2008;Yu et al., 1985;Zhang & Simon, 1985;Zimmer & Fischer, 2020).CompressionThe amo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "CompressionThe amount of visual sensory input that we receive at every moment is enormous. Therefore, efficient data compression is essential given the limited-resources available to the perceptual system. It is",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "The new MLR model of working memory",
              "id": ""
            },
            {
              "level": "h3",
              "text": "The visual knowledge in MLR",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Compression",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.73.012340",
      "title": "Operator quantum error-correcting subsystems for self-correcting quantum memories",
      "author": "Dave  Bacon, University of Washington, 98195, Seattle, Washington, USA",
      "published_date": "2006-01-30T00:00:00.000Z",
      "content": {
        "text": "<div><div><section><h4>Abstract\n<i></i>\n<i></i>\n</h4><div><p>The most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contrast, for operator error-correcting subsystems, the correction procedure need not undo the error which has occurred, but instead one must perform corrections only modulo the subsystem structure. This does not lead to codes which differ from subspace codes, but does lead to recovery routines which explicitly make use of the subsystem structure. Here we present two examples of such operator error-correcting subsystems. These examples are motivated by simple spatially local Hamiltonians on square and cubic lattices. In three dimensions we provide evidence, in the form a simple mean field theory, that our Hamiltonian gives rise to a system which is self-correcting. Such a system will be a natural high-temperature quantum memory, robust to noise without external intervening quantum error-correction procedures.</p><ul><li>Received 30 June 2005</li></ul><p>DOI:https://doi.org/10.1103/PhysRevA.73.012340</p><section><p>©2006 American Physical Society</p></section></div></section><section><h4>Authors &amp; Affiliations\n<i></i>\n<i></i>\n</h4><div><p><a href=\"/search/field/author/Dave%20Bacon\">Dave Bacon</a></p><ul><li><sup></sup>Department of Computer Science &amp; Engineering, University of Washington, Seattle, Washington 98195, USA</li></ul></div></section><section><h4>Article Text (Subscription Required)\n<i></i>\n<i></i>\n</h4><p>Click to Expand</p></section><section><h4>References (Subscription Required)\n<i></i>\n<i></i>\n</h4><p>Click to Expand</p></section></div>",
        "html": "<div><div><section><h4>Abstract\n<i></i>\n<i></i>\n</h4><div><p>The most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contrast, for operator error-correcting subsystems, the correction procedure need not undo the error which has occurred, but instead one must perform corrections only modulo the subsystem structure. This does not lead to codes which differ from subspace codes, but does lead to recovery routines which explicitly make use of the subsystem structure. Here we present two examples of such operator error-correcting subsystems. These examples are motivated by simple spatially local Hamiltonians on square and cubic lattices. In three dimensions we provide evidence, in the form a simple mean field theory, that our Hamiltonian gives rise to a system which is self-correcting. Such a system will be a natural high-temperature quantum memory, robust to noise without external intervening quantum error-correction procedures.</p><ul><li>Received 30 June 2005</li></ul><p>DOI:https://doi.org/10.1103/PhysRevA.73.012340</p><section><p>©2006 American Physical Society</p></section></div></section><section><h4>Authors &amp; Affiliations\n<i></i>\n<i></i>\n</h4><div><p><a href=\"/search/field/author/Dave%20Bacon\">Dave Bacon</a></p><ul><li><sup></sup>Department of Computer Science &amp; Engineering, University of Washington, Seattle, Washington 98195, USA</li></ul></div></section><section><h4>Article Text (Subscription Required)\n<i></i>\n<i></i>\n</h4><p>Click to Expand</p></section><section><h4>References (Subscription Required)\n<i></i>\n<i></i>\n</h4><p>Click to Expand</p></section></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "AbstractThe most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contr",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractThe most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contr",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractThe most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contr",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error-correcting codes, one requires the ability to apply a procedure which exactly reverses on the error-correcting subspace any correctable error. In contrast, for",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "©2006 American Physical Society",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Authors & AffiliationsDave BaconDepartment of Computer Science & Engineering, University of Washington, Seattle, Washington 98195, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Dave BaconDepartment of Computer Science & Engineering, University of Washington, Seattle, Washington 98195, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Article Text (Subscription Required)Click to Expand",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "References (Subscription Required)Click to Expand",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h4",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Authors & Affiliations",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Article Text (Subscription Required)",
              "id": ""
            },
            {
              "level": "h4",
              "text": "References (Subscription Required)",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC4471100/",
      "title": "Computational Fact Checking from Knowledge Networks",
      "author": "",
      "published_date": "2015-06-17T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>Traditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this approach is feasible with efficient computational techniques. We evaluate this approach by examining tens of thousands of claims related to history, entertainment, geography, and biographical information using a public knowledge graph extracted from Wikipedia. Statements independently known to be true consistently receive higher support via our method than do false ones. These findings represent a significant step toward scalable computational fact-checking methods that may one day mitigate the spread of harmful misinformation.</p></section><section><h2>Introduction</h2>\n<p>Online communication platforms, in particular social media, have created a situation in which the proverbial lie “can travel the world before the truth can get its boots on.” Misinformation [<a href=\"#pone.0128193.ref001\">1</a>], astroturf [<a href=\"#pone.0128193.ref002\">2</a>], spam [<a href=\"#pone.0128193.ref003\">3</a>], and outright fraud [<a href=\"#pone.0128193.ref004\">4</a>] have become widespread. They are now seemingly unavoidable components of our online information ecology [<a href=\"#pone.0128193.ref005\">5</a>] that jeopardize our ability as a society to make rapid and informed decisions [<a href=\"#pone.0128193.ref006\">6</a>–<a href=\"#pone.0128193.ref010\">10</a>].</p>\n<p>While attempts to partially automate the detection of various forms of misinformation are burgeoning [<a href=\"#pone.0128193.ref011\">11</a>–<a href=\"#pone.0128193.ref015\">15</a>], automated reasoning methods are hampered by the inherent ambiguity of language and by deliberate deception. However, under certain conditions, reliable knowledge transmission can take place online [<a href=\"#pone.0128193.ref016\">16</a>]. For example, Wikipedia, the crowd-sourced online encyclopedia, has been shown to be nearly as reliable as traditional encyclopedias, even though it covers many more topics [<a href=\"#pone.0128193.ref017\">17</a>]. It now serves as a large-scale knowledge repository for millions of individuals, who can also contribute to its content in an open way. Vandalism, bias, distortions, and outright lies are frequently repaired in a matter of minutes [<a href=\"#pone.0128193.ref018\">18</a>]. Its continuous editing process even indicates signs of collective human intelligence [<a href=\"#pone.0128193.ref019\">19</a>].</p>\n<p>Here we show that we can leverage any collection of factual human knowledge, such as Wikipedia, for automatic fact checking [<a href=\"#pone.0128193.ref020\">20</a>]. Loosely inspired by the principle of epistemic closure [<a href=\"#pone.0128193.ref021\">21</a>], we computationally gauge the support for statements by mining the connectivity patterns on a knowledge graph. Our initial focus is on computing the support of simple statements of fact using a large-scale knowledge graph obtained from Wikipedia. More in general, fact checking can be seen as a special case of link prediction in knowledge graphs [<a href=\"#pone.0128193.ref022\">22</a>].</p>\n<section><h3>Knowledge Graphs</h3>\n<p>Let a <em>statement of fact</em> be represented by a subject-predicate-object triple, e.g., (“Socrates,” “is a,” “person”). A set of such triples can be combined to produce a <em>knowledge graph</em> (KG), where nodes denote <em>entities</em> (i.e., subjects or objects of statements), and edges denote predicates. Given a set of statements that has been extracted from a knowledge repository—such as the aforementioned Wikipedia—the resulting KG network represents all factual relations among entities mentioned in those statements. Given a new statement, we expect it to be true if it exists as an edge of the KG, or if there is a short path linking its subject to its object within the KG. If, however, the statement is untrue, there should be neither edges nor short paths that connect subject and object.</p>\n<p>In a KG distinct paths between the same subject and object typically provide different factual support for the statement those nodes represent, even if the paths contain the same number of intermediate nodes. For example, paths that contain generic entities, such as “United States” or “Male,” provide weaker support because these nodes link to many entities and thus yield little specific information. Conversely, paths comprised of very specific entities, such as “positronic flux capacitor” or “terminal deoxynucleotidyl transferase,” provide stronger support. A fundamental insight that underpins our approach is that the definition of path length used for fact checking should account for such information-theoretic considerations.</p>\n<p>To test our method we use the DBpedia database [<a href=\"#pone.0128193.ref023\">23</a>], which consists of all factual statements extracted from Wikipedia’s “infoboxes” (see <a href=\"#pone.0128193.g001\">Fig 1(a)</a>). From this data we build the large-scale <em>Wikipedia Knowledge Graph</em> (WKG), with 3 million entity nodes linked by approximately 23 million edges (see <a href=\"#sec010\">Materials and Methods</a>). Since we use only facts within infoboxes, the WKG contains the most uncontroversial information available on Wikipedia. This conservative approach is employed to ensure that our process relies as much as possible on a human-annotated, collectively-vetted factual basis. The WKG could be augmented with automatic methods to infer facts from text and other unstructured sources available online. Indeed, other teams have proposed methods to infer knowledge from text [<a href=\"#pone.0128193.ref024\">24</a>] to be employed in large and sophisticated rule-based inference models [<a href=\"#pone.0128193.ref024\">24</a>–<a href=\"#pone.0128193.ref026\">26</a>]. Here we focus on the feasibility of automatic fact checking using <em>simple</em> network models that leverage DBpedia. For this initial goal, we do not need to enhance the WKG, but such improvements can later be incorporated.</p>\n<figure><h4>Fig 1. Using Wikipedia to fact-check statements.</h4>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=4471100_pone.0128193.g001.jpg\"></a></p>\n<figcaption><p>\n<strong>(a)</strong> To populate the knowledge graph with facts we use structured information contained in the ‘infoboxes’ of Wikipedia articles (in the figure, the infobox of the article about <em>Barack Obama</em>). <strong>(b)</strong> Using the Wikipedia Knowledge Graph, computing the truth value of a subject-predicate-object statement amounts to finding a path between subject and object entities. In the diagram we plot the shortest path returned by our method for the statement “<em>Barack Obama</em> is a <em>muslim</em>.” Numbers in parentheses indicate the degree of the nodes. The path traverses high-degree nodes representing generic entities, such as <em>Canada</em>, and is assigned a low truth value.</p></figcaption></figure></section><section><h3>Semantic Proximity from Transitive Closure</h3>\n<p>Let the WKG be an undirected graph <em>G</em> = (<em>V</em>, <em>E</em>) where <em>V</em> is a set of concept nodes and <em>E</em> is a set of predicate edges (see <a href=\"#sec010\">Materials and Methods</a>). Two nodes <em>v</em>, <em>w</em> ∈ <em>V</em> are said to be <em>adjacent</em> if there is an edge between them (<em>v</em>, <em>w</em>) ∈ <em>E</em>. They are said to be <em>connected</em> if there a sequence of <em>n</em> ≥ 2 nodes <em>v</em> = <em>v</em>\n<sub>1</sub>, <em>v</em>\n<sub>2</sub>, … <em>v</em>\n<sub><em>n</em></sub> = <em>w</em>, such that, for <em>i</em> = 1, …, <em>n</em>−1 the nodes <em>v</em>\n<sub><em>i</em></sub> and <em>v</em>\n<sub><em>i</em>+1</sub> are adjacent. The <em>transitive closure</em> of <em>G</em> is <em>G</em>* = (<em>V</em>, <em>E</em>*) where the set of edges is closed under adjacency, that is, two nodes are adjacent in <em>G</em>* <em>iff</em> they are connected in <em>G</em> via at least one path. This standard notion of closure has been extended to weighted graphs, allowing adjacency to be generalized by measures of path length [<a href=\"#pone.0128193.ref027\">27</a>], such as the semantic proximity for the WKG we introduce next.</p>\n<p>The truth value <em>τ</em>(<em>e</em>) ∈ [0, 1] of a new statement <em>e</em> = (<em>s</em>, <em>p</em>, <em>o</em>) is derived from a transitive closure of the WKG. More specifically, the truth value is obtained via a path evaluation function: <em>τ</em>(<em>e</em>) = max 𝒲(<em>P</em>\n<sub><em>s</em>,<em>o</em></sub>). This function maps the set of possible paths connecting <em>s</em> and <em>o</em> to a truth value <em>τ</em>. A path has the form <em>P</em>\n<sub><em>s</em>,<em>o</em></sub> = <em>v</em>\n<sub>1</sub>\n<em>v</em>\n<sub>2</sub>…<em>v</em>\n<sub><em>n</em></sub>, where <em>v</em>\n<sub><em>i</em></sub> is an entity node, (<em>v</em>\n<sub><em>i</em></sub>, <em>v</em>\n<sub><em>i</em>+1</sub>) is an edge, <em>n</em> is the path length measured by the number of its constituent nodes, <em>v</em>\n<sub>1</sub> = <em>s</em>, and <em>v</em>\n<sub><em>n</em></sub> = <em>o</em>. Various characteristics of a path can be taken as evidence in support of the truth value of <em>e</em>. Here we use the <em>generality</em> of the entities along a path as a measure of its length, which is in turn aggregated to define a <em>semantic proximity</em>:\n</p>\n<table><tbody><tr>\n<td></td>\n<td>(1)</td>\n</tr></tbody></table>\n<p>\nwhere <em>k</em>(<em>v</em>) is the degree o",
        "html": "<div><div>\n<main>\n<article><section></section><section><section><h2>Abstract</h2>\n<p>Traditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this approach is feasible with efficient computational techniques. We evaluate this approach by examining tens of thousands of claims related to history, entertainment, geography, and biographical information using a public knowledge graph extracted from Wikipedia. Statements independently known to be true consistently receive higher support via our method than do false ones. These findings represent a significant step toward scalable computational fact-checking methods that may one day mitigate the spread of harmful misinformation.</p></section><section><h2>Introduction</h2>\n<p>Online communication platforms, in particular social media, have created a situation in which the proverbial lie “can travel the world before the truth can get its boots on.” Misinformation [<a href=\"#pone.0128193.ref001\">1</a>], astroturf [<a href=\"#pone.0128193.ref002\">2</a>], spam [<a href=\"#pone.0128193.ref003\">3</a>], and outright fraud [<a href=\"#pone.0128193.ref004\">4</a>] have become widespread. They are now seemingly unavoidable components of our online information ecology [<a href=\"#pone.0128193.ref005\">5</a>] that jeopardize our ability as a society to make rapid and informed decisions [<a href=\"#pone.0128193.ref006\">6</a>–<a href=\"#pone.0128193.ref010\">10</a>].</p>\n<p>While attempts to partially automate the detection of various forms of misinformation are burgeoning [<a href=\"#pone.0128193.ref011\">11</a>–<a href=\"#pone.0128193.ref015\">15</a>], automated reasoning methods are hampered by the inherent ambiguity of language and by deliberate deception. However, under certain conditions, reliable knowledge transmission can take place online [<a href=\"#pone.0128193.ref016\">16</a>]. For example, Wikipedia, the crowd-sourced online encyclopedia, has been shown to be nearly as reliable as traditional encyclopedias, even though it covers many more topics [<a href=\"#pone.0128193.ref017\">17</a>]. It now serves as a large-scale knowledge repository for millions of individuals, who can also contribute to its content in an open way. Vandalism, bias, distortions, and outright lies are frequently repaired in a matter of minutes [<a href=\"#pone.0128193.ref018\">18</a>]. Its continuous editing process even indicates signs of collective human intelligence [<a href=\"#pone.0128193.ref019\">19</a>].</p>\n<p>Here we show that we can leverage any collection of factual human knowledge, such as Wikipedia, for automatic fact checking [<a href=\"#pone.0128193.ref020\">20</a>]. Loosely inspired by the principle of epistemic closure [<a href=\"#pone.0128193.ref021\">21</a>], we computationally gauge the support for statements by mining the connectivity patterns on a knowledge graph. Our initial focus is on computing the support of simple statements of fact using a large-scale knowledge graph obtained from Wikipedia. More in general, fact checking can be seen as a special case of link prediction in knowledge graphs [<a href=\"#pone.0128193.ref022\">22</a>].</p>\n<section><h3>Knowledge Graphs</h3>\n<p>Let a <em>statement of fact</em> be represented by a subject-predicate-object triple, e.g., (“Socrates,” “is a,” “person”). A set of such triples can be combined to produce a <em>knowledge graph</em> (KG), where nodes denote <em>entities</em> (i.e., subjects or objects of statements), and edges denote predicates. Given a set of statements that has been extracted from a knowledge repository—such as the aforementioned Wikipedia—the resulting KG network represents all factual relations among entities mentioned in those statements. Given a new statement, we expect it to be true if it exists as an edge of the KG, or if there is a short path linking its subject to its object within the KG. If, however, the statement is untrue, there should be neither edges nor short paths that connect subject and object.</p>\n<p>In a KG distinct paths between the same subject and object typically provide different factual support for the statement those nodes represent, even if the paths contain the same number of intermediate nodes. For example, paths that contain generic entities, such as “United States” or “Male,” provide weaker support because these nodes link to many entities and thus yield little specific information. Conversely, paths comprised of very specific entities, such as “positronic flux capacitor” or “terminal deoxynucleotidyl transferase,” provide stronger support. A fundamental insight that underpins our approach is that the definition of path length used for fact checking should account for such information-theoretic considerations.</p>\n<p>To test our method we use the DBpedia database [<a href=\"#pone.0128193.ref023\">23</a>], which consists of all factual statements extracted from Wikipedia’s “infoboxes” (see <a href=\"#pone.0128193.g001\">Fig 1(a)</a>). From this data we build the large-scale <em>Wikipedia Knowledge Graph</em> (WKG), with 3 million entity nodes linked by approximately 23 million edges (see <a href=\"#sec010\">Materials and Methods</a>). Since we use only facts within infoboxes, the WKG contains the most uncontroversial information available on Wikipedia. This conservative approach is employed to ensure that our process relies as much as possible on a human-annotated, collectively-vetted factual basis. The WKG could be augmented with automatic methods to infer facts from text and other unstructured sources available online. Indeed, other teams have proposed methods to infer knowledge from text [<a href=\"#pone.0128193.ref024\">24</a>] to be employed in large and sophisticated rule-based inference models [<a href=\"#pone.0128193.ref024\">24</a>–<a href=\"#pone.0128193.ref026\">26</a>]. Here we focus on the feasibility of automatic fact checking using <em>simple</em> network models that leverage DBpedia. For this initial goal, we do not need to enhance the WKG, but such improvements can later be incorporated.</p>\n<figure><h4>Fig 1. Using Wikipedia to fact-check statements.</h4>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=4471100_pone.0128193.g001.jpg\"></a></p>\n<figcaption><p>\n<strong>(a)</strong> To populate the knowledge graph with facts we use structured information contained in the ‘infoboxes’ of Wikipedia articles (in the figure, the infobox of the article about <em>Barack Obama</em>). <strong>(b)</strong> Using the Wikipedia Knowledge Graph, computing the truth value of a subject-predicate-object statement amounts to finding a path between subject and object entities. In the diagram we plot the shortest path returned by our method for the statement “<em>Barack Obama</em> is a <em>muslim</em>.” Numbers in parentheses indicate the degree of the nodes. The path traverses high-degree nodes representing generic entities, such as <em>Canada</em>, and is assigned a low truth value.</p></figcaption></figure></section><section><h3>Semantic Proximity from Transitive Closure</h3>\n<p>Let the WKG be an undirected graph <em>G</em> = (<em>V</em>, <em>E</em>) where <em>V</em> is a set of concept nodes and <em>E</em> is a set of predicate edges (see <a href=\"#sec010\">Materials and Methods</a>). Two nodes <em>v</em>, <em>w</em> ∈ <em>V</em> are said to be <em>adjacent</em> if there is an edge between them (<em>v</em>, <em>w</em>) ∈ <em>E</em>. They are said to be <em>connected</em> if there a sequence of <em>n</em> ≥ 2 nodes <em>v</em> = <em>v</em>\n<sub>1</sub>, <em>v</em>\n<sub>2</sub>, … <em>v</em>\n<sub><em>n</em></sub> = <em>w</em>, such that, for <em>i</em> = 1, …, <em>n</em>−1 the nodes <em>v</em>\n<sub><em>i</em></sub> and <em>v</em>\n<sub><em>i</em>+1</sub> are adjacent. The <em>transitive closure</em> of <em>G</em> is <em>G</em>* = (<em>V</em>, <em>E</em>*) where the set of edges is closed under adjacency, that is, two nodes are adjacent in <em>G</em>* <em>iff</em> they are connected in <em>G</em> via at least one path. This standard notion of closure has been extended to weighted graphs, allowing adjacency to be generalized by measures of path length [<a href=\"#pone.0128193.ref027\">27</a>], such as the semantic proximity for the WKG we introduce next.</p>\n<p>The truth value <em>τ</em>(<em>e</em>) ∈ [0, 1] of a new statement <em>e</em> = (<em>s</em>, <em>p</em>, <em>o</em>) is derived from a transitive closure of the WKG. More specifically, the truth value is obtained via a path evaluation function: <em>τ</em>(<em>e</em>) = max 𝒲(<em>P</em>\n<sub><em>s</em>,<em>o</em></sub>). This function maps the set of possible paths connecting <em>s</em> and <em>o</em> to a truth value <em>τ</em>. A path has the form <em>P</em>\n<sub><em>s</em>,<em>o</em></sub> = <em>v</em>\n<sub>1</sub>\n<em>v</em>\n<sub>2</sub>…<em>v</em>\n<sub><em>n</em></sub>, where <em>v</em>\n<sub><em>i</em></sub> is an entity node, (<em>v</em>\n<sub><em>i</em></sub>, <em>v</em>\n<sub><em>i</em>+1</sub>) is an edge, <em>n</em> is the path length measured by the number of its constituent nodes, <em>v</em>\n<sub>1</sub> = <em>s</em>, and <em>v</em>\n<sub><em>n</em></sub> = <em>o</em>. Various characteristics of a path can be taken as evidence in support of the truth value of <em>e</em>. Here we use the <em>generality</em> of the entities along a path as a measure of its length, which is in turn aggregated to define a <em>semantic proximity</em>:\n</p>\n<table><tbody><tr>\n<td></td>\n<td>(1)</td>\n</tr></tbody></table>\n<p>\nwhere <em>k</em>(<em>v</em>) is the degree o",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "AbstractTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this ",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "AbstractTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionOnline communication platforms, in particular social media, have created a situation in which the proverbial lie “can travel the world before the truth can get its boots on.” Misinformation [1], astroturf [2], spam [3], and outright fraud [4] have become widespread. They are now seemingly unavoidable components of our online information ecology [5] that jeopardize our ability as a society to make rapid and informed decisions [6–10].While attempts to partially automate the detection o",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Knowledge GraphsLet astatement of factbe represented by a subject-predicate-object triple, e.g., (“Socrates,” “is a,” “person”). A set of such triples can be combined to produce aknowledge graph(KG), where nodes denoteentities(i.e., subjects or objects of statements), and edges denote predicates. Given a set of statements that has been extracted from a knowledge repository—such as the aforementioned Wikipedia—the resulting KG network represents all factual relations among entities mentioned in t",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Semantic Proximity from Transitive ClosureLet the WKG be an undirected graphG= (V,E) whereVis a set of concept nodes andEis a set of predicate edges (seeMaterials and Methods). Two nodesv,w∈Vare said to beadjacentif there is an edge between them (v,w) ∈E. They are said to beconnectedif there a sequence ofn≥ 2 nodesv=v1,v2, …vn=w, such that, fori= 1, …,n−1 the nodesviandvi+1are adjacent. Thetransitive closureofGisG* = (V,E*) where the set of edges is closed under adjacency, that is, two nodes are",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Knowledge Graphs",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Fig 1. Using Wikipedia to fact-check statements.",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Semantic Proximity from Transitive Closure",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://positivepsychology.com/self-regulation/",
      "title": "What is Self-Regulation? (+95 Skills and Strategies)",
      "author": "Courtney E. Ackerman, MA.",
      "published_date": "2018-07-03T07:30:45.000Z",
      "content": {
        "text": "<div><div>\n<p>Why don’t adults always do exactly what we feel like doing, when we feel like doing it?</p>\n<p>This is a question that you might hear from kids, and it perfectly encapsulates what baffles them about adults.</p>\n<p>As adults, we pretty much have free rein to do whatever we want, whenever we want. The vast majority of us won’t get arrested for not showing up to work, and no one will haul us off to prison for eating cake for breakfast.</p>\n<p>So, why do we show up for work? Why don’t we eat cake for breakfast?</p>\n<p>Perhaps the better question is, how do we keep ourselves from shirking work when we don’t want to go? How do we refrain from eating cake for breakfast and eating healthy, less-delicious food instead?</p>\n<p>The answer is<em> self-regulation.</em> It’s a vital skill, but it’s also something we generally do without much thought.</p>\n<p>If you want to learn more about what self-regulation is, how we make the decisions we make, and why we are more susceptible to temptation at certain moments, read on. We also provide plenty of resources for teaching self-regulation skills to both children and adults.</p>\n<p>Before you continue, we thought you might like to <a href=\"https://pro.positivepsychology.com/opt-in/3-free-self-compassion-tools/\">download our three Self-Compassion Exercises for free</a>. These detailed, science-based exercises will not only help you increase the compassion and kindness you show yourself but will also give you the tools to help your clients, students, or employees show more compassion to themselves.</p>\n<div>\n<h2>This Article Contains:</h2>\n<ul>\n<li><a href=\"#what-self-regulation\">What Is Self-Regulation?</a></li>\n<li><a href=\"#theory-self-regulation\">What Is Self-Regulation Theory?</a></li>\n<li><a href=\"#psychology-self-regulation\">The Psychology of Self-Regulation</a></li>\n<li><a href=\"#model-self-regulation\">The Self-Regulatory Model</a></li>\n<li><a href=\"#why-self-regulation\">Why Self-Regulation Is Important for Wellbeing</a></li>\n<li><a href=\"#test-self-regulation\">Self-Regulation Test and Assessment</a></li>\n<li><a href=\"#child-self-regulation\">Early Childhood and Child Development</a></li>\n<li><a href=\"#adults-self-regulation\">Self-Regulation in Adults</a></li>\n<li><a href=\"#activities-self-regulation\">Activities and Worksheets for Training Self-Regulation (PDFs)</a></li>\n<li><a href=\"#resources-self-regulation\">Further Resources, Interventions, and Tools</a></li>\n<li><a href=\"#home\">A Take-Home Message</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n</div>\n<h2>What Is Self-Regulation?</h2>\n<p>Andrea Bell from <a href=\"https://www.goodtherapy.org/\">GoodTherapy.org</a> has a straightforward definition of self-regulation: It’s “control [of oneself] by oneself” (2016).</p>\n<p>Self-control can be used by a wide range of organisms and organizations, but for our purposes, we’ll focus on the psychological concept of self-regulation.</p>\n<p>As Bell also notes:</p>\n<blockquote><p>“Someone who has good emotional self-regulation has the ability to keep their emotions in check. They can resist impulsive behaviors that might worsen their situation, and they can cheer themselves up when they’re feeling down. They have a flexible range of emotional and behavioral responses that are well matched to the demands of their environment”</p></blockquote>\n<p>(2016).</p>\n<p>The goal of most types of therapy is to improve an individual’s ability to self-regulate and to gain (or regain) a sense of control over one’s behavior and life. Psychologists might be referring to one of two things when they use the term “self-regulation”: behavioral self-regulation or <a href=\"https://positivepsychology.com/emotion-regulation-worksheets-strategies-dbt-skills/\">emotional self-regulation</a>. We’ll explore the difference between the two below.</p>\n<h3>What Is Behavioral Self-Regulation?</h3>\n<p>Behavioral self-regulation is “the ability to act in your long-term best interest, consistent with your deepest values” (Stosny, 2011). It is what allows us to feel one way but act another.</p>\n<p>If you’ve ever dreaded getting up and going to work in the morning but convinced yourself to do it anyway after remembering your goals (e.g., a raise, a promotion) or your basic needs (e.g., food, shelter), you displayed effective behavioral self-regulation.</p>\n<h3>What Is Emotional Self-Regulation?</h3>\n<p>On the other hand, emotional self-regulation involves control of—or, at least, influence over—your emotions.</p>\n<p>If you had ever talked yourself out of a bad mood or calmed yourself down when you were angry, you were displaying effective emotional self-regulation.</p>\n<h2>What is Self-Regulation Theory?</h2>\n<p>Self-regulation theory (SRT) simply outlines the process and components involved when we decide what to think, feel, say, and do. It is particularly salient in the context of making a healthy choice when we have a strong desire to do the opposite (e.g., refraining from eating an entire pizza just because it tastes good).</p>\n<p>According to modern SRT expert Roy Baumeister, there are four components involved (2007):</p>\n<ol>\n<li><em>Standards</em> of desirable behavior;</li>\n<li><em><a href=\"https://positivepsychology.com/self-motivation/\">Motivation</a></em> to meet standards;</li>\n<li><em>Monitoring </em>of situations and thoughts that precede breaking standards;</li>\n<li><em><a href=\"https://positivepsychology.com/psychology-of-willpower/\">Willpower</a> </em>allowing one’s internal strength to control urges.</li>\n</ol>\n<p>These four components interact to determine our self-regulatory activity at any given moment. According to SRT, our behavior is determined by our personal standards of good behavior, our motivation to meet those standards, the degree to which we are consciously aware of our circumstances and our actions, and the extent of our willpower to resist temptations and choose the best path.<br/>\n</p>\n<h2>The Psychology of Self-Regulation</h2>\n<p>According to <a href=\"https://positivepsychology.com/bandura-self-efficacy/\">Albert Bandura</a>, an expert on <a href=\"https://positivepsychology.com/self-efficacy/\">self-efficacy</a> and a leading researcher of SRT, self-regulation is a continuously active process in which we:</p>\n<ol>\n<li>Monitor our own behavior, the influences on our behavior, and the consequences of our behavior;</li>\n<li>Judge our behavior in relation to our own personal standards and broader, more contextual standards;</li>\n<li>React to our own behavior (i.e., what we think and how we feel about our behavior) (1991).</li>\n</ol>\n<p>Bandura also notes that self-efficacy plays a significant role in this process, exerting its influence on our thoughts, feelings, motivations, and actions.</p>\n<p>A quick thought experiment can show the significance of self-efficacy:</p>\n<p>Imagine two people who are highly motivated to lose weight. They are both actively monitoring their food intake and their exercise, and they have specific, measurable goals that they have set for themselves.</p>\n<p>One of them has high self-efficacy and believes he can lose weight if he puts in the effort to do so. The other has low self-efficacy and feels that there’s no way he can hold to his prescribed weight loss plan.</p>\n<p>Who do you think will be better able to say no to second helpings and decadent desserts? Which of them do you think will be more successful in getting up early to exercise each morning?</p>\n<p>We can say with reasonable certainty that the man with higher self-efficacy is likely to be more effective, even if both men start with the exact same standards, motivation, monitoring, and willpower.</p>\n<p>Barry Zimmerman, another big name in SRT research, put forth his own theory founded on self-regulation: self-regulated learning theory.</p>\n<p>We explore this further in <a href=\"https://pro.positivepsychology.com/product/the-science-of-self-acceptance/\">The Science of Self-Acceptance Masterclass©</a>.</p>\n<h3>What is Self-Regulated Learning?</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408091/\">Self-regulated learning</a> (SRL) refers to the process a student engages in when she takes responsibility for her own learning and applies herself to academic success (Zimmerman, 2002).</p>\n<p>This process happens in three steps:</p>\n<ol>\n<li>Planning: The student plans her task, sets goals, outlines strategies to tackle the task, and/or creates a schedule for the task;</li>\n<li>Monitoring: In this stage, the student puts her plans into action and closely monitors her performance and her experience with the methods she chose;</li>\n<li>Reflection: Finally, after the task is complete and the results are in, the student reflects on how well she did and why she performed the way she did (Zimmerman, 2002).</li>\n</ol>\n<p>When students take initiative and regulate their own learning, they gain deeper insights into how they learn, what works best for them, and, ultimately, they perform at a higher level. This improvement springs from the many opportunities to learn during each phase:</p>\n<ol>\n<li>In the planning phase, students have an opportunity to work on their self-assessment and learn how to pick the best strategies for success;</li>\n<li>In the monitoring phase, students get experience implementing the strategies they chose and making real-time adjustments to their plans as needed;</li>\n<li>In the reflection phase, students synthesize everything they learned and reflect on their experience, learning what works for them and what should be altered or replaced with a new strategy.</li>\n</ol>\n<h2>The Self-Regulatory Model</h2>\n<p>It can be useful to consider the self-regulatory model to better understand SRT.</p>\n<p>While the model is specific to health- and illness-related (rather than emotional) self-regulation, it is still a good representation of the complex processes at work during self-regulation of any kind.</p>\n<p>The figure to the right shows how the model works:</p>",
        "html": "<div><div>\n<p>Why don’t adults always do exactly what we feel like doing, when we feel like doing it?</p>\n<p>This is a question that you might hear from kids, and it perfectly encapsulates what baffles them about adults.</p>\n<p>As adults, we pretty much have free rein to do whatever we want, whenever we want. The vast majority of us won’t get arrested for not showing up to work, and no one will haul us off to prison for eating cake for breakfast.</p>\n<p>So, why do we show up for work? Why don’t we eat cake for breakfast?</p>\n<p>Perhaps the better question is, how do we keep ourselves from shirking work when we don’t want to go? How do we refrain from eating cake for breakfast and eating healthy, less-delicious food instead?</p>\n<p>The answer is<em> self-regulation.</em> It’s a vital skill, but it’s also something we generally do without much thought.</p>\n<p>If you want to learn more about what self-regulation is, how we make the decisions we make, and why we are more susceptible to temptation at certain moments, read on. We also provide plenty of resources for teaching self-regulation skills to both children and adults.</p>\n<p>Before you continue, we thought you might like to <a href=\"https://pro.positivepsychology.com/opt-in/3-free-self-compassion-tools/\">download our three Self-Compassion Exercises for free</a>. These detailed, science-based exercises will not only help you increase the compassion and kindness you show yourself but will also give you the tools to help your clients, students, or employees show more compassion to themselves.</p>\n<div>\n<h2>This Article Contains:</h2>\n<ul>\n<li><a href=\"#what-self-regulation\">What Is Self-Regulation?</a></li>\n<li><a href=\"#theory-self-regulation\">What Is Self-Regulation Theory?</a></li>\n<li><a href=\"#psychology-self-regulation\">The Psychology of Self-Regulation</a></li>\n<li><a href=\"#model-self-regulation\">The Self-Regulatory Model</a></li>\n<li><a href=\"#why-self-regulation\">Why Self-Regulation Is Important for Wellbeing</a></li>\n<li><a href=\"#test-self-regulation\">Self-Regulation Test and Assessment</a></li>\n<li><a href=\"#child-self-regulation\">Early Childhood and Child Development</a></li>\n<li><a href=\"#adults-self-regulation\">Self-Regulation in Adults</a></li>\n<li><a href=\"#activities-self-regulation\">Activities and Worksheets for Training Self-Regulation (PDFs)</a></li>\n<li><a href=\"#resources-self-regulation\">Further Resources, Interventions, and Tools</a></li>\n<li><a href=\"#home\">A Take-Home Message</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n</div>\n<h2>What Is Self-Regulation?</h2>\n<p>Andrea Bell from <a href=\"https://www.goodtherapy.org/\">GoodTherapy.org</a> has a straightforward definition of self-regulation: It’s “control [of oneself] by oneself” (2016).</p>\n<p>Self-control can be used by a wide range of organisms and organizations, but for our purposes, we’ll focus on the psychological concept of self-regulation.</p>\n<p>As Bell also notes:</p>\n<blockquote><p>“Someone who has good emotional self-regulation has the ability to keep their emotions in check. They can resist impulsive behaviors that might worsen their situation, and they can cheer themselves up when they’re feeling down. They have a flexible range of emotional and behavioral responses that are well matched to the demands of their environment”</p></blockquote>\n<p>(2016).</p>\n<p>The goal of most types of therapy is to improve an individual’s ability to self-regulate and to gain (or regain) a sense of control over one’s behavior and life. Psychologists might be referring to one of two things when they use the term “self-regulation”: behavioral self-regulation or <a href=\"https://positivepsychology.com/emotion-regulation-worksheets-strategies-dbt-skills/\">emotional self-regulation</a>. We’ll explore the difference between the two below.</p>\n<h3>What Is Behavioral Self-Regulation?</h3>\n<p>Behavioral self-regulation is “the ability to act in your long-term best interest, consistent with your deepest values” (Stosny, 2011). It is what allows us to feel one way but act another.</p>\n<p>If you’ve ever dreaded getting up and going to work in the morning but convinced yourself to do it anyway after remembering your goals (e.g., a raise, a promotion) or your basic needs (e.g., food, shelter), you displayed effective behavioral self-regulation.</p>\n<h3>What Is Emotional Self-Regulation?</h3>\n<p>On the other hand, emotional self-regulation involves control of—or, at least, influence over—your emotions.</p>\n<p>If you had ever talked yourself out of a bad mood or calmed yourself down when you were angry, you were displaying effective emotional self-regulation.</p>\n<h2>What is Self-Regulation Theory?</h2>\n<p>Self-regulation theory (SRT) simply outlines the process and components involved when we decide what to think, feel, say, and do. It is particularly salient in the context of making a healthy choice when we have a strong desire to do the opposite (e.g., refraining from eating an entire pizza just because it tastes good).</p>\n<p>According to modern SRT expert Roy Baumeister, there are four components involved (2007):</p>\n<ol>\n<li><em>Standards</em> of desirable behavior;</li>\n<li><em><a href=\"https://positivepsychology.com/self-motivation/\">Motivation</a></em> to meet standards;</li>\n<li><em>Monitoring </em>of situations and thoughts that precede breaking standards;</li>\n<li><em><a href=\"https://positivepsychology.com/psychology-of-willpower/\">Willpower</a> </em>allowing one’s internal strength to control urges.</li>\n</ol>\n<p>These four components interact to determine our self-regulatory activity at any given moment. According to SRT, our behavior is determined by our personal standards of good behavior, our motivation to meet those standards, the degree to which we are consciously aware of our circumstances and our actions, and the extent of our willpower to resist temptations and choose the best path.<br/>\n</p>\n<h2>The Psychology of Self-Regulation</h2>\n<p>According to <a href=\"https://positivepsychology.com/bandura-self-efficacy/\">Albert Bandura</a>, an expert on <a href=\"https://positivepsychology.com/self-efficacy/\">self-efficacy</a> and a leading researcher of SRT, self-regulation is a continuously active process in which we:</p>\n<ol>\n<li>Monitor our own behavior, the influences on our behavior, and the consequences of our behavior;</li>\n<li>Judge our behavior in relation to our own personal standards and broader, more contextual standards;</li>\n<li>React to our own behavior (i.e., what we think and how we feel about our behavior) (1991).</li>\n</ol>\n<p>Bandura also notes that self-efficacy plays a significant role in this process, exerting its influence on our thoughts, feelings, motivations, and actions.</p>\n<p>A quick thought experiment can show the significance of self-efficacy:</p>\n<p>Imagine two people who are highly motivated to lose weight. They are both actively monitoring their food intake and their exercise, and they have specific, measurable goals that they have set for themselves.</p>\n<p>One of them has high self-efficacy and believes he can lose weight if he puts in the effort to do so. The other has low self-efficacy and feels that there’s no way he can hold to his prescribed weight loss plan.</p>\n<p>Who do you think will be better able to say no to second helpings and decadent desserts? Which of them do you think will be more successful in getting up early to exercise each morning?</p>\n<p>We can say with reasonable certainty that the man with higher self-efficacy is likely to be more effective, even if both men start with the exact same standards, motivation, monitoring, and willpower.</p>\n<p>Barry Zimmerman, another big name in SRT research, put forth his own theory founded on self-regulation: self-regulated learning theory.</p>\n<p>We explore this further in <a href=\"https://pro.positivepsychology.com/product/the-science-of-self-acceptance/\">The Science of Self-Acceptance Masterclass©</a>.</p>\n<h3>What is Self-Regulated Learning?</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408091/\">Self-regulated learning</a> (SRL) refers to the process a student engages in when she takes responsibility for her own learning and applies herself to academic success (Zimmerman, 2002).</p>\n<p>This process happens in three steps:</p>\n<ol>\n<li>Planning: The student plans her task, sets goals, outlines strategies to tackle the task, and/or creates a schedule for the task;</li>\n<li>Monitoring: In this stage, the student puts her plans into action and closely monitors her performance and her experience with the methods she chose;</li>\n<li>Reflection: Finally, after the task is complete and the results are in, the student reflects on how well she did and why she performed the way she did (Zimmerman, 2002).</li>\n</ol>\n<p>When students take initiative and regulate their own learning, they gain deeper insights into how they learn, what works best for them, and, ultimately, they perform at a higher level. This improvement springs from the many opportunities to learn during each phase:</p>\n<ol>\n<li>In the planning phase, students have an opportunity to work on their self-assessment and learn how to pick the best strategies for success;</li>\n<li>In the monitoring phase, students get experience implementing the strategies they chose and making real-time adjustments to their plans as needed;</li>\n<li>In the reflection phase, students synthesize everything they learned and reflect on their experience, learning what works for them and what should be altered or replaced with a new strategy.</li>\n</ol>\n<h2>The Self-Regulatory Model</h2>\n<p>It can be useful to consider the self-regulatory model to better understand SRT.</p>\n<p>While the model is specific to health- and illness-related (rather than emotional) self-regulation, it is still a good representation of the complex processes at work during self-regulation of any kind.</p>\n<p>The figure to the right shows how the model works:</p>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Why don’t adults always do exactly what we feel like doing, when we feel like doing it?This is a question that you might hear from kids, and it perfectly encapsulates what baffles them about adults.As adults, we pretty much have free rein to do whatever we want, whenever we want. The vast majority of us won’t get arrested for not showing up to work, and no one will haul us off to prison for eating cake for breakfast.So, why do we show up for work? Why don’t we eat cake for breakfast?Perhaps the ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Why don’t adults always do exactly what we feel like doing, when we feel like doing it?This is a question that you might hear from kids, and it perfectly encapsulates what baffles them about adults.As adults, we pretty much have free rein to do whatever we want, whenever we want. The vast majority of us won’t get arrested for not showing up to work, and no one will haul us off to prison for eating cake for breakfast.So, why do we show up for work? Why don’t we eat cake for breakfast?Perhaps the ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "This Article Contains:What Is Self-Regulation?What Is Self-Regulation Theory?The Psychology of Self-RegulationThe Self-Regulatory ModelWhy Self-Regulation Is Important for WellbeingSelf-Regulation Test and AssessmentEarly Childhood and Child DevelopmentSelf-Regulation in AdultsActivities and Worksheets for Training Self-Regulation (PDFs)Further Resources, Interventions, and ToolsA Take-Home MessageReferences",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "This Article Contains:",
              "id": ""
            },
            {
              "level": "h2",
              "text": "What Is Self-Regulation?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What Is Behavioral Self-Regulation?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What Is Emotional Self-Regulation?",
              "id": ""
            },
            {
              "level": "h2",
              "text": "What is Self-Regulation Theory?",
              "id": ""
            },
            {
              "level": "h2",
              "text": "The Psychology of Self-Regulation",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What is Self-Regulated Learning?",
              "id": ""
            },
            {
              "level": "h2",
              "text": "The Self-Regulatory Model",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.fastercapital.com/content/Feedback-loops--Information-Feedback--Information-Feedback--The-Key-to-Knowledge-Management.html",
      "title": "Feedback loops: Information Feedback: Information Feedback: The Key to Knowledge Management - FasterCapital",
      "author": "",
      "published_date": "2024-06-23T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<div>\n<div>\n</div>\n<hr/>\n</div>\n<h2>1. Introduction to Feedback Loops in Knowledge Management</h2>\n<p>Feedback loops are integral to the field of knowledge management, as they provide a <a href=\"https://www.fastercapital.com/content/Continuous-Improvement--Quality-Circles---Quality-Circles--Collaborative-Problem-Solving-for-Improvement.html\">mechanism for continuous improvement</a> and adaptation. By definition, a <u><a href=\"https://www.fastercapital.com/content/Customer-feedback-management--Feedback-Loop-System--Creating-an-Effective-Feedback-Loop-System-for-Continuous-Improvement.html\">feedback loop is a system</a></u> where outputs of a process are used as inputs for the next cycle, essentially feeding back into the system to inform and influence the ongoing process. In the context of knowledge management, <u><a href=\"https://www.fastercapital.com/content/Referral-marketing--Customer-Feedback-Loops--Utilizing-Customer-Feedback-Loops-to-Refine-Referral-Marketing.html\">feedback loops are used to refine</a></u>, update, and improve the knowledge base within an organization. They ensure that information remains current, relevant, and useful, thereby enhancing <b><a href=\"https://www.fastercapital.com/content/Effective-Decision-Making-Operational-Efficiency---Streamlining-Success--Operational-Efficiency-in-Decision-Making.html\">decision-making and operational efficiency</a></b>.</p>\n<p>From the perspective of an individual employee, feedback loops can manifest as peer reviews or performance evaluations, where the insights gained can <a href=\"https://www.fastercapital.com/content/Time-Tracking--Time-Investment---Time-Investment--How-Time-Tracking-Can-Lead-to-Better-Personal-Development.html\">lead to personal development</a> and better work practices. At an organizational level, feedback loops might involve <b><a href=\"https://www.fastercapital.com/content/Customer-workflow--Customer-Feedback-Mechanisms--Effective-Customer-Feedback-Mechanisms-to-Inform-Your-Workflow.html\">customer feedback mechanisms that inform</a></b> product development or service improvements. In the digital realm, feedback loops are often seen in the <u><a href=\"https://www.fastercapital.com/content/Unveiling-the-Untapped-Potential-of-Form-4506-Data-Analytics.html\">form of data analytics</a></u>, where user behavior informs algorithm adjustments and content curation.</p>\n<p>Here are some in-depth <a href=\"https://www.fastercapital.com/content/Customer-feedback--Consumer-Insights--Unlocking-Consumer-Insights--The-Role-of-Feedback-in-Market-Research.html\">insights into the role of feedback</a> loops in knowledge management:</p>\n<p>1. <strong>Continuous Learning</strong>: Feedback loops <b><a href=\"https://www.fastercapital.com/content/Strategic-Positioning-and-Learning-Organization--How-to-Foster-and-Promote-a-Culture-of-Continuous-Learning-and-Improvement.html\">promote a culture of continuous learning</a></b> and improvement. For example, a <u><a href=\"https://www.fastercapital.com/content/How-can-I-improve-myHigh-Growth-Company-s-customer-service.html\">company might use customer service</a></u> interactions to identify common issues and develop training programs to address these effectively.</p>\n<p>2. <strong>Adaptive Systems</strong>: <a href=\"https://www.fastercapital.com/content/Knowledge-Management-Systems--Orchestrating-Intelligence--Knowledge-Management-Systems-Unveiled.html\">knowledge management systems</a> must be adaptive to remain effective. A feedback loop that incorporates <u><a href=\"https://www.fastercapital.com/content/User-generated-content-campaigns--User-Engagement-Metrics--Measuring-Success--The-Role-of-User-Engagement-Metrics.html\">user engagement metrics</a></u> can help in refining search algorithms, making it easier for employees to find the information they need.</p>\n<p>3. <strong>Innovation Trigger</strong>: Feedback can be a powerful trigger for innovation. When employees are encouraged to provide feedback on processes and practices, this can <a href=\"https://www.fastercapital.com/content/Cost-of-Repair-------Explore-how-handling-repairs-can-lead-to-innovative-solutions-and-business-growth.html\">lead to innovative solutions</a> that <u><a href=\"https://www.fastercapital.com/content/Outsourcing-benefits--How-to-reap-the-benefits-of-outsourcing-team-tasks-and-improve-your-productivity-and-efficiency.html\">improve productivity and efficiency</a></u>.</p>\n<p>4. <strong>Quality Control</strong>: Regular feedback mechanisms serve as a <a href=\"https://www.fastercapital.com/content/Quality-control--How-to-ensure-quality-and-accountability-when-outsourcing-team-tasks.html\">form of quality control</a> for knowledge assets. For instance, a wiki-style company knowledge base that allows for employee edits and comments can maintain the accuracy and relevance of its content.</p>\n<p>5. <strong>Risk Management</strong>: Feedback loops help in identifying risks early on. By analyzing feedback, organizations can spot potential issues before they escalate and take proactive measures.</p>\n<p>6. <strong>Strategic Alignment</strong>: Feedback loops ensure that the <a href=\"https://www.fastercapital.com/content/Content-curation--Knowledge-Management--Integrating-Content-Curation-into-Your-Knowledge-Management-Strategy.html\">knowledge management strategy</a> remains aligned with the organization's goals. They allow for the strategy to be adjusted in response to changes in the business environment or organizational priorities.</p>\n<p>7. <strong>Employee Engagement</strong>: Engaging employees in feedback processes can increase their investment in the organization. An example of this is a suggestion box system that leads to actual changes in the workplace, demonstrating that employee input is valued and acted upon.</p>\n<p>8. <strong>Customer Satisfaction</strong>: External feedback loops, such as customer surveys, can directly influence the <a href=\"https://www.fastercapital.com/content/Co-creation-marketing--How-to-involve-your-customers-in-the-creation-and-improvement-of-your-products-and-services.html\">improvement of products and services</a>, leading to <u><a href=\"https://www.fastercapital.com/content/Customer-Relationship-marketing--Customer-Satisfaction--Achieving-Higher-Customer-Satisfaction-for-Better-Business-Outcomes.html\">higher customer satisfaction</a></u> and loyalty.</p>\n<p><a href=\"https://www.fastercapital.com/content/Referral-marketing--Customer-Feedback-Loops--Customer-Feedback-Loops--Vital-for-Referral-Marketing-Insights.html\">feedback loops are a vital</a> component of <u><a href=\"https://www.fastercapital.com/content/Knowledge-management-and-sharing--Effective-Knowledge-Management-for-Small-Business-Owners.html\">effective knowledge management</a></u>. They provide the means for an organization to remain dynamic, responsive, and ahead of the curve. By embracing feedback at all levels, organizations can foster an environment of growth, innovation, and excellence.</p>\n<div><p></p></div>\n<p>Introduction to Feedback Loops in Knowledge Management - Feedback loops: Information Feedback: Information Feedback: The Key to Knowledge Management</p>\n<h2>2. The Role of Information Feedback in Organizational Learning</h2>\n<p>In the realm of organizational learning, information feedback stands as a pivotal mechanism that propels the continuous cycle of learning and improvement. It is the lifeblood that flows through the veins of knowledge management systems, ensuring that organizations do not just accumulate data but transform it into actionable insights. This transformation is critical in a world where adaptability and responsiveness are key competitive advantages. <a href=\"https://www.fastercapital.com/content/Feedback-loops--Feedback-Ecosystem--Interconnected-Growth--The-Feedback-Ecosystem-in-Urban-Planning.html\">information feedback loops</a> enable organizations to reflect on their actions, outcomes, and the efficacy of their <u><a href=\"https://www.fastercapital.com/content/Success-Strategies--Decision-Making-Processes--Choose-Wisely--Decision-Making-Processes-that-Lead-to-Success.html\">decision-making processes</a></u>. By scrutinizing the feedback obtained from various internal and external sources, organizations can identify patterns, anticipate trends, and rectify errors before they escalate into more significant issues.</p>\n<p>From the perspective of the individual employee to the overarching view of the organization as a whole, information feedback serves multiple roles:</p>\n<p>1. <strong>Enhancing Decision-Making</strong>: Employees at all levels benefit from feedback as it informs their decisions. For example, a sales team <a href=\"https://www.fastercapital.com/content/Customer-feedback-management--Customer-Feedback-Trends--Staying-Ahead-of-the-Curve--Analyzing-Customer-Feedback-Trends.html\">analyzing customer feedback</a> can adjust their strategies to better <u><a href=\"https://www.fastercapital.com/content/Adapting-Your-Product-to-Meet-Market-Demands.html\">meet market demands</a></u>.</p>\n<p>2. <strong>Fostering Innovation</strong>: Feedback can spark innovation by highlighting areas ripe for improvement. Consider how user feedback led to the development of the now-ubiquitous smartphone touchscreens.</p>\n<p>3. <strong>Improving Processes</strong>: Continuous process improvement is a direct result of feedback. Toyota's production system, for instance, integrates worker feedback to enhance efficiency and quality.</p>\n<p>4. <strong>Cultivating a <b><a href=\"https://www.fastercapital.com/content/Growth-Mindset--Learning-Culture---Learning-Culture--Fostering-a-Growth-Mindset-in-Organizations.html\">learning culture</a></b></strong>: A learning culture is nurtured when feedback is actively sought and valued. Google's 'Project Aristotle' revealed that psychological safety, partly fostered by open feedback, was crucial for team performance.</p>\n<p>5. <strong>Aligning Goals</strong>: Feedback helps align individual ",
        "html": "<div><div>\n<div>\n<div>\n</div>\n<hr/>\n</div>\n<h2>1. Introduction to Feedback Loops in Knowledge Management</h2>\n<p>Feedback loops are integral to the field of knowledge management, as they provide a <a href=\"https://www.fastercapital.com/content/Continuous-Improvement--Quality-Circles---Quality-Circles--Collaborative-Problem-Solving-for-Improvement.html\">mechanism for continuous improvement</a> and adaptation. By definition, a <u><a href=\"https://www.fastercapital.com/content/Customer-feedback-management--Feedback-Loop-System--Creating-an-Effective-Feedback-Loop-System-for-Continuous-Improvement.html\">feedback loop is a system</a></u> where outputs of a process are used as inputs for the next cycle, essentially feeding back into the system to inform and influence the ongoing process. In the context of knowledge management, <u><a href=\"https://www.fastercapital.com/content/Referral-marketing--Customer-Feedback-Loops--Utilizing-Customer-Feedback-Loops-to-Refine-Referral-Marketing.html\">feedback loops are used to refine</a></u>, update, and improve the knowledge base within an organization. They ensure that information remains current, relevant, and useful, thereby enhancing <b><a href=\"https://www.fastercapital.com/content/Effective-Decision-Making-Operational-Efficiency---Streamlining-Success--Operational-Efficiency-in-Decision-Making.html\">decision-making and operational efficiency</a></b>.</p>\n<p>From the perspective of an individual employee, feedback loops can manifest as peer reviews or performance evaluations, where the insights gained can <a href=\"https://www.fastercapital.com/content/Time-Tracking--Time-Investment---Time-Investment--How-Time-Tracking-Can-Lead-to-Better-Personal-Development.html\">lead to personal development</a> and better work practices. At an organizational level, feedback loops might involve <b><a href=\"https://www.fastercapital.com/content/Customer-workflow--Customer-Feedback-Mechanisms--Effective-Customer-Feedback-Mechanisms-to-Inform-Your-Workflow.html\">customer feedback mechanisms that inform</a></b> product development or service improvements. In the digital realm, feedback loops are often seen in the <u><a href=\"https://www.fastercapital.com/content/Unveiling-the-Untapped-Potential-of-Form-4506-Data-Analytics.html\">form of data analytics</a></u>, where user behavior informs algorithm adjustments and content curation.</p>\n<p>Here are some in-depth <a href=\"https://www.fastercapital.com/content/Customer-feedback--Consumer-Insights--Unlocking-Consumer-Insights--The-Role-of-Feedback-in-Market-Research.html\">insights into the role of feedback</a> loops in knowledge management:</p>\n<p>1. <strong>Continuous Learning</strong>: Feedback loops <b><a href=\"https://www.fastercapital.com/content/Strategic-Positioning-and-Learning-Organization--How-to-Foster-and-Promote-a-Culture-of-Continuous-Learning-and-Improvement.html\">promote a culture of continuous learning</a></b> and improvement. For example, a <u><a href=\"https://www.fastercapital.com/content/How-can-I-improve-myHigh-Growth-Company-s-customer-service.html\">company might use customer service</a></u> interactions to identify common issues and develop training programs to address these effectively.</p>\n<p>2. <strong>Adaptive Systems</strong>: <a href=\"https://www.fastercapital.com/content/Knowledge-Management-Systems--Orchestrating-Intelligence--Knowledge-Management-Systems-Unveiled.html\">knowledge management systems</a> must be adaptive to remain effective. A feedback loop that incorporates <u><a href=\"https://www.fastercapital.com/content/User-generated-content-campaigns--User-Engagement-Metrics--Measuring-Success--The-Role-of-User-Engagement-Metrics.html\">user engagement metrics</a></u> can help in refining search algorithms, making it easier for employees to find the information they need.</p>\n<p>3. <strong>Innovation Trigger</strong>: Feedback can be a powerful trigger for innovation. When employees are encouraged to provide feedback on processes and practices, this can <a href=\"https://www.fastercapital.com/content/Cost-of-Repair-------Explore-how-handling-repairs-can-lead-to-innovative-solutions-and-business-growth.html\">lead to innovative solutions</a> that <u><a href=\"https://www.fastercapital.com/content/Outsourcing-benefits--How-to-reap-the-benefits-of-outsourcing-team-tasks-and-improve-your-productivity-and-efficiency.html\">improve productivity and efficiency</a></u>.</p>\n<p>4. <strong>Quality Control</strong>: Regular feedback mechanisms serve as a <a href=\"https://www.fastercapital.com/content/Quality-control--How-to-ensure-quality-and-accountability-when-outsourcing-team-tasks.html\">form of quality control</a> for knowledge assets. For instance, a wiki-style company knowledge base that allows for employee edits and comments can maintain the accuracy and relevance of its content.</p>\n<p>5. <strong>Risk Management</strong>: Feedback loops help in identifying risks early on. By analyzing feedback, organizations can spot potential issues before they escalate and take proactive measures.</p>\n<p>6. <strong>Strategic Alignment</strong>: Feedback loops ensure that the <a href=\"https://www.fastercapital.com/content/Content-curation--Knowledge-Management--Integrating-Content-Curation-into-Your-Knowledge-Management-Strategy.html\">knowledge management strategy</a> remains aligned with the organization's goals. They allow for the strategy to be adjusted in response to changes in the business environment or organizational priorities.</p>\n<p>7. <strong>Employee Engagement</strong>: Engaging employees in feedback processes can increase their investment in the organization. An example of this is a suggestion box system that leads to actual changes in the workplace, demonstrating that employee input is valued and acted upon.</p>\n<p>8. <strong>Customer Satisfaction</strong>: External feedback loops, such as customer surveys, can directly influence the <a href=\"https://www.fastercapital.com/content/Co-creation-marketing--How-to-involve-your-customers-in-the-creation-and-improvement-of-your-products-and-services.html\">improvement of products and services</a>, leading to <u><a href=\"https://www.fastercapital.com/content/Customer-Relationship-marketing--Customer-Satisfaction--Achieving-Higher-Customer-Satisfaction-for-Better-Business-Outcomes.html\">higher customer satisfaction</a></u> and loyalty.</p>\n<p><a href=\"https://www.fastercapital.com/content/Referral-marketing--Customer-Feedback-Loops--Customer-Feedback-Loops--Vital-for-Referral-Marketing-Insights.html\">feedback loops are a vital</a> component of <u><a href=\"https://www.fastercapital.com/content/Knowledge-management-and-sharing--Effective-Knowledge-Management-for-Small-Business-Owners.html\">effective knowledge management</a></u>. They provide the means for an organization to remain dynamic, responsive, and ahead of the curve. By embracing feedback at all levels, organizations can foster an environment of growth, innovation, and excellence.</p>\n<div><p></p></div>\n<p>Introduction to Feedback Loops in Knowledge Management - Feedback loops: Information Feedback: Information Feedback: The Key to Knowledge Management</p>\n<h2>2. The Role of Information Feedback in Organizational Learning</h2>\n<p>In the realm of organizational learning, information feedback stands as a pivotal mechanism that propels the continuous cycle of learning and improvement. It is the lifeblood that flows through the veins of knowledge management systems, ensuring that organizations do not just accumulate data but transform it into actionable insights. This transformation is critical in a world where adaptability and responsiveness are key competitive advantages. <a href=\"https://www.fastercapital.com/content/Feedback-loops--Feedback-Ecosystem--Interconnected-Growth--The-Feedback-Ecosystem-in-Urban-Planning.html\">information feedback loops</a> enable organizations to reflect on their actions, outcomes, and the efficacy of their <u><a href=\"https://www.fastercapital.com/content/Success-Strategies--Decision-Making-Processes--Choose-Wisely--Decision-Making-Processes-that-Lead-to-Success.html\">decision-making processes</a></u>. By scrutinizing the feedback obtained from various internal and external sources, organizations can identify patterns, anticipate trends, and rectify errors before they escalate into more significant issues.</p>\n<p>From the perspective of the individual employee to the overarching view of the organization as a whole, information feedback serves multiple roles:</p>\n<p>1. <strong>Enhancing Decision-Making</strong>: Employees at all levels benefit from feedback as it informs their decisions. For example, a sales team <a href=\"https://www.fastercapital.com/content/Customer-feedback-management--Customer-Feedback-Trends--Staying-Ahead-of-the-Curve--Analyzing-Customer-Feedback-Trends.html\">analyzing customer feedback</a> can adjust their strategies to better <u><a href=\"https://www.fastercapital.com/content/Adapting-Your-Product-to-Meet-Market-Demands.html\">meet market demands</a></u>.</p>\n<p>2. <strong>Fostering Innovation</strong>: Feedback can spark innovation by highlighting areas ripe for improvement. Consider how user feedback led to the development of the now-ubiquitous smartphone touchscreens.</p>\n<p>3. <strong>Improving Processes</strong>: Continuous process improvement is a direct result of feedback. Toyota's production system, for instance, integrates worker feedback to enhance efficiency and quality.</p>\n<p>4. <strong>Cultivating a <b><a href=\"https://www.fastercapital.com/content/Growth-Mindset--Learning-Culture---Learning-Culture--Fostering-a-Growth-Mindset-in-Organizations.html\">learning culture</a></b></strong>: A learning culture is nurtured when feedback is actively sought and valued. Google's 'Project Aristotle' revealed that psychological safety, partly fostered by open feedback, was crucial for team performance.</p>\n<p>5. <strong>Aligning Goals</strong>: Feedback helps align individual ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "1. Introduction to Feedback Loops in Knowledge ManagementFeedback loops are integral to the field of knowledge management, as they provide amechanism for continuous improvementand adaptation. By definition, afeedback loop is a systemwhere outputs of a process are used as inputs for the next cycle, essentially feeding back into the system to inform and influence the ongoing process. In the context of knowledge management,feedback loops are used to refine, update, and improve the knowledge base wi",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "1. Introduction to Feedback Loops in Knowledge ManagementFeedback loops are integral to the field of knowledge management, as they provide amechanism for continuous improvementand adaptation. By definition, afeedback loop is a systemwhere outputs of a process are used as inputs for the next cycle, essentially feeding back into the system to inform and influence the ongoing process. In the context of knowledge management,feedback loops are used to refine, update, and improve the knowledge base wi",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "1. Introduction to Feedback Loops in Knowledge Management",
              "id": ""
            },
            {
              "level": "h2",
              "text": "2. The Role of Information Feedback in Organizational Learning",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "knowledge_management"
    },
    {
      "url": "https://www.digitalnewsreport.org/publications/2018/factsheet-understanding-promise-limits-automated-fact-checking/",
      "title": "FACTSHEET: Understanding the Promise and Limits of Automated Fact-Checking",
      "author": "Lisa Smars",
      "published_date": "2018-02-28T00:01:28.000Z",
      "content": {
        "text": "<div><div>\n<article>\n<div>\n<div>\n<p>The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers.</p>\n<p>The influence of ‘fake news’ in different parts of the world remains poorly understood. Initial evidence from the US and Europe suggests that the share of online users who visit false news sites directly is quite limited, and that people exposed to these sites visit mainstream news sources far more (Allcott and Gentzkow 2017; Guess et al. 2018; Fletcher et al. 2018). However, the same studies indicate fabricated news stories may draw disproportionate attention on social networks, outperforming conventional news, and some partisans (e.g. Trump voters in the US) appear to be regular users of false news sites. Little is known about the dynamics by which individual viral stories may influence the opinions and behaviour of specific, targeted audiences around particular events or issues.</p>\n<p>In the US and Europe, concern about commercially or politically motivated misinformation online – in particular about mounting evidence of sophisticated, state-backed campaigns operating from Russia – has fuelled a vigorous debate over policy options. These include a raft of proposals to regulate platform companies like Facebook and Google in new ways, a question under review in the European Commission. Several countries, notably Germany, France, and Ireland, have passed or are considering legislation that penalises the distribution of false information.</p>\n<p>These concerns have also drawn new attention to the potential of various automated fact-checking (AFC) technologies to combat false information online. However, deciding the truth of public claims and separating legitimate views from misinformation is difficult and often controversial work (see Graves 2016), challenges that carry over into AFC. Based on a review of current efforts and interviews with both fact-checkers and computer scientists working in this area, this survey of the AFC landscape finds that:</p>\n<ul>\n<li>Much of the terrain covered by human fact-checkers requires a kind of judgement and sensitivity to context that remains far out of reach for fully automated verification.</li>\n<li>Rapid progress is being made in automatic verification of a narrow range of simple factual claims for which authoritative data are available. Even here, though, AFC systems will require human supervision for the foreseeable future.</li>\n<li>Both researchers and practitioners agree that the real promise of AFC technologies for now lies in tools to assist fact-checkers to identify and investigate claims, and to deliver their conclusions as effectively as possible.</li>\n<li>So far independent, nonprofit fact-checking organizations have led the way in developing and implementing AFC, with little activity from traditional media outlets.</li>\n<li>Some individual AFC tools have been built inexpensively by fact-checking groups. However, advancing capabilities and developing large-scale systems requires continuing support from foundations, universities, and platform companies.</li>\n</ul>\n<p><strong>Overview</strong></p>\n<p>AFC initiatives and research generally focus on one or more of three overlapping objectives: to spot false or questionable claims circulating online and in other media; to authoritatively verify claims or stories that are in doubt, or to facilitate their verification by journalists and members of the public; and to deliver corrections instantaneously, across different media, to audiences exposed to misinformation. End-to-end systems aim to address all three elements – identification, verification, and correction (see chart).</p>\n<p>The first proposals to automate online fact-checking appeared nearly a decade ago. Over the last several years a growing research literature has embraced AFC as an interesting problem in artificial intelligence, intersecting with practical experiments by fact-checkers.<span>1</span> Two recent programming competitions, the ‘Fast &amp; Furious Fact Check Challenge’ and the ‘Fake News Challenge’, allowed research teams from around the world to test different AFC techniques on common problem sets.<span>2</span> Dr Andreas Vlachos, a lecturer at University of Sheffield, remarks on the increased attention:</p>\n<p><em>We published our first paper in 2014. To us, apart from our interest in politics, we thought it was a great challenge for artificial intelligence to actually work on this problem. [But] for better or worse, Trump’s election increased the interest.</em></p>\n<p>Meanwhile, real-world AFC initiatives have enjoyed a wave of additional funding in the last two years. Full Fact, a London-based fact-checking charity, began developing AFC tools in 2016 with a €50,000 grant from Google and recently announced £500,000 additional funding from the Omidyar Foundation and the Open Society Foundations. The Duke Reporters Lab, based at Duke University, received $1.2m in late 2017 to launch the Tech &amp; Check Cooperative, a hub for AFC projects, from the Knight Foundation, the Facebook Journalism Project, and the Craig Newmark Foundation. In January, Factmata, a London-based startup developing an AFC platform, announced $1m in seed funding.</p>\n<p><a href=\"https://www.digitalnewsreport.org/wp-content/uploads/2018/02/18406.png\"></a></p>\n<p><strong>Approaches to AFC</strong></p>\n<p>Real-world AFC efforts begin with systems to monitor various forms of public discourse – speeches, debates, commentary, news reports, and so on – online and in traditional media. This is a difficult problem that may involve scraping transcripts and other material from media or political pages, monitoring live subtitle feeds, or using automatic transcription.<span>3</span></p>\n<p>Once monitoring is in place, the central research and design challenge revolves around the closely linked problems of identifying and verifying factual claims, explored below. A tension exists in that success in the first complicates the second, widening the range of claims that must be verified. In practice, AFC implementations constrain the problem by drawing on the work of human fact-checkers and/or by sharply limiting the kinds of claims being checked.</p>\n<p><strong>Identifying Claims</strong></p>\n<p>The greatest success in AFC research has come in the area of extracting discrete factual claims from a text such as a speech or an article. The most common approach relies on a combination of natural language processing and machine learning to identify and prioritise claims to be checked. For instance, ClaimBuster, an AFC platform developed at the University of Texas-Arlington, at a cost of roughly $150,000 so far, trained on about 20,000 sentences from past US presidential debates, classified by paid human coders, to learn to distinguish ‘check-worthy’ factual claims from opinions and boring statements (Hassan et al. 2017). In a test during a US primary debate in 2016, more than 70% of actual claims checked by fact-checkers at PolitiFact and CNN were among the top fifth of statements identified by ClaimBuster.<span>4</span></p>\n<p>A number of fact-checking outlets around the world have begun relying on software to help spot claims to check. In the US, for instance, the Duke Reporters Lab recently deployed a tool that uses ClaimBuster to deliver potentially interesting claims to fact-checkers at PolitiFact, FactCheck.org, the <span>Washington Post</span>, and the Associated Press (see the box). However, so far these systems can only identify simple declarative statements, missing implied claims or claims embedded in complex sentences which humans recognise easily. This is a particular challenge with conversational sources, like discussion programmes, in which people often use pronouns and refer back to earlier points.</p>\n<p>It is also important to note that the ‘ground truth’ established by training algorithms on human work is neither universal not permanent. For instance, ClaimBuster has been optimised to detect debate claims and does somewhat less well harvesting statements on Twitter. More broadly, the meaning and the importance of a particular statement may shift depending on historical or political context. Will Moy, director of Full Fact, gives the example of claims about the EU – polls show UK residents cared very little about the issue until the Brexit campaign brought it into the headlines.</p>\n<p><a href=\"https://www.digitalnewsreport.org/wp-content/uploads/2018/02/Factchecking.png\"></a></p>\n</div>\n<p>Mevan Babakar, the groups’ digital product manager, highlights the difference between knowing a factual statement has been made and understanding what is being claimed, a vital step in determining the importance of a question:</p>\n<p><em>Identifying a factual statement is not easy but it is consistently possible. If you show me a sentence I can probably tell you if it’s a claim. Understanding the meaning of a claim is hard – you need to understand the geography, what years it’s referring to, and so on. Understanding how important a claim is is even harder, because it changes depending on who’s doing the asking, and it changes depending on the political context, and that’s something that’s shifting all the time.</em></p>\n<p><strong>Verifying Claims</strong></p>\n<p>The conclusions reached by professional fact-checking organ",
        "html": "<div><div>\n<article>\n<div>\n<div>\n<p>The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers.</p>\n<p>The influence of ‘fake news’ in different parts of the world remains poorly understood. Initial evidence from the US and Europe suggests that the share of online users who visit false news sites directly is quite limited, and that people exposed to these sites visit mainstream news sources far more (Allcott and Gentzkow 2017; Guess et al. 2018; Fletcher et al. 2018). However, the same studies indicate fabricated news stories may draw disproportionate attention on social networks, outperforming conventional news, and some partisans (e.g. Trump voters in the US) appear to be regular users of false news sites. Little is known about the dynamics by which individual viral stories may influence the opinions and behaviour of specific, targeted audiences around particular events or issues.</p>\n<p>In the US and Europe, concern about commercially or politically motivated misinformation online – in particular about mounting evidence of sophisticated, state-backed campaigns operating from Russia – has fuelled a vigorous debate over policy options. These include a raft of proposals to regulate platform companies like Facebook and Google in new ways, a question under review in the European Commission. Several countries, notably Germany, France, and Ireland, have passed or are considering legislation that penalises the distribution of false information.</p>\n<p>These concerns have also drawn new attention to the potential of various automated fact-checking (AFC) technologies to combat false information online. However, deciding the truth of public claims and separating legitimate views from misinformation is difficult and often controversial work (see Graves 2016), challenges that carry over into AFC. Based on a review of current efforts and interviews with both fact-checkers and computer scientists working in this area, this survey of the AFC landscape finds that:</p>\n<ul>\n<li>Much of the terrain covered by human fact-checkers requires a kind of judgement and sensitivity to context that remains far out of reach for fully automated verification.</li>\n<li>Rapid progress is being made in automatic verification of a narrow range of simple factual claims for which authoritative data are available. Even here, though, AFC systems will require human supervision for the foreseeable future.</li>\n<li>Both researchers and practitioners agree that the real promise of AFC technologies for now lies in tools to assist fact-checkers to identify and investigate claims, and to deliver their conclusions as effectively as possible.</li>\n<li>So far independent, nonprofit fact-checking organizations have led the way in developing and implementing AFC, with little activity from traditional media outlets.</li>\n<li>Some individual AFC tools have been built inexpensively by fact-checking groups. However, advancing capabilities and developing large-scale systems requires continuing support from foundations, universities, and platform companies.</li>\n</ul>\n<p><strong>Overview</strong></p>\n<p>AFC initiatives and research generally focus on one or more of three overlapping objectives: to spot false or questionable claims circulating online and in other media; to authoritatively verify claims or stories that are in doubt, or to facilitate their verification by journalists and members of the public; and to deliver corrections instantaneously, across different media, to audiences exposed to misinformation. End-to-end systems aim to address all three elements – identification, verification, and correction (see chart).</p>\n<p>The first proposals to automate online fact-checking appeared nearly a decade ago. Over the last several years a growing research literature has embraced AFC as an interesting problem in artificial intelligence, intersecting with practical experiments by fact-checkers.<span>1</span> Two recent programming competitions, the ‘Fast &amp; Furious Fact Check Challenge’ and the ‘Fake News Challenge’, allowed research teams from around the world to test different AFC techniques on common problem sets.<span>2</span> Dr Andreas Vlachos, a lecturer at University of Sheffield, remarks on the increased attention:</p>\n<p><em>We published our first paper in 2014. To us, apart from our interest in politics, we thought it was a great challenge for artificial intelligence to actually work on this problem. [But] for better or worse, Trump’s election increased the interest.</em></p>\n<p>Meanwhile, real-world AFC initiatives have enjoyed a wave of additional funding in the last two years. Full Fact, a London-based fact-checking charity, began developing AFC tools in 2016 with a €50,000 grant from Google and recently announced £500,000 additional funding from the Omidyar Foundation and the Open Society Foundations. The Duke Reporters Lab, based at Duke University, received $1.2m in late 2017 to launch the Tech &amp; Check Cooperative, a hub for AFC projects, from the Knight Foundation, the Facebook Journalism Project, and the Craig Newmark Foundation. In January, Factmata, a London-based startup developing an AFC platform, announced $1m in seed funding.</p>\n<p><a href=\"https://www.digitalnewsreport.org/wp-content/uploads/2018/02/18406.png\"></a></p>\n<p><strong>Approaches to AFC</strong></p>\n<p>Real-world AFC efforts begin with systems to monitor various forms of public discourse – speeches, debates, commentary, news reports, and so on – online and in traditional media. This is a difficult problem that may involve scraping transcripts and other material from media or political pages, monitoring live subtitle feeds, or using automatic transcription.<span>3</span></p>\n<p>Once monitoring is in place, the central research and design challenge revolves around the closely linked problems of identifying and verifying factual claims, explored below. A tension exists in that success in the first complicates the second, widening the range of claims that must be verified. In practice, AFC implementations constrain the problem by drawing on the work of human fact-checkers and/or by sharply limiting the kinds of claims being checked.</p>\n<p><strong>Identifying Claims</strong></p>\n<p>The greatest success in AFC research has come in the area of extracting discrete factual claims from a text such as a speech or an article. The most common approach relies on a combination of natural language processing and machine learning to identify and prioritise claims to be checked. For instance, ClaimBuster, an AFC platform developed at the University of Texas-Arlington, at a cost of roughly $150,000 so far, trained on about 20,000 sentences from past US presidential debates, classified by paid human coders, to learn to distinguish ‘check-worthy’ factual claims from opinions and boring statements (Hassan et al. 2017). In a test during a US primary debate in 2016, more than 70% of actual claims checked by fact-checkers at PolitiFact and CNN were among the top fifth of statements identified by ClaimBuster.<span>4</span></p>\n<p>A number of fact-checking outlets around the world have begun relying on software to help spot claims to check. In the US, for instance, the Duke Reporters Lab recently deployed a tool that uses ClaimBuster to deliver potentially interesting claims to fact-checkers at PolitiFact, FactCheck.org, the <span>Washington Post</span>, and the Associated Press (see the box). However, so far these systems can only identify simple declarative statements, missing implied claims or claims embedded in complex sentences which humans recognise easily. This is a particular challenge with conversational sources, like discussion programmes, in which people often use pronouns and refer back to earlier points.</p>\n<p>It is also important to note that the ‘ground truth’ established by training algorithms on human work is neither universal not permanent. For instance, ClaimBuster has been optimised to detect debate claims and does somewhat less well harvesting statements on Twitter. More broadly, the meaning and the importance of a particular statement may shift depending on historical or political context. Will Moy, director of Full Fact, gives the example of claims about the EU – polls show UK residents cared very little about the issue until the Brexit campaign brought it into the headlines.</p>\n<p><a href=\"https://www.digitalnewsreport.org/wp-content/uploads/2018/02/Factchecking.png\"></a></p>\n</div>\n<p>Mevan Babakar, the groups’ digital product manager, highlights the difference between knowing a factual statement has been made and understanding what is being claimed, a vital step in determining the importance of a question:</p>\n<p><em>Identifying a factual statement is not easy but it is consistently possible. If you show me a sentence I can probably tell you if it’s a claim. Understanding the meaning of a claim is hard – you need to understand the geography, what years it’s referring to, and so on. Understanding how important a claim is is even harder, because it changes depending on who’s doing the asking, and it changes depending on the political context, and that’s something that’s shifting all the time.</em></p>\n<p><strong>Verifying Claims</strong></p>\n<p>The conclusions reached by professional fact-checking organ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police ",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police ",
              "class": [],
              "id": ""
            }
          ],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://theeducationhub.org.nz/self-regulation/",
      "title": "The importance of self-regulation for learning",
      "author": "Amanda Howley-Rouse",
      "published_date": "2020-01-12T22:40:28.000Z",
      "content": {
        "text": "<div><div>\n<p><strong>Self-regulation is the process by which students monitor and control their cognition, motivation, and behaviour in order to achieve certain goals. There are several interweaving theories of self-regulation, but most common models conceptualise self-regulation in terms of a series of steps involving forethought or planning, performance, and reflection</strong><a href=\"#_edn1\"><strong>[i]</strong></a><strong> </strong><a href=\"#_edn2\"><strong>[ii]</strong></a><strong>. These steps can be explicitly taught and, while self-regulation increases to some extent with age, the research is clear that self-regulation can be improved and that the role of the teacher is crucial in supporting and promoting self-regulated learning. What is more, students’ emotions and their beliefs about their own ability play a key role in the development and exercise of self-regulation, and teachers can further support self-regulation by teaching students about </strong><a href=\"https://theeducationhub.org.nz/category/school-resources/growth-mindset/\"><strong>growth mindset</strong></a><strong> and the role of </strong><a href=\"https://theeducationhub.org.nz/category/school-resources/learning-and-our-emotions/\"><strong>the emotions in learning</strong></a><strong>.</strong></p>\n<p>The\nfirst step in self-regulated learning is to plan and <strong>set goals</strong>. Goals are guideposts that students use to check their\nown progress. Setting goals involves activating prior knowledge about the\ndifficulty of the task and about one’s own ability in that content area.\nStudents may weigh in their mind how long an activity may take and set a time\nmanagement plan in place. They may also think about particular learning\nstrategies (such as asking themselves questions as they read) that they will\nuse in reaching their goal/s. </p>\n<p>Students\nself-regulate by focusing their energy and attention on the task at hand. This next\nstep involves <strong>exercising</strong> <strong>control. </strong>Control can be exercised by\nimplementing any of the learning strategies (such as rehearsal, elaboration, summarising\nor asking themselves questions) chosen in the first step. Help-seeking can also\nbe a form of control, but only when the learner uses it to develop their own\nskill or understanding: help-seeking is not considered self-regulatory behaviour\nwhen it is used as a crutch to arrive at the answer without the hard work.\nControl can also take the form of using attention-focusing strategies such as\nturning off all music, sitting alone, or going to the library, and it involves\npostponing enjoyable activities in order to make progress towards one’s goals.\nSimply put, control is general persistence to stick with the strategies that\nwork.</p>\n<p>Next,\nself-regulated learners <strong>monitor progress</strong>\ntowards their goal. Individuals can monitor their own understanding,\nmotivation, feelings, or behaviour towards a goal. For example, by using the\nmetacognitive strategy they decided to use in the goal-setting stage (asking\nthemselves questions), students can clarify for themselves what they do and do\nnot yet know. Other ways of self-monitoring include keeping track of how much\nstudying truly gets done with a study group, or noticing which contexts and\nenvironments allow them to focus on their work.</p>\n<p>Finally,\nstudents use the information gathered through the previous self-evaluation to\nmetacognitively <strong>reflect and respond</strong>.\nA student’s confidence in their own abilities will shape how they reflect on\ntheir progress or lack thereof. For example, a student with a stable, high\nbelief that they are capable will attribute a low grade on a math test to their\nlack of sleep the night before or their minimal study time as opposed to a lack\nof intelligence. Responding to a self-evaluation functions like a thermostat,\neither turning up the dial on effort to increase progress towards one’s goals\nor easing back to focus on other tasks. This adjustment can manifest as\nhelp-seeking behaviour, persistence, or shifting learning strategies.</p>\n<h4><strong>Why\nis self-regulation important?</strong></h4>\n<p>It\nis increasingly important that students are able to proactively evaluate and\nimprove upon their own learning. In a rapidly changing world, successful\nindividuals must be life-long learners who are metacognitive about and able to\neffectively evaluate their learning. Within the education system, students\nwithout the ability to focus their attention and maintain perseverance will be\nconstantly pulled left and right by their immediate impulses. Furthermore,\nstudents who fail to learn self-evaluation strategies will not be able to\neffectively direct their attention towards the areas that need it the most.\nWhile some students may find poor study conditions, confusing lessons or difficult\ntexts to be insurmountable obstacles, self-regulation allows learners to navigate\nthese conditions by discovering solutions that work.</p>\n<p>In\naddition to developing personal responsibility about learning, self-regulation also\nsolidifies the content of learning. Self-regulation practices improve the\nencoding of knowledge and skills in memory, especially in reading comprehension\nand writing.<a href=\"#_edn3\">[iii]</a>\nResearch has also identified that self-regulation strategies are associated\nwith increased student effort and motivation, improved scores on standardised\ntests and general preparedness for class.</p>\n<h4><strong>How\ndo we cultivate self-regulation?</strong></h4>\n<p>As\ndiscussed above, the self-regulation process is composed of a series of steps.\nThese steps are not rigid in their order. In actuality, self-regulated learners\nengage in many of these processes simultaneously or shift the steps as they\nbecome adept self-regulators. To teach and develop student self-regulation as a\nwhole, teachers can support each of the underlying stages. It is also important\nto support students’ self-efficacy, encourage them to adopt a growth mindset and\nprioritise learning over grades and marks.</p>\n<p><strong>Match the form of learning with appropriate strategies</strong></p>\n<p>In\nthis first stage, students identify particular learning strategies that fit\nwith their goals. Basic learning tasks such as encoding information for memory\nrecall are best learned through rehearsal, organisation or categorisation,\nmnemonic devices, or paraphrasing the information. However, more elaborate\nstrategies are used when students are asked to make information meaningful. In\nbuilding connections between new concepts and a learner’s existing knowledge,\nstudents may choose to list underlying causes or themes, outline the structure\nof the process or paper, or diagram spatial relationships to create a network of\nideas. This is not a comprehensive catalogue of learning strategies but serves\nto illustrate the value in carefully choosing a learning strategy to align with\ngoals. It is important for teachers to explicitly teach a range of learning strategies,\nand to enable and support students to determine which form of learning strategy\nis most appropriate for the type of work. </p>\n<p><strong>Always include positive feedback</strong></p>\n<p>Maintaining\nattention throughout a task takes practice. However, teachers can support students’\nfocus through positive feedback. Students often adopt their teacher’s\nevaluations of their work as their own, which means that teachers can highly\ninfluence a student’s persistence in engaging with a task or giving up. In\naddition, developing a culture around celebrating mistakes as opportunities to\nlearn is crucial. Authentically discussing areas of improvement allows room for\ngrowth, and an inclusion of positive feedback should not be interpreted as giving\nexclusively positive feedback.\nTeachers can also use their expertise to differentiate their level of positive\nand negative feedback according to student self-efficacy in a particular task.</p>\n<p><strong>Maintain an environment conducive to focus</strong></p>\n<p>Teachers\ncan ensure that the study environment is conducive to focus, as a relatively\nquiet space for individual work is invaluable. Beyond this, students learn how\nto regulate their own attention and impulses best through sustained and regular\npractice, increasing in duration each session. While collaboration and\ndiscussion are an important part of learning, self-regulation becomes much more\nchallenging in a noisy environment. In secondary education this is particularly\nimportant, as the higher critical thinking skills required by adolescents are\nseverely inhibited by distractions. Teachers can further support the\ndevelopment of self-regulation by providing complex, open-ended tasks that give\nstudents the opportunity to practise managing distractions and maintaining\nfocus while tackling increasingly challenging academic work.</p>\n<p><strong>Guide students to track their progress</strong></p>\n<p>At\nthe heart of monitoring understanding lies the question: ‘what do I know, and\nhow can I improve?’ Students can push themselves to become aware of the limits\nof their own knowledge through recall, practice and extension, depending on the\nnature of the goal. One monitoring strategy might be summarising the main\npoints of a lesson following direct instruction. A student trying to increase\nher reading comprehension may pause to ask herself questions about the text (at\nvarying levels of complexity). </p>\n<p>Some\nstudents may wish to improve their time management skills. These students would\nbenefit from keeping a record of how they spend their time and then comparing\nit with their task goals. For example, I may believe that two hours of studying\nwith a study group each week is a strong plan in preparing for a test at the\nend of the term. However, I may in fact find that one of the two hours is\ngenerally spent socialising. This new information can then be used to shift my behaviour\nmoving forward. </p>\n<p><strong>Practise evaluating ‘like a detective’<",
        "html": "<div><div>\n<p><strong>Self-regulation is the process by which students monitor and control their cognition, motivation, and behaviour in order to achieve certain goals. There are several interweaving theories of self-regulation, but most common models conceptualise self-regulation in terms of a series of steps involving forethought or planning, performance, and reflection</strong><a href=\"#_edn1\"><strong>[i]</strong></a><strong> </strong><a href=\"#_edn2\"><strong>[ii]</strong></a><strong>. These steps can be explicitly taught and, while self-regulation increases to some extent with age, the research is clear that self-regulation can be improved and that the role of the teacher is crucial in supporting and promoting self-regulated learning. What is more, students’ emotions and their beliefs about their own ability play a key role in the development and exercise of self-regulation, and teachers can further support self-regulation by teaching students about </strong><a href=\"https://theeducationhub.org.nz/category/school-resources/growth-mindset/\"><strong>growth mindset</strong></a><strong> and the role of </strong><a href=\"https://theeducationhub.org.nz/category/school-resources/learning-and-our-emotions/\"><strong>the emotions in learning</strong></a><strong>.</strong></p>\n<p>The\nfirst step in self-regulated learning is to plan and <strong>set goals</strong>. Goals are guideposts that students use to check their\nown progress. Setting goals involves activating prior knowledge about the\ndifficulty of the task and about one’s own ability in that content area.\nStudents may weigh in their mind how long an activity may take and set a time\nmanagement plan in place. They may also think about particular learning\nstrategies (such as asking themselves questions as they read) that they will\nuse in reaching their goal/s. </p>\n<p>Students\nself-regulate by focusing their energy and attention on the task at hand. This next\nstep involves <strong>exercising</strong> <strong>control. </strong>Control can be exercised by\nimplementing any of the learning strategies (such as rehearsal, elaboration, summarising\nor asking themselves questions) chosen in the first step. Help-seeking can also\nbe a form of control, but only when the learner uses it to develop their own\nskill or understanding: help-seeking is not considered self-regulatory behaviour\nwhen it is used as a crutch to arrive at the answer without the hard work.\nControl can also take the form of using attention-focusing strategies such as\nturning off all music, sitting alone, or going to the library, and it involves\npostponing enjoyable activities in order to make progress towards one’s goals.\nSimply put, control is general persistence to stick with the strategies that\nwork.</p>\n<p>Next,\nself-regulated learners <strong>monitor progress</strong>\ntowards their goal. Individuals can monitor their own understanding,\nmotivation, feelings, or behaviour towards a goal. For example, by using the\nmetacognitive strategy they decided to use in the goal-setting stage (asking\nthemselves questions), students can clarify for themselves what they do and do\nnot yet know. Other ways of self-monitoring include keeping track of how much\nstudying truly gets done with a study group, or noticing which contexts and\nenvironments allow them to focus on their work.</p>\n<p>Finally,\nstudents use the information gathered through the previous self-evaluation to\nmetacognitively <strong>reflect and respond</strong>.\nA student’s confidence in their own abilities will shape how they reflect on\ntheir progress or lack thereof. For example, a student with a stable, high\nbelief that they are capable will attribute a low grade on a math test to their\nlack of sleep the night before or their minimal study time as opposed to a lack\nof intelligence. Responding to a self-evaluation functions like a thermostat,\neither turning up the dial on effort to increase progress towards one’s goals\nor easing back to focus on other tasks. This adjustment can manifest as\nhelp-seeking behaviour, persistence, or shifting learning strategies.</p>\n<h4><strong>Why\nis self-regulation important?</strong></h4>\n<p>It\nis increasingly important that students are able to proactively evaluate and\nimprove upon their own learning. In a rapidly changing world, successful\nindividuals must be life-long learners who are metacognitive about and able to\neffectively evaluate their learning. Within the education system, students\nwithout the ability to focus their attention and maintain perseverance will be\nconstantly pulled left and right by their immediate impulses. Furthermore,\nstudents who fail to learn self-evaluation strategies will not be able to\neffectively direct their attention towards the areas that need it the most.\nWhile some students may find poor study conditions, confusing lessons or difficult\ntexts to be insurmountable obstacles, self-regulation allows learners to navigate\nthese conditions by discovering solutions that work.</p>\n<p>In\naddition to developing personal responsibility about learning, self-regulation also\nsolidifies the content of learning. Self-regulation practices improve the\nencoding of knowledge and skills in memory, especially in reading comprehension\nand writing.<a href=\"#_edn3\">[iii]</a>\nResearch has also identified that self-regulation strategies are associated\nwith increased student effort and motivation, improved scores on standardised\ntests and general preparedness for class.</p>\n<h4><strong>How\ndo we cultivate self-regulation?</strong></h4>\n<p>As\ndiscussed above, the self-regulation process is composed of a series of steps.\nThese steps are not rigid in their order. In actuality, self-regulated learners\nengage in many of these processes simultaneously or shift the steps as they\nbecome adept self-regulators. To teach and develop student self-regulation as a\nwhole, teachers can support each of the underlying stages. It is also important\nto support students’ self-efficacy, encourage them to adopt a growth mindset and\nprioritise learning over grades and marks.</p>\n<p><strong>Match the form of learning with appropriate strategies</strong></p>\n<p>In\nthis first stage, students identify particular learning strategies that fit\nwith their goals. Basic learning tasks such as encoding information for memory\nrecall are best learned through rehearsal, organisation or categorisation,\nmnemonic devices, or paraphrasing the information. However, more elaborate\nstrategies are used when students are asked to make information meaningful. In\nbuilding connections between new concepts and a learner’s existing knowledge,\nstudents may choose to list underlying causes or themes, outline the structure\nof the process or paper, or diagram spatial relationships to create a network of\nideas. This is not a comprehensive catalogue of learning strategies but serves\nto illustrate the value in carefully choosing a learning strategy to align with\ngoals. It is important for teachers to explicitly teach a range of learning strategies,\nand to enable and support students to determine which form of learning strategy\nis most appropriate for the type of work. </p>\n<p><strong>Always include positive feedback</strong></p>\n<p>Maintaining\nattention throughout a task takes practice. However, teachers can support students’\nfocus through positive feedback. Students often adopt their teacher’s\nevaluations of their work as their own, which means that teachers can highly\ninfluence a student’s persistence in engaging with a task or giving up. In\naddition, developing a culture around celebrating mistakes as opportunities to\nlearn is crucial. Authentically discussing areas of improvement allows room for\ngrowth, and an inclusion of positive feedback should not be interpreted as giving\nexclusively positive feedback.\nTeachers can also use their expertise to differentiate their level of positive\nand negative feedback according to student self-efficacy in a particular task.</p>\n<p><strong>Maintain an environment conducive to focus</strong></p>\n<p>Teachers\ncan ensure that the study environment is conducive to focus, as a relatively\nquiet space for individual work is invaluable. Beyond this, students learn how\nto regulate their own attention and impulses best through sustained and regular\npractice, increasing in duration each session. While collaboration and\ndiscussion are an important part of learning, self-regulation becomes much more\nchallenging in a noisy environment. In secondary education this is particularly\nimportant, as the higher critical thinking skills required by adolescents are\nseverely inhibited by distractions. Teachers can further support the\ndevelopment of self-regulation by providing complex, open-ended tasks that give\nstudents the opportunity to practise managing distractions and maintaining\nfocus while tackling increasingly challenging academic work.</p>\n<p><strong>Guide students to track their progress</strong></p>\n<p>At\nthe heart of monitoring understanding lies the question: ‘what do I know, and\nhow can I improve?’ Students can push themselves to become aware of the limits\nof their own knowledge through recall, practice and extension, depending on the\nnature of the goal. One monitoring strategy might be summarising the main\npoints of a lesson following direct instruction. A student trying to increase\nher reading comprehension may pause to ask herself questions about the text (at\nvarying levels of complexity). </p>\n<p>Some\nstudents may wish to improve their time management skills. These students would\nbenefit from keeping a record of how they spend their time and then comparing\nit with their task goals. For example, I may believe that two hours of studying\nwith a study group each week is a strong plan in preparing for a test at the\nend of the term. However, I may in fact find that one of the two hours is\ngenerally spent socialising. This new information can then be used to shift my behaviour\nmoving forward. </p>\n<p><strong>Practise evaluating ‘like a detective’<",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Self-regulation is the process by which students monitor and control their cognition, motivation, and behaviour in order to achieve certain goals. There are several interweaving theories of self-regulation, but most common models conceptualise self-regulation in terms of a series of steps involving forethought or planning, performance, and reflection[i][ii]. These steps can be explicitly taught and, while self-regulation increases to some extent with age, the research is clear that self-regulati",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Self-regulation is the process by which students monitor and control their cognition, motivation, and behaviour in order to achieve certain goals. There are several interweaving theories of self-regulation, but most common models conceptualise self-regulation in terms of a series of steps involving forethought or planning, performance, and reflection[i][ii]. These steps can be explicitly taught and, while self-regulation increases to some extent with age, the research is clear that self-regulati",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h4",
              "text": "Why\nis self-regulation important?",
              "id": ""
            },
            {
              "level": "h4",
              "text": "How\ndo we cultivate self-regulation?",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://originality.ai/blog/fact-checking-guide-to-online-automated-and-manual-methods",
      "title": "Fact-Checking: A Guide to Online, Automated, and Manual Methods – Originality.AI",
      "author": "Sherice JacobAugust 8, 2024",
      "published_date": "2024-08-08T00:00:00.000Z",
      "content": {
        "text": "<div><div><p>Mark Twain once said, “A lie can travel halfway around the world while the truth is still putting on its shoes.” Even he would’ve been impressed at the sheer speed at which information as a whole travels today, to say nothing of misinformation and active disinformation campaigns. </p><p>As a result of the surge in unverified information, fact-checking, or the process of verifying the accuracy of information shared, has become a vital part of the online content creation, communication and publishing world. </p><p>But what are some of the ways that writers, content creators, journalists and other content specialists fact-check the material they write? In this guide, we’ll take a closer look at online, automated and manual methods for fact checking to help you determine which one may be right for your needs. Let’s take a closer look. </p><h2><strong>Online Fact Checking Methods</strong></h2><p>With so much information available online, it’s easy to see how online fact checking tools can be so popular. Fact-checking websites, image search and easy source verification make it faster than ever to verify the authenticity of everything from photos to news stories. </p><h3><strong>Fact Checking Websites</strong></h3><p>Websites like Snopes have historically been popular for debunking urban legends and outrageous claims, but in recent years, with so much riding on U.S. political elections and what the candidates say, websites like PolitiFact and FactCheck.org have also become more popular to evaluate the candidates in a way that tries to be non-partisan and authentic in its results. </p><h3><strong>Reverse Image Search</strong></h3><p>Online tools like Google Reverse Image Search or TinEye help users to determine the origin of an image. If you’ve ever wondered, “Is that photo real?” these services will help you verify the authenticity and originality of a photo and see for yourself whether or not it has been digitally manipulated. </p><h3><strong>Source Verification</strong></h3><p>It’s easy to simply hit “share” online and not think of the consequences, but these days, with fake news, deepfake videos and outrageous news claims as the order of the day (and spreading) on social media, taking an extra step to <a href=\"https://originality.ai/blog/article-credibility-authenticy-check\"><strong>check the credibility</strong></a> of the source is crucial. Websites like Media Bias/Fact Check exist for this very reason – to determine how reliable and reputable media outlets are when they make claims. </p><h2><strong>Automated Fact Checking Methods</strong></h2><p>Thanks to the development of AI, machine learning and similar technologies, much of the heavy lifting of fact-checking can be automated. AI and other systems are essentially trained on various patterns and when trained well, can excel at sifting through large amounts of data to flag something peculiar or otherwise inconsistent. Although no automated <a href=\"https://originality.ai/automated-fact-checker\"><strong>fact checker</strong></a> is 100% accurate all of the time, recent developments have continued to refine them. </p><h3><strong>Claim Verification</strong></h3><p>Through the use of pattern recognition and a large assortment of databases, AI algorithms can cross-reference statements with reliable sources. Full Fact’s own live fact checking tool uses this type of technology to verify claims in real-time. </p><h3><strong>Natural Language Processing (NLP)</strong></h3><p>Natural language processing or NLP helps in the identification of false or misleading information by looking at the structure and context of the statements. Misinformation often shows patterns that NLP tools can readily identify. </p><h3><strong>Deepfake Detection</strong></h3><p>Deepfakes, or AI-generated videos that mimic real people, pose a considerable threat to the spread of misinformation. There are several online tools that have been developed recently to help with deepfake detection, including Sentinel, Intel’s real-time Deepfake Director and Microsoft’s Video Authenticator among others. </p><h2><strong>Manual Fact Checking Methods</strong></h2><p>Despite advances in technology and its role in automating the detection of everything from videos to language patterns, there are still some things that humans admittedly do better. Our intuition and critical thinking skills can’t be matched by a machine, which is where manual fact checking thrives. </p><h3><strong>Primary Source Verification</strong></h3><p>This method involves going to the original source of a claim. For example, if a statistic is quoted from a research paper, the actual paper in question should be consulted to check its accuracy. </p><h3><strong>Expert Consultations</strong></h3><p>Consulting with experts in a particular field, especially in highly scientific or technical fields can help shed more light on controversial topics that are ripe for misinformation or disinformation campaigns. </p><h3><strong>Cross-Referencing</strong></h3><p>Cross referencing involves comparing information from a variety of sources. If all of the sources verify the claim, it’s likely that it’s true. </p><h2><strong>Not a One Size Fits All Process</strong></h2><p>Fact-checking isn’t a one-size fits all process. There are going to be instances where one type of fact check is preferable to another purely because of speed and efficiency. With this in mind, here are our suggestions for the best type of fact-checking for your specific needs:</p><h3><strong>Verify Breaking News</strong></h3><p>Manual fact checking is ideal for verifying breaking news. Since these types of situations are so dynamic and there are so few facts at the beginning, journalists need to cross-reference with primary sources including any eyewitness accounts, together with experts and other officials to report quickly and accurately on an evolving situation. </p><h3><a href=\"https://originality.ai/blog/fact-checking-for-academics\"><strong>Academic Claims</strong></a></h3><p>Academic (or deeply scientific or technical) information often requires deep and specialized knowledge. Being able to consult with experts can be invaluable in these highly specialized fields. There’s a reason why scholarly papers and research are often peer-reviewed! Consulting with experts in the field helps ensure that the information is reported on accurately and that the data is interpreted correctly. </p><h3><strong>Image or Video Verification</strong></h3><p>Being able to determine the authenticity of an image is a much easier and faster task thanks to online tools like reverse image search. These online tools can help you determine if the content has appeared elsewhere or if it’s showing signs of having been doctored or manipulated. </p><h3><strong>Long-Form Investigative Journalism</strong></h3><p>Investigative pieces often dive deep to uncover new information. Online resources can help to scratch the surface but manual fact checking will make sure that all of the details are correct before publication. </p><h3><strong>Speeches or Live Debates</strong></h3><p>Automated tools can cross-reference statements against reliable sources in real-time, which can help give the audience immediate context or corrections if need be. </p><p>As you can see, proper fact checking is essential in this day and age. And although technology offers us several tools to help shortcut the process, human scrutiny of data remains at the heart of being able to tell facts from falsehoods. Whether you ultimately decide to use online resources, automated tools or more manual methods, making sure to fact-check your information before publishing is one small step that you can take toward making a big difference in how we consume and understand the information around us.</p></div></div>",
        "html": "<div><div><p>Mark Twain once said, “A lie can travel halfway around the world while the truth is still putting on its shoes.” Even he would’ve been impressed at the sheer speed at which information as a whole travels today, to say nothing of misinformation and active disinformation campaigns. </p><p>As a result of the surge in unverified information, fact-checking, or the process of verifying the accuracy of information shared, has become a vital part of the online content creation, communication and publishing world. </p><p>But what are some of the ways that writers, content creators, journalists and other content specialists fact-check the material they write? In this guide, we’ll take a closer look at online, automated and manual methods for fact checking to help you determine which one may be right for your needs. Let’s take a closer look. </p><h2><strong>Online Fact Checking Methods</strong></h2><p>With so much information available online, it’s easy to see how online fact checking tools can be so popular. Fact-checking websites, image search and easy source verification make it faster than ever to verify the authenticity of everything from photos to news stories. </p><h3><strong>Fact Checking Websites</strong></h3><p>Websites like Snopes have historically been popular for debunking urban legends and outrageous claims, but in recent years, with so much riding on U.S. political elections and what the candidates say, websites like PolitiFact and FactCheck.org have also become more popular to evaluate the candidates in a way that tries to be non-partisan and authentic in its results. </p><h3><strong>Reverse Image Search</strong></h3><p>Online tools like Google Reverse Image Search or TinEye help users to determine the origin of an image. If you’ve ever wondered, “Is that photo real?” these services will help you verify the authenticity and originality of a photo and see for yourself whether or not it has been digitally manipulated. </p><h3><strong>Source Verification</strong></h3><p>It’s easy to simply hit “share” online and not think of the consequences, but these days, with fake news, deepfake videos and outrageous news claims as the order of the day (and spreading) on social media, taking an extra step to <a href=\"https://originality.ai/blog/article-credibility-authenticy-check\"><strong>check the credibility</strong></a> of the source is crucial. Websites like Media Bias/Fact Check exist for this very reason – to determine how reliable and reputable media outlets are when they make claims. </p><h2><strong>Automated Fact Checking Methods</strong></h2><p>Thanks to the development of AI, machine learning and similar technologies, much of the heavy lifting of fact-checking can be automated. AI and other systems are essentially trained on various patterns and when trained well, can excel at sifting through large amounts of data to flag something peculiar or otherwise inconsistent. Although no automated <a href=\"https://originality.ai/automated-fact-checker\"><strong>fact checker</strong></a> is 100% accurate all of the time, recent developments have continued to refine them. </p><h3><strong>Claim Verification</strong></h3><p>Through the use of pattern recognition and a large assortment of databases, AI algorithms can cross-reference statements with reliable sources. Full Fact’s own live fact checking tool uses this type of technology to verify claims in real-time. </p><h3><strong>Natural Language Processing (NLP)</strong></h3><p>Natural language processing or NLP helps in the identification of false or misleading information by looking at the structure and context of the statements. Misinformation often shows patterns that NLP tools can readily identify. </p><h3><strong>Deepfake Detection</strong></h3><p>Deepfakes, or AI-generated videos that mimic real people, pose a considerable threat to the spread of misinformation. There are several online tools that have been developed recently to help with deepfake detection, including Sentinel, Intel’s real-time Deepfake Director and Microsoft’s Video Authenticator among others. </p><h2><strong>Manual Fact Checking Methods</strong></h2><p>Despite advances in technology and its role in automating the detection of everything from videos to language patterns, there are still some things that humans admittedly do better. Our intuition and critical thinking skills can’t be matched by a machine, which is where manual fact checking thrives. </p><h3><strong>Primary Source Verification</strong></h3><p>This method involves going to the original source of a claim. For example, if a statistic is quoted from a research paper, the actual paper in question should be consulted to check its accuracy. </p><h3><strong>Expert Consultations</strong></h3><p>Consulting with experts in a particular field, especially in highly scientific or technical fields can help shed more light on controversial topics that are ripe for misinformation or disinformation campaigns. </p><h3><strong>Cross-Referencing</strong></h3><p>Cross referencing involves comparing information from a variety of sources. If all of the sources verify the claim, it’s likely that it’s true. </p><h2><strong>Not a One Size Fits All Process</strong></h2><p>Fact-checking isn’t a one-size fits all process. There are going to be instances where one type of fact check is preferable to another purely because of speed and efficiency. With this in mind, here are our suggestions for the best type of fact-checking for your specific needs:</p><h3><strong>Verify Breaking News</strong></h3><p>Manual fact checking is ideal for verifying breaking news. Since these types of situations are so dynamic and there are so few facts at the beginning, journalists need to cross-reference with primary sources including any eyewitness accounts, together with experts and other officials to report quickly and accurately on an evolving situation. </p><h3><a href=\"https://originality.ai/blog/fact-checking-for-academics\"><strong>Academic Claims</strong></a></h3><p>Academic (or deeply scientific or technical) information often requires deep and specialized knowledge. Being able to consult with experts can be invaluable in these highly specialized fields. There’s a reason why scholarly papers and research are often peer-reviewed! Consulting with experts in the field helps ensure that the information is reported on accurately and that the data is interpreted correctly. </p><h3><strong>Image or Video Verification</strong></h3><p>Being able to determine the authenticity of an image is a much easier and faster task thanks to online tools like reverse image search. These online tools can help you determine if the content has appeared elsewhere or if it’s showing signs of having been doctored or manipulated. </p><h3><strong>Long-Form Investigative Journalism</strong></h3><p>Investigative pieces often dive deep to uncover new information. Online resources can help to scratch the surface but manual fact checking will make sure that all of the details are correct before publication. </p><h3><strong>Speeches or Live Debates</strong></h3><p>Automated tools can cross-reference statements against reliable sources in real-time, which can help give the audience immediate context or corrections if need be. </p><p>As you can see, proper fact checking is essential in this day and age. And although technology offers us several tools to help shortcut the process, human scrutiny of data remains at the heart of being able to tell facts from falsehoods. Whether you ultimately decide to use online resources, automated tools or more manual methods, making sure to fact-check your information before publishing is one small step that you can take toward making a big difference in how we consume and understand the information around us.</p></div></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Mark Twain once said, “A lie can travel halfway around the world while the truth is still putting on its shoes.” Even he would’ve been impressed at the sheer speed at which information as a whole travels today, to say nothing of misinformation and active disinformation campaigns.As a result of the surge in unverified information, fact-checking, or the process of verifying the accuracy of information shared, has become a vital part of the online content creation, communication and publishing worl",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Mark Twain once said, “A lie can travel halfway around the world while the truth is still putting on its shoes.” Even he would’ve been impressed at the sheer speed at which information as a whole travels today, to say nothing of misinformation and active disinformation campaigns.As a result of the surge in unverified information, fact-checking, or the process of verifying the accuracy of information shared, has become a vital part of the online content creation, communication and publishing worl",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Online Fact Checking Methods",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Fact Checking Websites",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Reverse Image Search",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Source Verification",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Automated Fact Checking Methods",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Claim Verification",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Natural Language Processing (NLP)",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Deepfake Detection",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Manual Fact Checking Methods",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Primary Source Verification",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Expert Consultations",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Cross-Referencing",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Not a One Size Fits All Process",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Verify Breaking News",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Academic Claims",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Image or Video Verification",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Long-Form Investigative Journalism",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Speeches or Live Debates",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.researchgate.net/publication/307552369_Error_detection_and_self-assessment_as_mechanisms_to_promote_self-regulation_of_learning_among_secondary_education_students",
      "title": "(PDF) Error detection and self-assessment as mechanisms to promote self-regulation of learning among secondary education students",
      "author": "",
      "published_date": "2018-04-01T00:00:00.000Z",
      "content": {
        "text": "<div><section><div><div><p>The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on student’s self-assessment. Nevertheless, the divergence in both their self-assessment and error detection was most pronounced when not using the assessment instrument. Furthermore, the correlation analysis revealed that error detection was significantly and positively correlated with students’ performance. Students who conducted error detection and subsequently formulated and completed self-regulation activities achieved better performance. Our results suggest that the use of error detection as a tool has considerable potential in the teaching and learning process.</p><div><p>Figures - uploaded by <a href=\"https://www.researchgate.net/publication/profile/Angela-Zamora\">Ángela Zamora</a></p><div><p><span>Author content</span></p><div><p>All figure content in this area was uploaded by Ángela Zamora</p></div></div></div><p>Content may be subject to copyright.</p></div><div><p><strong>Discover the world's research</strong></p><ul><li>25+ million members</li><li>160+ million publication pages</li><li>2.3+ billion citations</li></ul><p><a href=\"https://www.researchgate.net/publication/signup.SignUp.html\"><span>Join for free</span></a></p></div></div><section><span></span><a href=\"https://www.researchgate.net/publication/publication/307552369_Error_detection_and_self-assessment_as_mechanisms_to_promote_self-regulation_of_learning_among_secondary_education_students#read-preview\"></a><div>\n<div><div><p>Error detection and self-assessment as mechanisms to promote self-regulation</p><p>of learning among secondary education students</p><p>!</p><p>Angela Zamora</p><p>a</p><p>, Jos<span>!</span></p><p>e Manuel Su<span>!</span></p><p>arez</p><p>b</p><p>, and Diego Ardura</p><p>c</p><p>,</p><p>d</p><p>a</p><p>Universidad Nacional de Educaci<span>!</span></p><p>on a Distancia, Gij<span>!</span></p><p>on, Spain;</p><p>b</p><p>Departamento de M<span>!</span></p><p>etodos de Investigaci<span>!</span></p><p>on y Diagn<span>!</span></p><p>ostico en Educaci<span>!</span></p><p>on II, Universidad</p><p>Nacional de Educaci<span>!</span></p><p>on a Distancia, Madrid, Spain;</p><p>c</p><p>Universidad Internacional de la Rioja, La Rioja, Spain;</p><p>d</p><p>Colegio Santo Domingo de Guzm<span>!</span></p><p>an-FESD,</p><p>Oviedo, Spain</p><p>ARTICLE HISTORY</p><p>Received 3 March 2016</p><p>Revised 26 May 2016</p><p>Accepted 7 August 2016</p><p>ABSTRACT</p><p>The authors<span>’</span><span>objective was to study the role of error detection and retroactive self-regulation as</span></p><p>determinants of performance in secondary education students. A total of 198 students participated in the</p><p>quasiexperimental study, which involved a control group and two experimental groups. This enabled the</p><p>authors to analyze the effects of both error detection and the subsequent self-regulation by means of</p><p>several analyses of variance. In addition, the authors analyzed the effect of an assessment script on</p><p>student<span>’</span><span>s self-assessment. Nevertheless, the divergence in both their self-assessment and error detection</span></p><p>was most pronounced when not using the assessment instrument. Furthermore, the correlation analysis</p><p>revealed that error detection was signi<span>ﬁ</span><span>cantly and positively correlated with students<span>’</span>performance.</span></p><p>Students who conducted error detection and subsequently formulated and completed self-regulation</p><p>activities achieved better performance. The study results suggest that the use of error detection<span> </span>as a tool</p><p>has considerable potential in the teaching and learning process.</p><p>KEYWORDS</p><p>Assessment scripts; error</p><p>detection; secondary school;</p><p>self-assessment; self-</p><p>regulated learning</p><p>The competence of learning to learn is closely related to the</p><p>process of self-regulation. The goal of the former is to equip</p><p>people for independent learning by helping them develop the</p><p>tools necessary to undertake life-long learning bey<span></span>ond their</p><p>schooling (N<span>!</span></p><p>u<span>~</span></p><p>nez, Solano, Gonz<span>!</span></p><p>alez-Pienda, &amp; Ros<span>!</span></p><p>ario, <span>2006</span>).</p><p>Furthermore, learners must acquire the necessary compe-</p><p>tencies that will enable them to identify when they are learning</p><p>and when they are not, and above all to recognize the strategies</p><p>that help them learn better (Mart<span></span><span>!</span></p><p>ı<span>n, <span>2008</span><span>). The competence of</span></span></p><p>learning to learn refers to:</p><p>The awareness, management and self-control of their own capabili-</p><p>ties and knowledge based on a sense of personal ef<span>ﬁ</span><span>cacy, and</span></p><p>includes strategic thinking, the ability to cooperate and to assess</p><p>themselves, and the ef<span>ﬁ</span><span>cient deployment of a series of intellectual</span></p><p>resources and techniques, all of which are developed through con-</p><p>scious and rewarding learning experiences, whether individual or</p><p>collective. (Cabrerizo, Rubio, &amp; Castillo, <span>2008</span><span>, p. 106)</span></p><p>Although the superposition of several effective practices are</p><p>key to the success of the learning to learn approach, there are</p><p>two basic principles to help students develop their learning to</p><p>learn competence (Sanmart<span></span><span>!</span></p><p>ı<span>,<span>2007</span><span>). First, students must be</span></span></p><p>aware of their academic goals. Second, students must<span> </span>be able to</p><p>detect and learn from their mistakes. The acquisition of this</p><p>competence entails the development of both cognitive and</p><p>emotional skills (Mart<span></span><span>!</span></p><p>ı<span>n, <span>2008</span><span>). Therefore, self-regulated learn-</span></span></p><p>ing is crucial to the development of the competence of learning</p><p>to learn, as it is related to forms of independent and<span> </span>effective</p><p>academic learning that involve metacognition, intrinsic motiva-</p><p>tion, and strategic action (Perry, <span>2002</span><span>). Besides, self-assessment</span></p><p>is fundamental to self-regulation, as it implies the re<span></span><span>ﬂ</span>ection</p><p>and awareness of how the learning process in progressing (Pan-</p><p>adero &amp; Alonso, <span>2013</span><span>). For these reasons, we brie<span>ﬂ</span><span>y review pre-</span></span></p><p>vious research <span>ﬁ</span><span>ndings on the role of self-regulation, error</span></p><p>detection, and self-assessment as key variables for the develop-</p><p>ment of learning to learn competence and, subse<span></span>quently, for</p><p>academic performance.</p><p>Self-regulated learning and performance</p><p>Self-r<span></span>egulated le<span></span>arning is un<span></span>derstood as an<span> </span>activi<span></span>ty performe<span></span>d by</p><p>the stud<span></span>ents themse<span></span>lves (i.e.<span></span>, proactive<span></span>ly). Accord<span></span>ing to Zimme<span></span>r-</p><p>man (<span>2001</span><span>), </span><span>“</span><span>Lea<span></span>rning is not s<span></span>omething th<span></span>at happens to s<span></span>tudents;</span></p><p>it is some<span></span>thing that h<span></span>appens by stu<span></span>dents<span>”</span><span>(p. 33). The construct</span></p><p>known as self<span></span>-regulat<span></span>ed learning can be de<span></span><span>ﬁ<span>ned as the process</span></span></p><p>whereby<span> </span>student<span></span>s activate an<span></span>d sustain cog<span></span>nitions, be<span></span>haviors, an<span></span>d</p><p>affect<span></span>s that are syst<span></span>ematicall<span></span>y oriented t<span></span>oward attai<span></span>nment of the<span></span>ir</p><p>goals, an<span></span>d which are al<span></span>l produced i<span></span>n a cyclical m<span></span>anner (Schu<span></span>nk &amp;</p><p>Zimmer<span></span>man, <span>1994</span><span>; Zimme<span></span>rman, <span>1989</span><span>,<span>2000</span></span><span>). Accor<span></span>ding to the</span></span></p><p>model pro<span></span>posed by Zimme<span></span>rman (<span>2000</span><span>), the sel<span></span>f-regulat<span></span>ed learn-</span></p><p>ing cycle<span> </span>occurs i<span></span>n three phas<span></span>es: foretho<span></span>ught, whic<span></span>h involves th<span></span>e</p><p>proces<span></span>ses that pre<span></span>cede perfor<span></span>mance of the ta<span></span>sk; perform<span></span>ance,</p><p>involv<span></span>ing the proce<span></span>sses that occ<span></span>ur during the t<span></span>ask; and self<span></span>-re<span>ﬂ</span><span>ec-</span></p><p>tion, in<span></span>volving the p<span></span>rocesses th<span></span>at occur afte<span></span>r learning an<span></span>d are</p><p>aimed at ass<span></span>essing perf<span></span>ormance.</p><p>CONTACT</p><p>!</p><p>Angela Zamora<span> </span><span>angzamora@gijon.uned.es </span><span>Department of Science Education, Universidad Nacional de Educaci<span>!</span></span></p><p>on a Distancia, Apdo. Correos 60.141,</p><p>28080 Madrid, Espa<span>~</span></p><p>na.</p><p>© 2018 Taylor &amp; Francis Group, LLC</p><p>THE JOURNAL OF EDUCATIONAL RESEARCH</p><p>2018, VOL. 111, NO. 2, 175<span>–</span>185</p><p>https://doi.org/10.1080/00220671.2016.1225657</p></div><a href=\"https://www.researchgate.net/publicati",
        "html": "<div><section><div><div><p>The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on student’s self-assessment. Nevertheless, the divergence in both their self-assessment and error detection was most pronounced when not using the assessment instrument. Furthermore, the correlation analysis revealed that error detection was significantly and positively correlated with students’ performance. Students who conducted error detection and subsequently formulated and completed self-regulation activities achieved better performance. Our results suggest that the use of error detection as a tool has considerable potential in the teaching and learning process.</p><div><p>Figures - uploaded by <a href=\"https://www.researchgate.net/publication/profile/Angela-Zamora\">Ángela Zamora</a></p><div><p><span>Author content</span></p><div><p>All figure content in this area was uploaded by Ángela Zamora</p></div></div></div><p>Content may be subject to copyright.</p></div><div><p><strong>Discover the world's research</strong></p><ul><li>25+ million members</li><li>160+ million publication pages</li><li>2.3+ billion citations</li></ul><p><a href=\"https://www.researchgate.net/publication/signup.SignUp.html\"><span>Join for free</span></a></p></div></div><section><span></span><a href=\"https://www.researchgate.net/publication/publication/307552369_Error_detection_and_self-assessment_as_mechanisms_to_promote_self-regulation_of_learning_among_secondary_education_students#read-preview\"></a><div>\n<div><div><p>Error detection and self-assessment as mechanisms to promote self-regulation</p><p>of learning among secondary education students</p><p>!</p><p>Angela Zamora</p><p>a</p><p>, Jos<span>!</span></p><p>e Manuel Su<span>!</span></p><p>arez</p><p>b</p><p>, and Diego Ardura</p><p>c</p><p>,</p><p>d</p><p>a</p><p>Universidad Nacional de Educaci<span>!</span></p><p>on a Distancia, Gij<span>!</span></p><p>on, Spain;</p><p>b</p><p>Departamento de M<span>!</span></p><p>etodos de Investigaci<span>!</span></p><p>on y Diagn<span>!</span></p><p>ostico en Educaci<span>!</span></p><p>on II, Universidad</p><p>Nacional de Educaci<span>!</span></p><p>on a Distancia, Madrid, Spain;</p><p>c</p><p>Universidad Internacional de la Rioja, La Rioja, Spain;</p><p>d</p><p>Colegio Santo Domingo de Guzm<span>!</span></p><p>an-FESD,</p><p>Oviedo, Spain</p><p>ARTICLE HISTORY</p><p>Received 3 March 2016</p><p>Revised 26 May 2016</p><p>Accepted 7 August 2016</p><p>ABSTRACT</p><p>The authors<span>’</span><span>objective was to study the role of error detection and retroactive self-regulation as</span></p><p>determinants of performance in secondary education students. A total of 198 students participated in the</p><p>quasiexperimental study, which involved a control group and two experimental groups. This enabled the</p><p>authors to analyze the effects of both error detection and the subsequent self-regulation by means of</p><p>several analyses of variance. In addition, the authors analyzed the effect of an assessment script on</p><p>student<span>’</span><span>s self-assessment. Nevertheless, the divergence in both their self-assessment and error detection</span></p><p>was most pronounced when not using the assessment instrument. Furthermore, the correlation analysis</p><p>revealed that error detection was signi<span>ﬁ</span><span>cantly and positively correlated with students<span>’</span>performance.</span></p><p>Students who conducted error detection and subsequently formulated and completed self-regulation</p><p>activities achieved better performance. The study results suggest that the use of error detection<span> </span>as a tool</p><p>has considerable potential in the teaching and learning process.</p><p>KEYWORDS</p><p>Assessment scripts; error</p><p>detection; secondary school;</p><p>self-assessment; self-</p><p>regulated learning</p><p>The competence of learning to learn is closely related to the</p><p>process of self-regulation. The goal of the former is to equip</p><p>people for independent learning by helping them develop the</p><p>tools necessary to undertake life-long learning bey<span></span>ond their</p><p>schooling (N<span>!</span></p><p>u<span>~</span></p><p>nez, Solano, Gonz<span>!</span></p><p>alez-Pienda, &amp; Ros<span>!</span></p><p>ario, <span>2006</span>).</p><p>Furthermore, learners must acquire the necessary compe-</p><p>tencies that will enable them to identify when they are learning</p><p>and when they are not, and above all to recognize the strategies</p><p>that help them learn better (Mart<span></span><span>!</span></p><p>ı<span>n, <span>2008</span><span>). The competence of</span></span></p><p>learning to learn refers to:</p><p>The awareness, management and self-control of their own capabili-</p><p>ties and knowledge based on a sense of personal ef<span>ﬁ</span><span>cacy, and</span></p><p>includes strategic thinking, the ability to cooperate and to assess</p><p>themselves, and the ef<span>ﬁ</span><span>cient deployment of a series of intellectual</span></p><p>resources and techniques, all of which are developed through con-</p><p>scious and rewarding learning experiences, whether individual or</p><p>collective. (Cabrerizo, Rubio, &amp; Castillo, <span>2008</span><span>, p. 106)</span></p><p>Although the superposition of several effective practices are</p><p>key to the success of the learning to learn approach, there are</p><p>two basic principles to help students develop their learning to</p><p>learn competence (Sanmart<span></span><span>!</span></p><p>ı<span>,<span>2007</span><span>). First, students must be</span></span></p><p>aware of their academic goals. Second, students must<span> </span>be able to</p><p>detect and learn from their mistakes. The acquisition of this</p><p>competence entails the development of both cognitive and</p><p>emotional skills (Mart<span></span><span>!</span></p><p>ı<span>n, <span>2008</span><span>). Therefore, self-regulated learn-</span></span></p><p>ing is crucial to the development of the competence of learning</p><p>to learn, as it is related to forms of independent and<span> </span>effective</p><p>academic learning that involve metacognition, intrinsic motiva-</p><p>tion, and strategic action (Perry, <span>2002</span><span>). Besides, self-assessment</span></p><p>is fundamental to self-regulation, as it implies the re<span></span><span>ﬂ</span>ection</p><p>and awareness of how the learning process in progressing (Pan-</p><p>adero &amp; Alonso, <span>2013</span><span>). For these reasons, we brie<span>ﬂ</span><span>y review pre-</span></span></p><p>vious research <span>ﬁ</span><span>ndings on the role of self-regulation, error</span></p><p>detection, and self-assessment as key variables for the develop-</p><p>ment of learning to learn competence and, subse<span></span>quently, for</p><p>academic performance.</p><p>Self-regulated learning and performance</p><p>Self-r<span></span>egulated le<span></span>arning is un<span></span>derstood as an<span> </span>activi<span></span>ty performe<span></span>d by</p><p>the stud<span></span>ents themse<span></span>lves (i.e.<span></span>, proactive<span></span>ly). Accord<span></span>ing to Zimme<span></span>r-</p><p>man (<span>2001</span><span>), </span><span>“</span><span>Lea<span></span>rning is not s<span></span>omething th<span></span>at happens to s<span></span>tudents;</span></p><p>it is some<span></span>thing that h<span></span>appens by stu<span></span>dents<span>”</span><span>(p. 33). The construct</span></p><p>known as self<span></span>-regulat<span></span>ed learning can be de<span></span><span>ﬁ<span>ned as the process</span></span></p><p>whereby<span> </span>student<span></span>s activate an<span></span>d sustain cog<span></span>nitions, be<span></span>haviors, an<span></span>d</p><p>affect<span></span>s that are syst<span></span>ematicall<span></span>y oriented t<span></span>oward attai<span></span>nment of the<span></span>ir</p><p>goals, an<span></span>d which are al<span></span>l produced i<span></span>n a cyclical m<span></span>anner (Schu<span></span>nk &amp;</p><p>Zimmer<span></span>man, <span>1994</span><span>; Zimme<span></span>rman, <span>1989</span><span>,<span>2000</span></span><span>). Accor<span></span>ding to the</span></span></p><p>model pro<span></span>posed by Zimme<span></span>rman (<span>2000</span><span>), the sel<span></span>f-regulat<span></span>ed learn-</span></p><p>ing cycle<span> </span>occurs i<span></span>n three phas<span></span>es: foretho<span></span>ught, whic<span></span>h involves th<span></span>e</p><p>proces<span></span>ses that pre<span></span>cede perfor<span></span>mance of the ta<span></span>sk; perform<span></span>ance,</p><p>involv<span></span>ing the proce<span></span>sses that occ<span></span>ur during the t<span></span>ask; and self<span></span>-re<span>ﬂ</span><span>ec-</span></p><p>tion, in<span></span>volving the p<span></span>rocesses th<span></span>at occur afte<span></span>r learning an<span></span>d are</p><p>aimed at ass<span></span>essing perf<span></span>ormance.</p><p>CONTACT</p><p>!</p><p>Angela Zamora<span> </span><span>angzamora@gijon.uned.es </span><span>Department of Science Education, Universidad Nacional de Educaci<span>!</span></span></p><p>on a Distancia, Apdo. Correos 60.141,</p><p>28080 Madrid, Espa<span>~</span></p><p>na.</p><p>© 2018 Taylor &amp; Francis Group, LLC</p><p>THE JOURNAL OF EDUCATIONAL RESEARCH</p><p>2018, VOL. 111, NO. 2, 175<span>–</span>185</p><p>https://doi.org/10.1080/00220671.2016.1225657</p></div><a href=\"https://www.researchgate.net/publicati",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on stud",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on stud",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on stud",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The objective of this research was to study the role of error detection and retroactive self-regulation as determinants of performance in secondary education students. A total of 198 students participated in this quasi-experimental study, which involved a control group and two experimental groups. This enabled us to analyse the effects of both error detection and the subsequent self-regulation by means of several ANOVA analyses. In addition, we analysed the effect of an assessment script on stud",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figures - uploaded byÁngela ZamoraAuthor contentAll figure content in this area was uploaded by Ángela Zamora",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Author contentAll figure content in this area was uploaded by Ángela Zamora",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "All figure content in this area was uploaded by Ángela Zamora",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Discover the world's research25+ million members160+ million publication pages2.3+ billion citationsJoin for free",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Error detection and self-assessment as mechanisms to promote self-regulationof learning among secondary education students!Angela Zamoraa, Jos!e Manuel Su!arezb, and Diego Ardurac,daUniversidad Nacional de Educaci!on a Distancia, Gij!on, Spain;bDepartamento de M!etodos de Investigaci!on y Diagn!ostico en Educaci!on II, UniversidadNacional de Educaci!on a Distancia, Madrid, Spain;cUniversidad Internacional de la Rioja, La Rioja, Spain;dColegio Santo Domingo de Guzm!an-FESD,Oviedo, SpainARTICLE HI",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Error detection and self-assessment as mechanisms to promote self-regulationof learning among secondary education students!Angela Zamoraa, Jos!e Manuel Su!arezb, and Diego Ardurac,daUniversidad Nacional de Educaci!on a Distancia, Gij!on, Spain;bDepartamento de M!etodos de Investigaci!on y Diagn!ostico en Educaci!on II, UniversidadNacional de Educaci!on a Distancia, Madrid, Spain;cUniversidad Internacional de la Rioja, La Rioja, Spain;dColegio Santo Domingo de Guzm!an-FESD,Oviedo, SpainARTICLE HI",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Error detection and self-assessment as mechanisms to promote self-regulationof learning among secondary education students!Angela Zamoraa, Jos!e Manuel Su!arezb, and Diego Ardurac,daUniversidad Nacional de Educaci!on a Distancia, Gij!on, Spain;bDepartamento de M!etodos de Investigaci!on y Diagn!ostico en Educaci!on II, UniversidadNacional de Educaci!on a Distancia, Madrid, Spain;cUniversidad Internacional de la Rioja, La Rioja, Spain;dColegio Santo Domingo de Guzm!an-FESD,Oviedo, SpainARTICLE HI",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Error detection and self-assessment as mechanisms to promote self-regulationof learning among secondary education students!Angela Zamoraa, Jos!e Manuel Su!arezb, and Diego Ardurac,daUniversidad Nacional de Educaci!on a Distancia, Gij!on, Spain;bDepartamento de M!etodos de Investigaci!on y Diagn!ostico en Educaci!on II, UniversidadNacional de Educaci!on a Distancia, Madrid, Spain;cUniversidad Internacional de la Rioja, La Rioja, Spain;dColegio Santo Domingo de Guzm!an-FESD,Oviedo, SpainARTICLE HI",
              "class": [],
              "id": ""
            }
          ],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.researchgate.net/publication/221181882_Creating_Feedback_Loops_to_Support_Organizational_Learning_and_Knowledge_Management_in_Inquiring_Organizations",
      "title": "Creating Feedback Loops to Support Organizational Learning and Knowledge Management in Inquiring Organizations. | Request PDF",
      "author": "",
      "published_date": "2001-01-01T00:00:00.000Z",
      "content": {
        "text": "<div><section><div><p>Information technology both facilitates and exacerbates the need for organizations to manage growing information and knowledge bases. Soon, the only successful enterprises may be those who have successfully evolved into learning organizations. The philosophical bases underlying traditional decision support systems (DSSs) are ill-equipped either to handle this explosion of information in today's rapidly changing business environment, or to support a learning organization. A comprehensive knowledge management system (KMS) based on the philosophies of inquiring systems (ISs) allows organizational learning to occur by providing both decision support and management of existing and created knowledge. By combining the DSS paradigm with an IS, an organization is capable of designing a comprehensive KMS that fully supports learning within the organization. The development of such a system allows executives to rely more fully on the support system, freeing up valuable time to establish goals and to guide the organization to its ultimate success.</p></div><div><div></div><p>To read the full-text of this research,<br/> you can request a copy directly from the authors.</p></div><div><div><div><div><div><p>... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives. <mark>Hall, Paradice, and Courtney (2001)</mark> argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...</p></div><div><p>... Focus on application of knowledge in decision making, rather than decision making method As reviewed in Section 2.3, a decision-making method is generally defined. And application of knowledge in organization and decision-making (Alavi &amp; Leidner, 2001;Courtney, 2001;<mark>Hall et al., 2001;</mark>Ozbayrak &amp; Bell, 2003) is an extensive research issue, and highly relevant in the field. ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Nam-Hong-Yim-31693360\"><span><span>Nam-Hong Yim</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Soung-Hie-Kim-72293646\"><span><span>Soung-Hie Kim</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Hee-Woong-Kim\"><span><span></span><span>Hee-Woong Kim</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Kee-Young-Kwahk-18684629\"><span><span>Kee-Young Kwahk</span></span></a></li></ul></div><div><p><span>In recognizing knowledge as a new resource in gaining organizational competitiveness, knowledge management suggests a method in managing and applying knowledge for improving organizational performance. Much knowledge management research has focused on identifying, storing, and disseminating process related knowledge in an organized manner. Applying knowledge to decision making has a significant impact on organizational performance than solely processing transactions for knowledge management. In this research, we suggest a method of knowledge-based decision-making using system dynamics, with an emphasis to strategic concerns. The proposed method transforms individual mental models into explicit knowledge by translating partial and implicit knowledge into an integrated knowledge model. The scenario-based test of the organized knowledge model enables decision-makers to understand the structure of the target problem and identify its basic cause, which facilitates effective decision-making. This method facilitates the linkage between knowledge management initiatives and achieving strategic goals and objectives of an organization.</span></p></div></div></div><div><div><p>... A series of foundation projects occurred to investigate improvements to the fundamental design that was emerging. Software technology components that would be needed to collect organizational information and the role of feedback loops in these systems <mark>(Hall, Paradice, &amp; Courtney, 2001;</mark>Hall, 2002) were emerging as research focus areas. Feedback is required to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...</p></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/David-Paradice-2295108499\"><span><span>David Paradice</span></span></a></li></ul></div><div><p><span>A review of Decision Support Systems (DSS) research shows technology and DSS evolve in a synchronized fashion. As new technological tools are introduced, researchers leverage the tools to expand the capabilities of DSS. However, advances in DSS are often piecemeal, lacking synergies that could come from adopting a grand challenge. The future will exhibit a similar pattern of technological advances, with analytics and artificial intelligence being two technologies that can be expected to impact DSS design. Analytics and artificial intelligence are broad technologies that have the potential to make significant impacts on decision support. For DSS to have a meaningful impact on decision-making processes, DSS must get “smarter.” DSS can get smarter by having greater understanding the contexts in which they operate. Two grand challenges are proposed: expanding the model of context implicit in all DSS and implementing a model of shared context understanding for networks of DSS. Each grand challenge provides opportunities for DSS researchers in many specialty areas to contribute, while also moving the discipline forward in a significant way.</span></p></div></div></div><div><div><p>... Such mechanisms of change are often enabled by loops (Van de Ven &amp; Poole, 1995), which are circular processes of communication, behaviour, or decision making that provide information to the system about what is working and what needs changing. Integral to the feedback course is the constant scanning by actors, like a leader, of the internal and external environment and existing knowledge <mark>(Hall et al., 2001)</mark>. Internal environments refer to actors within the system who continually look for opportunities to create new knowledge or to remove irrelevant information from the knowledge base. ...</p></div><div><p><span>The theoretical framework for this study draws on conceptual advances from two bodies of scholarship: 1) complexity thinking in education, which has recently focused on school system change and, 2) school leadership research, which has recently attended to the effects of leadership interventions to school improvement. Using a complexity-thinking framework, the purpose of this study was to understand how leadership practices contribute to shaping change in school systems and how change occurred across the system. Our study was conducted in an urban centre in Alberta within a public-school jurisdiction and in an area of the city that had a high population of students from culturally and linguistically diverse backgrounds from low-income households compared to other areas across the school jurisdiction. Students in this area typically scored in the lowest quartile on provincial standardized examinations. Our findings are significant because complexity thinking in the context of school leadership has not received sufficient empirical attention. In our study we identified and described pedagogical leadership practices that play a central role in redressing disparities currently found in schools.</span></p></div></div><div><p>... Researchers conducted a series of foundation projects to investigate improvements to the fundamental design that had begun to emerge. Software technology components that systems would need to collect organizational information ) and the role of feedback loops in these systems <mark>(Hall, Paradice, &amp; Courtney, 2001;</mark>Hall, 2002) emerged as research focus areas. A DSS requires feedback to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...</p></div><div><p>... In the face of no information inconsistencies to eliminate, the Singerian system challenges existing knowledge and works to refine the measures on which that knowledge is based (Churchman 1971, Courtney 2001, Courtney, Chae, and Hall 2000, Courtney, Croasdell, and Paradice 1998<mark>, Hall, Paradice, and Courtney 2001</mark>). The Singerian system strives, through the challenge process, to increase the number of alternative perspectives considered by a decision maker. ...</p></div><div><div><p>... KM processes consist of knowledge creation, acquisition, refinement, storage, transfer, sharing, and utilization within an organization (King et al., 2008). Moreover, there also exists feedback loops and limited understanding between different processes of KM <mark>(Hall et al., 2001)</mark>. In contrast, knowledge-based organizations are not categorized as tightly coupled systems based on the fact that their elements are often tied together frequently and loosely (Weick, 1976). ...</p></div><div><div><ul><li><a href=\"https://www.researchgate.net/profile/Roozbeh-Hesamamiri\"><span><span></span><span>Roozbeh Hesamamiri</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Mohammad-Mahdavi-Mazdeh-70890657\"><span><span>Mohammad Mahdavi Mazdeh</span></span></a></li></ul></div><div><p><span>Purpose - Although the topic of knowledge management (KM) failure has emerged over the past several years, no specific theory has been proposed about the ability of an organization to discover and manage unexpected failures in the organizational capabilities of KM. Thus, the main purpose of this study is to develop a theory of KM reliability by taking into account the availability of existing theory of high reliabil",
        "html": "<div><section><div><p>Information technology both facilitates and exacerbates the need for organizations to manage growing information and knowledge bases. Soon, the only successful enterprises may be those who have successfully evolved into learning organizations. The philosophical bases underlying traditional decision support systems (DSSs) are ill-equipped either to handle this explosion of information in today's rapidly changing business environment, or to support a learning organization. A comprehensive knowledge management system (KMS) based on the philosophies of inquiring systems (ISs) allows organizational learning to occur by providing both decision support and management of existing and created knowledge. By combining the DSS paradigm with an IS, an organization is capable of designing a comprehensive KMS that fully supports learning within the organization. The development of such a system allows executives to rely more fully on the support system, freeing up valuable time to establish goals and to guide the organization to its ultimate success.</p></div><div><div></div><p>To read the full-text of this research,<br/> you can request a copy directly from the authors.</p></div><div><div><div><div><div><p>... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives. <mark>Hall, Paradice, and Courtney (2001)</mark> argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...</p></div><div><p>... Focus on application of knowledge in decision making, rather than decision making method As reviewed in Section 2.3, a decision-making method is generally defined. And application of knowledge in organization and decision-making (Alavi &amp; Leidner, 2001;Courtney, 2001;<mark>Hall et al., 2001;</mark>Ozbayrak &amp; Bell, 2003) is an extensive research issue, and highly relevant in the field. ...</p></div></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/Nam-Hong-Yim-31693360\"><span><span>Nam-Hong Yim</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Soung-Hie-Kim-72293646\"><span><span>Soung-Hie Kim</span></span></a></li><li><a href=\"https://www.researchgate.net/profile/Hee-Woong-Kim\"><span><span></span><span>Hee-Woong Kim</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Kee-Young-Kwahk-18684629\"><span><span>Kee-Young Kwahk</span></span></a></li></ul></div><div><p><span>In recognizing knowledge as a new resource in gaining organizational competitiveness, knowledge management suggests a method in managing and applying knowledge for improving organizational performance. Much knowledge management research has focused on identifying, storing, and disseminating process related knowledge in an organized manner. Applying knowledge to decision making has a significant impact on organizational performance than solely processing transactions for knowledge management. In this research, we suggest a method of knowledge-based decision-making using system dynamics, with an emphasis to strategic concerns. The proposed method transforms individual mental models into explicit knowledge by translating partial and implicit knowledge into an integrated knowledge model. The scenario-based test of the organized knowledge model enables decision-makers to understand the structure of the target problem and identify its basic cause, which facilitates effective decision-making. This method facilitates the linkage between knowledge management initiatives and achieving strategic goals and objectives of an organization.</span></p></div></div></div><div><div><p>... A series of foundation projects occurred to investigate improvements to the fundamental design that was emerging. Software technology components that would be needed to collect organizational information and the role of feedback loops in these systems <mark>(Hall, Paradice, &amp; Courtney, 2001;</mark>Hall, 2002) were emerging as research focus areas. Feedback is required to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...</p></div><div><div><ul><li><a href=\"https://www.researchgate.net/scientific-contributions/David-Paradice-2295108499\"><span><span>David Paradice</span></span></a></li></ul></div><div><p><span>A review of Decision Support Systems (DSS) research shows technology and DSS evolve in a synchronized fashion. As new technological tools are introduced, researchers leverage the tools to expand the capabilities of DSS. However, advances in DSS are often piecemeal, lacking synergies that could come from adopting a grand challenge. The future will exhibit a similar pattern of technological advances, with analytics and artificial intelligence being two technologies that can be expected to impact DSS design. Analytics and artificial intelligence are broad technologies that have the potential to make significant impacts on decision support. For DSS to have a meaningful impact on decision-making processes, DSS must get “smarter.” DSS can get smarter by having greater understanding the contexts in which they operate. Two grand challenges are proposed: expanding the model of context implicit in all DSS and implementing a model of shared context understanding for networks of DSS. Each grand challenge provides opportunities for DSS researchers in many specialty areas to contribute, while also moving the discipline forward in a significant way.</span></p></div></div></div><div><div><p>... Such mechanisms of change are often enabled by loops (Van de Ven &amp; Poole, 1995), which are circular processes of communication, behaviour, or decision making that provide information to the system about what is working and what needs changing. Integral to the feedback course is the constant scanning by actors, like a leader, of the internal and external environment and existing knowledge <mark>(Hall et al., 2001)</mark>. Internal environments refer to actors within the system who continually look for opportunities to create new knowledge or to remove irrelevant information from the knowledge base. ...</p></div><div><p><span>The theoretical framework for this study draws on conceptual advances from two bodies of scholarship: 1) complexity thinking in education, which has recently focused on school system change and, 2) school leadership research, which has recently attended to the effects of leadership interventions to school improvement. Using a complexity-thinking framework, the purpose of this study was to understand how leadership practices contribute to shaping change in school systems and how change occurred across the system. Our study was conducted in an urban centre in Alberta within a public-school jurisdiction and in an area of the city that had a high population of students from culturally and linguistically diverse backgrounds from low-income households compared to other areas across the school jurisdiction. Students in this area typically scored in the lowest quartile on provincial standardized examinations. Our findings are significant because complexity thinking in the context of school leadership has not received sufficient empirical attention. In our study we identified and described pedagogical leadership practices that play a central role in redressing disparities currently found in schools.</span></p></div></div><div><p>... Researchers conducted a series of foundation projects to investigate improvements to the fundamental design that had begun to emerge. Software technology components that systems would need to collect organizational information ) and the role of feedback loops in these systems <mark>(Hall, Paradice, &amp; Courtney, 2001;</mark>Hall, 2002) emerged as research focus areas. A DSS requires feedback to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...</p></div><div><p>... In the face of no information inconsistencies to eliminate, the Singerian system challenges existing knowledge and works to refine the measures on which that knowledge is based (Churchman 1971, Courtney 2001, Courtney, Chae, and Hall 2000, Courtney, Croasdell, and Paradice 1998<mark>, Hall, Paradice, and Courtney 2001</mark>). The Singerian system strives, through the challenge process, to increase the number of alternative perspectives considered by a decision maker. ...</p></div><div><div><p>... KM processes consist of knowledge creation, acquisition, refinement, storage, transfer, sharing, and utilization within an organization (King et al., 2008). Moreover, there also exists feedback loops and limited understanding between different processes of KM <mark>(Hall et al., 2001)</mark>. In contrast, knowledge-based organizations are not categorized as tightly coupled systems based on the fact that their elements are often tied together frequently and loosely (Weick, 1976). ...</p></div><div><div><ul><li><a href=\"https://www.researchgate.net/profile/Roozbeh-Hesamamiri\"><span><span></span><span>Roozbeh Hesamamiri</span></span></a></li><li><a href=\"https://www.researchgate.net/scientific-contributions/Mohammad-Mahdavi-Mazdeh-70890657\"><span><span>Mohammad Mahdavi Mazdeh</span></span></a></li></ul></div><div><p><span>Purpose - Although the topic of knowledge management (KM) failure has emerged over the past several years, no specific theory has been proposed about the ability of an organization to discover and manage unexpected failures in the organizational capabilities of KM. Thus, the main purpose of this study is to develop a theory of KM reliability by taking into account the availability of existing theory of high reliabil",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Information technology both facilitates and exacerbates the need for organizations to manage growing information and knowledge bases. Soon, the only successful enterprises may be those who have successfully evolved into learning organizations. The philosophical bases underlying traditional decision support systems (DSSs) are ill-equipped either to handle this explosion of information in today's rapidly changing business environment, or to support a learning organization. A comprehensive knowledg",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Information technology both facilitates and exacerbates the need for organizations to manage growing information and knowledge bases. Soon, the only successful enterprises may be those who have successfully evolved into learning organizations. The philosophical bases underlying traditional decision support systems (DSSs) are ill-equipped either to handle this explosion of information in today's rapidly changing business environment, or to support a learning organization. A comprehensive knowledg",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Information technology both facilitates and exacerbates the need for organizations to manage growing information and knowledge bases. Soon, the only successful enterprises may be those who have successfully evolved into learning organizations. The philosophical bases underlying traditional decision support systems (DSSs) are ill-equipped either to handle this explosion of information in today's rapidly changing business environment, or to support a learning organization. A comprehensive knowledg",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "To read the full-text of this research,you can request a copy directly from the authors.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives.Hall, Paradice, and Courtney (2001)argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...... Focus on application of knowledge in dec",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives.Hall, Paradice, and Courtney (2001)argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...... Focus on application of knowledge in dec",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives.Hall, Paradice, and Courtney (2001)argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...... Focus on application of knowledge in dec",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives.Hall, Paradice, and Courtney (2001)argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...... Focus on application of knowledge in dec",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... When individuals or groups make decisions without full consideration of organizational parameters, the decisions made may be sub-optimal from the organizational perspectives.Hall, Paradice, and Courtney (2001)argue that DSS is on the verge of a paradigm shift. This shift is needed to adequately support organizational learning (Stein, 1995) and KM and organization learning and KM are activities executed to take organizations toward desired goal. ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Focus on application of knowledge in decision making, rather than decision making method As reviewed in Section 2.3, a decision-making method is generally defined. And application of knowledge in organization and decision-making (Alavi & Leidner, 2001;Courtney, 2001;Hall et al., 2001;Ozbayrak & Bell, 2003) is an extensive research issue, and highly relevant in the field. ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Nam-Hong YimSoung-Hie KimHee-Woong KimKee-Young KwahkIn recognizing knowledge as a new resource in gaining organizational competitiveness, knowledge management suggests a method in managing and applying knowledge for improving organizational performance. Much knowledge management research has focused on identifying, storing, and disseminating process related knowledge in an organized manner. Applying knowledge to decision making has a significant impact on organizational performance than solely ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Nam-Hong YimSoung-Hie KimHee-Woong KimKee-Young Kwahk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "In recognizing knowledge as a new resource in gaining organizational competitiveness, knowledge management suggests a method in managing and applying knowledge for improving organizational performance. Much knowledge management research has focused on identifying, storing, and disseminating process related knowledge in an organized manner. Applying knowledge to decision making has a significant impact on organizational performance than solely processing transactions for knowledge management. In ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... A series of foundation projects occurred to investigate improvements to the fundamental design that was emerging. Software technology components that would be needed to collect organizational information and the role of feedback loops in these systems(Hall, Paradice, & Courtney, 2001;Hall, 2002) were emerging as research focus areas. Feedback is required to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...Davi",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... A series of foundation projects occurred to investigate improvements to the fundamental design that was emerging. Software technology components that would be needed to collect organizational information and the role of feedback loops in these systems(Hall, Paradice, & Courtney, 2001;Hall, 2002) were emerging as research focus areas. Feedback is required to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to decision makers. ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "David ParadiceA review of Decision Support Systems (DSS) research shows technology and DSS evolve in a synchronized fashion. As new technological tools are introduced, researchers leverage the tools to expand the capabilities of DSS. However, advances in DSS are often piecemeal, lacking synergies that could come from adopting a grand challenge. The future will exhibit a similar pattern of technological advances, with analytics and artificial intelligence being two technologies that can be expect",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "David Paradice",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "A review of Decision Support Systems (DSS) research shows technology and DSS evolve in a synchronized fashion. As new technological tools are introduced, researchers leverage the tools to expand the capabilities of DSS. However, advances in DSS are often piecemeal, lacking synergies that could come from adopting a grand challenge. The future will exhibit a similar pattern of technological advances, with analytics and artificial intelligence being two technologies that can be expected to impact D",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Such mechanisms of change are often enabled by loops (Van de Ven & Poole, 1995), which are circular processes of communication, behaviour, or decision making that provide information to the system about what is working and what needs changing. Integral to the feedback course is the constant scanning by actors, like a leader, of the internal and external environment and existing knowledge(Hall et al., 2001). Internal environments refer to actors within the system who continually look for oppo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Such mechanisms of change are often enabled by loops (Van de Ven & Poole, 1995), which are circular processes of communication, behaviour, or decision making that provide information to the system about what is working and what needs changing. Integral to the feedback course is the constant scanning by actors, like a leader, of the internal and external environment and existing knowledge(Hall et al., 2001). Internal environments refer to actors within the system who continually look for oppo",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The theoretical framework for this study draws on conceptual advances from two bodies of scholarship: 1) complexity thinking in education, which has recently focused on school system change and, 2) school leadership research, which has recently attended to the effects of leadership interventions to school improvement. Using a complexity-thinking framework, the purpose of this study was to understand how leadership practices contribute to shaping change in school systems and how change occurred a",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... Researchers conducted a series of foundation projects to investigate improvements to the fundamental design that had begun to emerge. Software technology components that systems would need to collect organizational information ) and the role of feedback loops in these systems(Hall, Paradice, & Courtney, 2001;Hall, 2002) emerged as research focus areas. A DSS requires feedback to update the knowledge base in a dynamic business environment so that the DSS can provide accurate information to de",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... In the face of no information inconsistencies to eliminate, the Singerian system challenges existing knowledge and works to refine the measures on which that knowledge is based (Churchman 1971, Courtney 2001, Courtney, Chae, and Hall 2000, Courtney, Croasdell, and Paradice 1998, Hall, Paradice, and Courtney 2001). The Singerian system strives, through the challenge process, to increase the number of alternative perspectives considered by a decision maker. ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... KM processes consist of knowledge creation, acquisition, refinement, storage, transfer, sharing, and utilization within an organization (King et al., 2008). Moreover, there also exists feedback loops and limited understanding between different processes of KM(Hall et al., 2001). In contrast, knowledge-based organizations are not categorized as tightly coupled systems based on the fact that their elements are often tied together frequently and loosely (Weick, 1976). ...Roozbeh HesamamiriMoham",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "... KM processes consist of knowledge creation, acquisition, refinement, storage, transfer, sharing, and utilization within an organization (King et al., 2008). Moreover, there also exists feedback loops and limited understanding between different processes of KM(Hall et al., 2001). In contrast, knowledge-based organizations are not categorized as tightly coupled systems based on the fact that their elements are often tied together frequently and loosely (Weick, 1976). ...",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Roozbeh HesamamiriMohammad Mahdavi MazdehPurpose - Although the topic of knowledge management (KM) failure has emerged over the past several years, no specific theory has been proposed about the ability of an organization to discover and manage unexpected failures in the organizational capabilities of KM. Thus, the main purpose of this study is to develop a theory of KM reliability by taking into account the availability of existing theory of high reliabil",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Roozbeh HesamamiriMohammad Mahdavi Mazdeh",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Purpose - Although the topic of knowledge management (KM) failure has emerged over the past several years, no specific theory has been proposed about the ability of an organization to discover and manage unexpected failures in the organizational capabilities of KM. Thus, the main purpose of this study is to develop a theory of KM reliability by taking into account the availability of existing theory of high reliabil",
              "class": [],
              "id": ""
            }
          ],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://dl.acm.org/doi/fullHtml/10.1145/3184558.3191616",
      "title": "Towards a Benchmark for Fact Checking with Knowledge Bases",
      "author": "Viet-Phi Huynh EURECOM, France Paolo Papotti EURECOM, France",
      "published_date": "2018-01-01T00:00:00.000Z",
      "content": {
        "text": "<div>\n<section>\n<header>\n</header>\n<p> <small>Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and assumptions, as well as algorithms that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of the different methods.</small> </p>\n<div>\n<p> <small> <span>ACM Reference Format:</span> <br/> Viet-Phi Huynh and Paolo Papotti. 2018. Towards a Benchmark for Fact Checking with Knowledge Bases. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href=\"https://doi.org/10.1145/3184558.3191616\">https://doi.org/10.1145/3184558.3191616</a></small> </p>\n</div>\n</section>\n<section>\n<section>\n<header>\n<p>\n</p><h2> <span><span>1</span></span> Introduction</h2>\n<p></p>\n</header>\n<p>Fact checking refers to the task of verification of textual content. Given the increase of incorrect claims over the Web, fact checking is no longer an activity for journalists only. To tackle the spread of misleading information, Web companies have introduced forms of fact checking in their services. In these approaches, information on the trust of the news is gathered from websites (e.g., politifact.com, factcheck.org, and snopes.com), where journalists manually assess the quality of the information.</p>\n<p>However, the proliferation of websites and bots spreading false information has motivated an effort for computational fact checking [<a href=\"#BibPLXBIB0004\">4</a>], which aims at automatic verification to scale over thousands of daily facts. Several algorithms focus on different types of facts and different domains. We are interested here in techniques that focus on validating “worth checking” facts against trustful Knowledge Bases (KBs) [<a href=\"#BibPLXBIB0003\">3</a>, <a href=\"#BibPLXBIB0010\">10</a>]. In the following, we assume the facts and the entities involved have been identified [<a href=\"#BibPLXBIB0008\">8</a>], and focus our attention on the step estimating the <em>veracity</em> of a given fact (expressed as structured data) w.r.t. reference data considered trusted.</p>\n<p>The core problem for fact checking with KBs is that we cannot assume the reference information to be complete (Open World Assumption), i.e., we cannot say if a fact not in the KB is false or just missing [<a href=\"#BibPLXBIB0005\">5</a>]. Given a KB <em>K</em> and a fact <em>f</em>, the fact checking algorithms should therefore state if <em>f</em> is a valid missing fact in <em>K</em>.</p>\n<p>There are many factors that affect the outcome of the fact checking step. The quality of the reference KB plays a pivotal role, and it must be selected according to the specific domain. Once the KB has been fixed, there is the challenge of picking the right algorithm to validate the facts. There are several proposals available, that differ significantly in their principles and assumptions. Also, given the novelty of the field and the complexity of the problem, there are still no established and unified sets of metrics and datasets.</p>\n<p>In this work, we lay the foundation for a benchmark to compare and contrast fact checking algorithms that rely on external information in the form of RDF KBs. Our ultimate goal is to create a benchmark with a large variety of annotated datasets, tools to create synthetic datasets with different properties, and metrics to evaluate different algorithms on a level playing field.</p>\n<p>Our contributions in this work are the following.</p>\n<p>1. We classify most of the existing fact checking algorithms by their methodology and discuss their main properties (Section <a href=\"#sec-3\">2</a>).</p>\n<p>2. We craft the datasets and the metrics for a fair evaluation of the methods in an early version of our benchmark (Section <a href=\"#sec-13\">3</a>).</p>\n<p>3. We demonstrate the use of the benchmark<a href=\"#fn1\"><sup>1</sup></a> with an experimental analysis of representative algorithms. (Section <a href=\"#sec-14\">4</a>).</p>\n</section>\n<section>\n<header>\n<p>\n</p><h2> <span><span>2</span></span> Algorithm Classification</h2>\n<p></p>\n</header>\n<p>We first give some background and fix the terminology. We then classify different fact checking algorithms according the methods they use to solve the problem.</p>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.1</span></span> Background</h3>\n<p></p>\n</header>\n<p>A <em>fact</em> is defined as a triple that has the form of (”subject” <em>s</em>, ”predicate” <em>p</em>, ”object” <em>o</em>). Natural language processing (NLP) techniques are used to convert a claim in natural language into a structured format. Facts can be classified into categories, such as numerical, quote, and object property. We focus on object properties, which are facts stating a relationship between the subject and the object in a sentence, e.g., Sacramento is the capital of California.</p>\n<p>A <em>Knowledge Base</em> (KB) is a direct graph where nodes correspond to entities (subject or object in a fact) and edges correspond to binary predicates among entities. We focus on algorithms that take as input a KB and a fact that is not part of it. Such algorithms assess if the given fact belongs to the missing part of the KB (therefore is “true”) or no (is “false”).</p>\n<p>Methods assume that training examples (labelled facts) are available to build the models. We will detail how we craft our datasets in terms of training data, but, in general, the common assumption is that the KB is trustable, and true facts are extracted from it.</p>\n</section>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.2</span></span> Path Based Algorithms</h3>\n<p></p>\n</header>\n<p>Given a fact (<em>s</em>, <em>p</em>, <em>o</em>), this group of algorithms makes a decision for it by exploiting the paths in the KB of existing <em>p</em> triples. These KB triples act as positive examples for the learning of the alternative paths (different from <em>p</em>) between their subjects and objects. Properties of the paths are then modeled as features in a classifier that decides if predicate <em>p</em> holds for the given <em>s</em> and <em>o</em>.</p>\n<section>\n<p><em>2.2.1 Knowledge Linker (KL).</em> This algorithm builds an internal model based on a weighted adjacency matrix with edge weights computed as the in-degree of each node in the KB, then transformed to similarity scores [<a href=\"#BibPLXBIB0003\">3</a>]. As the model ignores the labels of the predicates, it evaluates the validity of an input fact based on measuring the proximity between its subject and object. Two distance closures are introduced. With the <em>Metric closure</em>, every path connecting a given subject and object is mapped to a score computed on the generality of the nodes in the path, where the generality of a node is its frequency in the KB. The more often the node occurs in KB, the less information it conveys. With the <em>Ultra-metric closure</em>, only the node with highest generality is used to compute the score for each possible path. In both cases, the maximum score is equivalent to the shortest path between subject and object.</p>\n</section>\n<section>\n<p><em>2.2.2 Discriminate Predicate Path Mining (KG-Miner).</em> This algorithm identifies and exploits frequent anchored predicate paths between pair of entities in the KB [<a href=\"#BibPLXBIB0010\">10</a>]. Given an input fact (<em>s, p, o</em>), it collects as training data the predicate paths for node pairs that satisfy <em>p</em> in the KB and have subject with the same type of <em>s</em> (denoted ϕ(<em>s</em>)) and object with type of <em>o</em> (denoted ϕ(<em>o</em>)). From each subject <em>u</em> ∈ ϕ(<em>s</em>) and corresponding object <em>v</em> ∈ ϕ(<em>o</em>), predicate paths that alternatively represent predicate <em>p</em> are extracted from the KB with a depth-first search (DFS) traversing the graph from <em>u</em> to <em>v</em> up to a length of <em>m</em>. The information gain of these paths and corresponding labels is computed based on their number of occurrences. It then selects the most discriminative paths and plugs them into training for a logistic regression model that optimizes the Area Under Receiver Operating Characteristic (AUROC). This model is then used to compute the likelihood of an input fact.</p>\n</section>\n<section>\n<p><em>2.2.3 Path Ranking Algorithm (PRA).</em> PRA discovers the relationships among entities in a stochastic way by performing random walk inference over the KB [<a href=\"#BibPLXBIB0007\">7</a>]. For the feature extraction, from a given training set of triples, it uses <em>two-sided, unconstrained</em> random walk starting at the source and corresponding target nodes to retrieve paths connected between them. Top <em>k</em> paths for each training instance are kept based on their number of occurrences and are collected into a feature matrix. A value in the matrix corresponding to a training instance (<em>s, p, o</em>) is the probability of arriving at the target node <em>o</em> by a random walk starting at source node <em>s</em> and following a specific path among its top <em>k</em> paths. This probability is computed using the approximate method of rejection sampling to reduce the computational complexity. The feature matrix is then used with a classifier to validate the input fact.</p>\n</section>\n</section>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.3</span></span> Sub-Graph Based Algorithms</h3>\n<p></p>\n</header>\n<p>Given a fact, this method exten",
        "html": "<div>\n<section>\n<header>\n</header>\n<p> <small>Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and assumptions, as well as algorithms that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of the different methods.</small> </p>\n<div>\n<p> <small> <span>ACM Reference Format:</span> <br/> Viet-Phi Huynh and Paolo Papotti. 2018. Towards a Benchmark for Fact Checking with Knowledge Bases. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href=\"https://doi.org/10.1145/3184558.3191616\">https://doi.org/10.1145/3184558.3191616</a></small> </p>\n</div>\n</section>\n<section>\n<section>\n<header>\n<p>\n</p><h2> <span><span>1</span></span> Introduction</h2>\n<p></p>\n</header>\n<p>Fact checking refers to the task of verification of textual content. Given the increase of incorrect claims over the Web, fact checking is no longer an activity for journalists only. To tackle the spread of misleading information, Web companies have introduced forms of fact checking in their services. In these approaches, information on the trust of the news is gathered from websites (e.g., politifact.com, factcheck.org, and snopes.com), where journalists manually assess the quality of the information.</p>\n<p>However, the proliferation of websites and bots spreading false information has motivated an effort for computational fact checking [<a href=\"#BibPLXBIB0004\">4</a>], which aims at automatic verification to scale over thousands of daily facts. Several algorithms focus on different types of facts and different domains. We are interested here in techniques that focus on validating “worth checking” facts against trustful Knowledge Bases (KBs) [<a href=\"#BibPLXBIB0003\">3</a>, <a href=\"#BibPLXBIB0010\">10</a>]. In the following, we assume the facts and the entities involved have been identified [<a href=\"#BibPLXBIB0008\">8</a>], and focus our attention on the step estimating the <em>veracity</em> of a given fact (expressed as structured data) w.r.t. reference data considered trusted.</p>\n<p>The core problem for fact checking with KBs is that we cannot assume the reference information to be complete (Open World Assumption), i.e., we cannot say if a fact not in the KB is false or just missing [<a href=\"#BibPLXBIB0005\">5</a>]. Given a KB <em>K</em> and a fact <em>f</em>, the fact checking algorithms should therefore state if <em>f</em> is a valid missing fact in <em>K</em>.</p>\n<p>There are many factors that affect the outcome of the fact checking step. The quality of the reference KB plays a pivotal role, and it must be selected according to the specific domain. Once the KB has been fixed, there is the challenge of picking the right algorithm to validate the facts. There are several proposals available, that differ significantly in their principles and assumptions. Also, given the novelty of the field and the complexity of the problem, there are still no established and unified sets of metrics and datasets.</p>\n<p>In this work, we lay the foundation for a benchmark to compare and contrast fact checking algorithms that rely on external information in the form of RDF KBs. Our ultimate goal is to create a benchmark with a large variety of annotated datasets, tools to create synthetic datasets with different properties, and metrics to evaluate different algorithms on a level playing field.</p>\n<p>Our contributions in this work are the following.</p>\n<p>1. We classify most of the existing fact checking algorithms by their methodology and discuss their main properties (Section <a href=\"#sec-3\">2</a>).</p>\n<p>2. We craft the datasets and the metrics for a fair evaluation of the methods in an early version of our benchmark (Section <a href=\"#sec-13\">3</a>).</p>\n<p>3. We demonstrate the use of the benchmark<a href=\"#fn1\"><sup>1</sup></a> with an experimental analysis of representative algorithms. (Section <a href=\"#sec-14\">4</a>).</p>\n</section>\n<section>\n<header>\n<p>\n</p><h2> <span><span>2</span></span> Algorithm Classification</h2>\n<p></p>\n</header>\n<p>We first give some background and fix the terminology. We then classify different fact checking algorithms according the methods they use to solve the problem.</p>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.1</span></span> Background</h3>\n<p></p>\n</header>\n<p>A <em>fact</em> is defined as a triple that has the form of (”subject” <em>s</em>, ”predicate” <em>p</em>, ”object” <em>o</em>). Natural language processing (NLP) techniques are used to convert a claim in natural language into a structured format. Facts can be classified into categories, such as numerical, quote, and object property. We focus on object properties, which are facts stating a relationship between the subject and the object in a sentence, e.g., Sacramento is the capital of California.</p>\n<p>A <em>Knowledge Base</em> (KB) is a direct graph where nodes correspond to entities (subject or object in a fact) and edges correspond to binary predicates among entities. We focus on algorithms that take as input a KB and a fact that is not part of it. Such algorithms assess if the given fact belongs to the missing part of the KB (therefore is “true”) or no (is “false”).</p>\n<p>Methods assume that training examples (labelled facts) are available to build the models. We will detail how we craft our datasets in terms of training data, but, in general, the common assumption is that the KB is trustable, and true facts are extracted from it.</p>\n</section>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.2</span></span> Path Based Algorithms</h3>\n<p></p>\n</header>\n<p>Given a fact (<em>s</em>, <em>p</em>, <em>o</em>), this group of algorithms makes a decision for it by exploiting the paths in the KB of existing <em>p</em> triples. These KB triples act as positive examples for the learning of the alternative paths (different from <em>p</em>) between their subjects and objects. Properties of the paths are then modeled as features in a classifier that decides if predicate <em>p</em> holds for the given <em>s</em> and <em>o</em>.</p>\n<section>\n<p><em>2.2.1 Knowledge Linker (KL).</em> This algorithm builds an internal model based on a weighted adjacency matrix with edge weights computed as the in-degree of each node in the KB, then transformed to similarity scores [<a href=\"#BibPLXBIB0003\">3</a>]. As the model ignores the labels of the predicates, it evaluates the validity of an input fact based on measuring the proximity between its subject and object. Two distance closures are introduced. With the <em>Metric closure</em>, every path connecting a given subject and object is mapped to a score computed on the generality of the nodes in the path, where the generality of a node is its frequency in the KB. The more often the node occurs in KB, the less information it conveys. With the <em>Ultra-metric closure</em>, only the node with highest generality is used to compute the score for each possible path. In both cases, the maximum score is equivalent to the shortest path between subject and object.</p>\n</section>\n<section>\n<p><em>2.2.2 Discriminate Predicate Path Mining (KG-Miner).</em> This algorithm identifies and exploits frequent anchored predicate paths between pair of entities in the KB [<a href=\"#BibPLXBIB0010\">10</a>]. Given an input fact (<em>s, p, o</em>), it collects as training data the predicate paths for node pairs that satisfy <em>p</em> in the KB and have subject with the same type of <em>s</em> (denoted ϕ(<em>s</em>)) and object with type of <em>o</em> (denoted ϕ(<em>o</em>)). From each subject <em>u</em> ∈ ϕ(<em>s</em>) and corresponding object <em>v</em> ∈ ϕ(<em>o</em>), predicate paths that alternatively represent predicate <em>p</em> are extracted from the KB with a depth-first search (DFS) traversing the graph from <em>u</em> to <em>v</em> up to a length of <em>m</em>. The information gain of these paths and corresponding labels is computed based on their number of occurrences. It then selects the most discriminative paths and plugs them into training for a logistic regression model that optimizes the Area Under Receiver Operating Characteristic (AUROC). This model is then used to compute the likelihood of an input fact.</p>\n</section>\n<section>\n<p><em>2.2.3 Path Ranking Algorithm (PRA).</em> PRA discovers the relationships among entities in a stochastic way by performing random walk inference over the KB [<a href=\"#BibPLXBIB0007\">7</a>]. For the feature extraction, from a given training set of triples, it uses <em>two-sided, unconstrained</em> random walk starting at the source and corresponding target nodes to retrieve paths connected between them. Top <em>k</em> paths for each training instance are kept based on their number of occurrences and are collected into a feature matrix. A value in the matrix corresponding to a training instance (<em>s, p, o</em>) is the probability of arriving at the target node <em>o</em> by a random walk starting at source node <em>s</em> and following a specific path among its top <em>k</em> paths. This probability is computed using the approximate method of rejection sampling to reduce the computational complexity. The feature matrix is then used with a classifier to validate the input fact.</p>\n</section>\n</section>\n<section>\n<header>\n<p>\n</p><h3> <span><span>2.3</span></span> Sub-Graph Based Algorithms</h3>\n<p></p>\n</header>\n<p>Given a fact, this method exten",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "ACM Reference Format:Viet-Phi Huynh and Paolo Papotti. 2018. Towards a Benchmark for Fact Checking with Knowledge Bases. InWWW '18 Companion: The 2018 Web Conference Companion,April 23–27, 2018,Lyon, France. ACM, New York, NY, USA5 Pages.https://doi.org/10.1145/3184558.3191616",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "1IntroductionFact checking refers to the task of verification of textual content. Given the increase of incorrect claims over the Web, fact checking is no longer an activity for journalists only. To tackle the spread of misleading information, Web companies have introduced forms of fact checking in their services. In these approaches, information on the trust of the news is gathered from websites (e.g., politifact.com, factcheck.org, and snopes.com), where journalists manually assess the quality",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "1IntroductionFact checking refers to the task of verification of textual content. Given the increase of incorrect claims over the Web, fact checking is no longer an activity for journalists only. To tackle the spread of misleading information, Web companies have introduced forms of fact checking in their services. In these approaches, information on the trust of the news is gathered from websites (e.g., politifact.com, factcheck.org, and snopes.com), where journalists manually assess the quality",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2Algorithm ClassificationWe first give some background and fix the terminology. We then classify different fact checking algorithms according the methods they use to solve the problem.2.1BackgroundAfactis defined as a triple that has the form of (”subject”s, ”predicate”p, ”object”o). Natural language processing (NLP) techniques are used to convert a claim in natural language into a structured format. Facts can be classified into categories, such as numerical, quote, and object property. We focus",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.1BackgroundAfactis defined as a triple that has the form of (”subject”s, ”predicate”p, ”object”o). Natural language processing (NLP) techniques are used to convert a claim in natural language into a structured format. Facts can be classified into categories, such as numerical, quote, and object property. We focus on object properties, which are facts stating a relationship between the subject and the object in a sentence, e.g., Sacramento is the capital of California.AKnowledge Base(KB) is a d",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.2Path Based AlgorithmsGiven a fact (s,p,o), this group of algorithms makes a decision for it by exploiting the paths in the KB of existingptriples. These KB triples act as positive examples for the learning of the alternative paths (different fromp) between their subjects and objects. Properties of the paths are then modeled as features in a classifier that decides if predicatepholds for the givensando.2.2.1 Knowledge Linker (KL).This algorithm builds an internal model based on a weighted adja",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.2.1 Knowledge Linker (KL).This algorithm builds an internal model based on a weighted adjacency matrix with edge weights computed as the in-degree of each node in the KB, then transformed to similarity scores [3]. As the model ignores the labels of the predicates, it evaluates the validity of an input fact based on measuring the proximity between its subject and object. Two distance closures are introduced. With theMetric closure, every path connecting a given subject and object is mapped to a",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.2.2 Discriminate Predicate Path Mining (KG-Miner).This algorithm identifies and exploits frequent anchored predicate paths between pair of entities in the KB [10]. Given an input fact (s, p, o), it collects as training data the predicate paths for node pairs that satisfypin the KB and have subject with the same type ofs(denoted ϕ(s)) and object with type ofo(denoted ϕ(o)). From each subjectu∈ ϕ(s) and corresponding objectv∈ ϕ(o), predicate paths that alternatively represent predicatepare extra",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.2.3 Path Ranking Algorithm (PRA).PRA discovers the relationships among entities in a stochastic way by performing random walk inference over the KB [7]. For the feature extraction, from a given training set of triples, it usestwo-sided, unconstrainedrandom walk starting at the source and corresponding target nodes to retrieve paths connected between them. Topkpaths for each training instance are kept based on their number of occurrences and are collected into a feature matrix. A value in the m",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "2.3Sub-Graph Based AlgorithmsGiven a fact, this method exten",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "1Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "2Algorithm Classification",
              "id": ""
            },
            {
              "level": "h3",
              "text": "2.1Background",
              "id": ""
            },
            {
              "level": "h3",
              "text": "2.2Path Based Algorithms",
              "id": ""
            },
            {
              "level": "h3",
              "text": "2.3Sub-Graph Based Algorithms",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.nature.com/articles/s41599-024-04322-5",
      "title": "Exploring the impact of self-regulation on vocabulary learning strategies and knowledge in CSL: A structural equation modeling approach",
      "author": "Dai, Yanmei",
      "published_date": "2025-01-02T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<div><h2>Introduction</h2><div><p>Vocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR11\">2023</a>; Ghalebi et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR20\">2020</a>, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR21\">2021</a>; Leontjev, et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR36\">2023</a>; Li and Hafner <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR38\">2022</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>; Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR62\">2014</a>; Zhang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR85\">2017</a>). VK can be further divided into receptive knowledge (recognition and understanding of words) and productive knowledge (the ability to use words accurately in communication). This study examines both dimensions of VK, as they are key indicators of language proficiency and essential for effective communication in L2 contexts.</p><p>Despite substantial research on the effectiveness of VLSs in vocabulary learning, the complex interactions between VLSs, VK, and self-regulation (SR) remain underexplored in CSL learning contexts (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR16\">2001</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Zhao and Ji <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR86\">2018</a>). SR, which encompasses cognitive, motivational, and behavioral aspects, enables learners to actively set learning goals and regulate their learning behaviors, ultimately leading to improve academic performance (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR17\">2005</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Tseng and Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR75\">2008</a>; Wang and Bai <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR78\">2017</a>).</p><p>In L2 learning contexts, particularly CSL, understanding the role of SR is crucial, as it directly influences the application of VLSs and predicts the effectiveness of vocabulary learning (Law et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR34\">2008</a>; Nietfeld et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR47\">2014</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Tseng and Schmitt, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR75\">2008</a>; Zhu and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR87\">2022</a>). SR also equips learners to manage critical elements such as time, motivation, and self-awareness, all of which are vital for academic success (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Broadbent and Poon <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR5\">2015</a>; Hwang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR26\">2021</a>; Panadero et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR49\">2017</a>; Pintrich <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR51\">2000</a>; Theobald <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR71\">2021</a>; Zimmerman <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR89\">2000</a>). For younger L2 learners, VLSs are often employed as part of self-regulatory behaviors, particularly in time management (Choi <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR12\">2016</a>; Wolters et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR80\">2017</a>). The effective time management can contribute to higher academic achievement (Archambault et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR3\">2022</a>; Broadbent and Poon <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR5\">2015</a>; Crede and Phillips <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR14\">2011</a>; Hwang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR26\">2021</a>; Kitsantas et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR29\">2008</a>).</p><p>Previous research has explored the relationships between VLSs and VK in L2 vocabulary learning, as well as the correlations among SR, VLSs, and VK (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR16\">2001</a>; Ghalebi et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR20\">2020</a>, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR21\">2021</a>; Law et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR34\">2008</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>; Read <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR52\">1988</a>; Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR62\">2014</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Teng and Zhang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR65\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Zhao and Ji <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR86\">2018</a>). However, despite the importance of SR in vocabulary learning, few studies have thoroughly examined how VLSs indirectly influence L2 learners’ VK through SR, particularly in CSL contexts. While prior research has investigated the relationship between the depth and breadth of vocabulary learning (Laufer and Goldstein <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR33\">2004</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>), it has not adequately addressed the impact of other variables on these dimensions. A more in-depth investigation is required to understand the nuanced interactions between VLSs, SR, and VK, especially when additional factors that influence VK dimensions are considered. Several studies have utilized scales to measure L2 learners’ VK (Calafato and Clausen <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR9\">2024</a>; Laufer and Goldstein <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR33\">2004</a>; Laufer and Paribakht <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR32\">1998</a>; Paribakht and Wesche <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR50\">1993</a>), but the role of VLSs in these findings has not been adequately explored. Similarly, while research has examined the correlation between SR, vocabulary depth and breadth separately (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Rød and Calafato <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR55\">2023</a>; Teng and Zhang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR65\">2022</a>; Wang and Bai <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR78\">2017</a>), it has not thoroughly analyzed the source of these relationships or their implications for VLS use. In addition, existing studies have utilized methods such as analysis of variance (ANOVA) to investigate factors affecting L2 vocabulary learning (Gu <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR22\">2002</a>; Nietfeld et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR47\">2014</a>). However, the influence of individual academic effort, such as academic achievement and time management, on the combined effect of VLSs, SR, and VK among CSL learners remains underexplored. A deeper understanding of how these academic factors shape the interplay between VLSs, SR, and VK could yield valuable insights for improving vocabulary learning in the context of CSL.</p><p>The present study aims to develop and test a research model grounded in critical theoretical frameworks: Schmitt’s (<a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR60\">1997</a>) model of Vocabulary Learning Strategies (VLSs) and Tseng et al.’s (<a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>) concept of Self-Regulating Capacity in Vocabulary Learning (SRCvoc). VLSs categorizes VLSs into six types, providing a framework to examine how learners employ strategies to acquire vocabulary. SRCvoc, on the other hand, highlights the role of self-regulation in L2 vocabulary learning. The research model guided the literature review, helping to identify core concepts, relationships between variables, and gaps in the existing literature. Drawing on prior studies that developed comprehensive questionnaires based on the VLSs and SRCvoc frameworks, this research model sign",
        "html": "<div><div>\n<div><h2>Introduction</h2><div><p>Vocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR11\">2023</a>; Ghalebi et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR20\">2020</a>, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR21\">2021</a>; Leontjev, et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR36\">2023</a>; Li and Hafner <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR38\">2022</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>; Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR62\">2014</a>; Zhang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR85\">2017</a>). VK can be further divided into receptive knowledge (recognition and understanding of words) and productive knowledge (the ability to use words accurately in communication). This study examines both dimensions of VK, as they are key indicators of language proficiency and essential for effective communication in L2 contexts.</p><p>Despite substantial research on the effectiveness of VLSs in vocabulary learning, the complex interactions between VLSs, VK, and self-regulation (SR) remain underexplored in CSL learning contexts (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR16\">2001</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Zhao and Ji <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR86\">2018</a>). SR, which encompasses cognitive, motivational, and behavioral aspects, enables learners to actively set learning goals and regulate their learning behaviors, ultimately leading to improve academic performance (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR17\">2005</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Tseng and Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR75\">2008</a>; Wang and Bai <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR78\">2017</a>).</p><p>In L2 learning contexts, particularly CSL, understanding the role of SR is crucial, as it directly influences the application of VLSs and predicts the effectiveness of vocabulary learning (Law et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR34\">2008</a>; Nietfeld et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR47\">2014</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Tseng and Schmitt, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR75\">2008</a>; Zhu and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR87\">2022</a>). SR also equips learners to manage critical elements such as time, motivation, and self-awareness, all of which are vital for academic success (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Broadbent and Poon <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR5\">2015</a>; Hwang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR26\">2021</a>; Panadero et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR49\">2017</a>; Pintrich <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR51\">2000</a>; Theobald <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR71\">2021</a>; Zimmerman <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR89\">2000</a>). For younger L2 learners, VLSs are often employed as part of self-regulatory behaviors, particularly in time management (Choi <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR12\">2016</a>; Wolters et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR80\">2017</a>). The effective time management can contribute to higher academic achievement (Archambault et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR3\">2022</a>; Broadbent and Poon <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR5\">2015</a>; Crede and Phillips <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR14\">2011</a>; Hwang et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR26\">2021</a>; Kitsantas et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR29\">2008</a>).</p><p>Previous research has explored the relationships between VLSs and VK in L2 vocabulary learning, as well as the correlations among SR, VLSs, and VK (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Dörnyei <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR16\">2001</a>; Ghalebi et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR20\">2020</a>, <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR21\">2021</a>; Law et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR34\">2008</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>; Read <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR52\">1988</a>; Schmitt <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR62\">2014</a>; Teng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR66\">2022</a>; Teng and Zhang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR65\">2022</a>; Tseng et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>; Zhao and Ji <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR86\">2018</a>). However, despite the importance of SR in vocabulary learning, few studies have thoroughly examined how VLSs indirectly influence L2 learners’ VK through SR, particularly in CSL contexts. While prior research has investigated the relationship between the depth and breadth of vocabulary learning (Laufer and Goldstein <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR33\">2004</a>; Nation <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR43\">2001</a>), it has not adequately addressed the impact of other variables on these dimensions. A more in-depth investigation is required to understand the nuanced interactions between VLSs, SR, and VK, especially when additional factors that influence VK dimensions are considered. Several studies have utilized scales to measure L2 learners’ VK (Calafato and Clausen <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR9\">2024</a>; Laufer and Goldstein <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR33\">2004</a>; Laufer and Paribakht <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR32\">1998</a>; Paribakht and Wesche <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR50\">1993</a>), but the role of VLSs in these findings has not been adequately explored. Similarly, while research has examined the correlation between SR, vocabulary depth and breadth separately (Bai and Wang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR4\">2023</a>; Rød and Calafato <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR55\">2023</a>; Teng and Zhang <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR65\">2022</a>; Wang and Bai <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR78\">2017</a>), it has not thoroughly analyzed the source of these relationships or their implications for VLS use. In addition, existing studies have utilized methods such as analysis of variance (ANOVA) to investigate factors affecting L2 vocabulary learning (Gu <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR22\">2002</a>; Nietfeld et al. <a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR47\">2014</a>). However, the influence of individual academic effort, such as academic achievement and time management, on the combined effect of VLSs, SR, and VK among CSL learners remains underexplored. A deeper understanding of how these academic factors shape the interplay between VLSs, SR, and VK could yield valuable insights for improving vocabulary learning in the context of CSL.</p><p>The present study aims to develop and test a research model grounded in critical theoretical frameworks: Schmitt’s (<a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR60\">1997</a>) model of Vocabulary Learning Strategies (VLSs) and Tseng et al.’s (<a href=\"https://www.nature.com/articles/s41599-024-04322-5#ref-CR74\">2006</a>) concept of Self-Regulating Capacity in Vocabulary Learning (SRCvoc). VLSs categorizes VLSs into six types, providing a framework to examine how learners employ strategies to acquire vocabulary. SRCvoc, on the other hand, highlights the role of self-regulation in L2 vocabulary learning. The research model guided the literature review, helping to identify core concepts, relationships between variables, and gaps in the existing literature. Drawing on prior studies that developed comprehensive questionnaires based on the VLSs and SRCvoc frameworks, this research model sign",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "IntroductionVocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al.2023; Ghalebi et al.2020,2021; Leontjev, et al.2023; Li and Hafne",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "IntroductionVocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al.2023; Ghalebi et al.2020,2021; Leontjev, et al.2023; Li and Hafne",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "IntroductionVocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al.2023; Ghalebi et al.2020,2021; Leontjev, et al.2023; Li and Hafne",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Vocabulary learning strategies (VLSs) are essential in second language (L2) learning, particularly within Chinese as a second language (CSL) learning contexts. VLSs support vocabulary learning and enhance learners’ vocabulary knowledge (VK) in both breadth-the number of words known- and depth, which includes understanding of word meaning, pronunciations, grammatical forms, and appropriate contexts for use (Chiang et al.2023; Ghalebi et al.2020,2021; Leontjev, et al.2023; Li and Hafner2022; Natio",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.pnas.org/doi/10.1073/pnas.2104235118",
      "title": "The global effectiveness of fact-checking: Evidence from simultaneous experiments in Argentina, Nigeria, South Africa, and the United Kingdom | Proceedings of the National Academy of Sciences",
      "author": "Ethan Porter; Thomas J Wood; Porter; Ethan; Wood; Thomas J",
      "published_date": "2023-02-22T12:41:32.000Z",
      "content": {
        "text": "<div><article><header><div><div><p>Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)</p></div><div><p>September 10, 2021</p><p>118 (37) e2104235118</p></div></div></header><div><section><h2>Significance</h2><p>Little evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in all countries, with most effects detectable more than 2 wk later and with surprisingly little variation by country. Our evidence underscores that fact-checking can serve as a pivotal tool in the fight against misinformation.</p></section><section><h2>Abstract</h2><p>The spread of misinformation is a global phenomenon, with implications for elections, state-sanctioned violence, and health outcomes. Yet, even though scholars have investigated the capacity of fact-checking to reduce belief in misinformation, little evidence exists on the global effectiveness of this approach. We describe fact-checking experiments conducted simultaneously in Argentina, Nigeria, South Africa, and the United Kingdom, in which we studied whether fact-checking can durably reduce belief in misinformation. In total, we evaluated 22 fact-checks, including two that were tested in all four countries. Fact-checking reduced belief in misinformation, with most effects still apparent more than 2 wk later. A meta-analytic procedure indicates that fact-checks reduced belief in misinformation by at least 0.59 points on a 5-point scale. Exposure to misinformation, however, only increased false beliefs by less than 0.07 points on the same scale. Across continents, fact-checks reduce belief in misinformation, often durably so.</p></section>\n<div>\n<h3>Sign up for PNAS alerts.</h3>\n<p>Get alerts for new articles, or get an alert when an article is cited.</p>\n</div>\n</div><div><p>The spread of misinformation is a global phenomenon (<a href=\"#core-r1\">1</a>). Misinformation is said to have played a role in the Myanmar genocide (<a href=\"#core-r2\">2</a>), national elections (<a href=\"#core-r3\">3</a>), and the resurgence of measles (<a href=\"#core-r4\">4</a>). Scholars have investigated various means of reducing belief in misinformation, including, but not limited to, fact-checking (<a href=\"#core-r5\">5</a>–<a href=\"#core-r8\">8</a>). Yet, despite the global scope of the challenge, much of the available evidence about decreasing false beliefs comes from single-country samples gathered in North America, Europe, or Australia. The available evidence also pays scant attention to the durability of accuracy increases that fact-checking may generate. Prior research has shown that fact-checking can reduce false beliefs in single countries (<a href=\"#core-r9\">9</a>, <a href=\"#core-r10\">10</a>). Yet, whether fact-checking can reduce belief in misinformation around the world and whether any such reductions endure are unknown.</p><p>We describe simultaneous experiments conducted in four countries that help resolve both questions. In partnership with fact-checking organizations, we administered experiments in September and October 2020 in Argentina, Nigeria, South Africa, and the United Kingdom. The four countries are diverse along racial, economic, and political lines, but are unified by the presence of fact-checking organizations that have signed on to the standards of the International Fact-Checking Network. The experiments evaluated the effects of fact-checks on beliefs about both country-specific and global misinformation.</p><p>In total, we conducted 28 experiments, evaluating 22 distinct fact-checks. To limit the extent to which differences in timing may have been responsible for differential effects, particularly on the global misinformation items, we fielded all experiments in each country at the same time. In each experiment, participants were randomly assigned to misinformation; misinformation followed by a fact-check; or control. All participants then immediately answered outcome questions about their belief in the false claim advanced by the misinformation. Fact-checking stimuli consisted of fact-checks produced by fact-checking organizations in each country, while misinformation stimuli consisted of brief summaries of the false claims that led to the corresponding fact-checks. This allowed us to estimate misinformation effects (the effect of misinformation on belief accuracy compared to control) and correction effects (the effect of corrections on belief accuracy compared to misinformation).</p><p>The fact-checks targeted a broad swath of misinformation topics, including COVID-19, local politics, crime, and the economy. In Argentina, South Africa, and the United Kingdom, we were able to evaluate the durability of effects by recontacting subjects approximately 2 wk after the first survey. In the second wave, we asked subjects outcome questions once again, without reminding them of earlier stimuli or providing any signal about each claim’s truthfulness.</p><p>The tested fact-checks caused significant gains in factual accuracy. A meta-analytic procedure indicated that, on average, fact-checks increased factual accuracy by 0.59 points on a 5-point scale. In comparison, the same procedure showed that misinformation decreased factual accuracy by less than 0.07 on the same scale and that this decrease was not significant. The observed accuracy increases attributable to fact-checks were durable, with most detectable more than 2 wk after initial exposure to the fact-check. Despite concerns that fact-checking can “backfire” and increase false beliefs (<a href=\"#core-r11\">11</a>), we were unable to identify any instances of such behavior. Instead, in all countries studied, fact-checks reduced belief in misinformation, often for a time beyond immediate exposure.</p><p>Scholars are perennially concerned that their conclusions about human behavior are overly reliant on samples of Western, educated, industrialized, rich, and democratic, or “WEIRD,” populations (<a href=\"#core-r12\">12</a>, <a href=\"#core-r13\">13</a>). WEIRD populations may be distinct from other populations, minimizing the external validity of psychological findings (<a href=\"#core-r14\">14</a>). Our evidence suggests that, when it comes to the effects of fact-checking on belief in misinformation, this is not the case. Although the countries in our study differ starkly along educational, economic, and racial lines, the effects of fact-checking were remarkably similar in all of them.</p><section><h2>Misinformation and Fact-Checking in Global Context</h2><p>Exposure to misinformation is widespread (<a href=\"#core-r3\">3</a>). On social media, misinformation appears to be more appealing to users than factually accurate information (<a href=\"#core-r15\">15</a>). However, research has identified various ways of rebutting the false beliefs that misinformation generates. Relying on crowd-sourcing (<a href=\"#core-r8\">8</a>), delivering news-literacy interventions (<a href=\"#core-r7\">7</a>), and providing fact-checks (<a href=\"#core-r9\">9</a>, <a href=\"#core-r16\">16</a>) have all been shown to have sharp, positive effects on factual accuracy. Our experiments in the present study evaluated fact-checking efforts; for this reason, we hypothesized that exposure to factual corrections would increase subjects’ factual accuracy (H1). (We preregistered our hypotheses, research questions, and research design with the Open Science Framework [OSF]. The preregistration is included in <a href=\"http://www.pnas.org/lookup/doi/10.1073/pnas.2104235118#supplementary-materials\"><i>SI Appendix</i></a>.)</p><p>Little prior work of which we are aware has examined whether national setting affects the size and direction of correction effects. Critical for our purposes, a previous meta-analysis of the effects of attempts to correct misinformation (<a href=\"#core-r9\">9</a>) includes only WEIRD samples, none of which attempted to compare the effects of corrections across countries, let alone non-WEIRD countries. The populations of the four countries studied here are distinct along numerous lines, including aggregate ideological orientation and traditional demographics. It may be the case that the size of accuracy increases generated by fact-checks are different in different national settings. The size of any increases may also vary with different demographics. For these reasons, we studied research questions concerning the relationship between national setting and correction effects; the relationship between participants’ ideology and correction effects; and the relationship between other demographics and correction effects.</p><p>The existing literature is also unclear on the duration of accuracy increases that may follow factual corrections. Even when fact-checks bring about greater accuracy, the initial misinformation can continue to affect reasoning over time (<a href=\"#core-r17\">17</a>). Differences in the duration of effects may be attributable to differences among the topics of misinformation and fact-checks. If the accuracy increases that follow fact-checks are only temporary, this suggests that the increases do not represent meaningful gains in accurate knowledge (<a href=\"#core-r18\">18</a>). While the effects of factual information in general can endure (<a href=\"#core-r19\">19</a>, <a href=\"#core-r20\">20</a>), the durability of accuracy increases prompted by fact-checking in particular is not known. Given the uncertainty of existing findings, we investigate a research question pertaining to the duration of accuracy increases.</p><p>Finally, the existing literature does not systematically investigate whether different topics of political misinformation are more (or less) susceptible to factual correction. Scholars have studied a wide",
        "html": "<div><article><header><div><div><p>Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)</p></div><div><p>September 10, 2021</p><p>118 (37) e2104235118</p></div></div></header><div><section><h2>Significance</h2><p>Little evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in all countries, with most effects detectable more than 2 wk later and with surprisingly little variation by country. Our evidence underscores that fact-checking can serve as a pivotal tool in the fight against misinformation.</p></section><section><h2>Abstract</h2><p>The spread of misinformation is a global phenomenon, with implications for elections, state-sanctioned violence, and health outcomes. Yet, even though scholars have investigated the capacity of fact-checking to reduce belief in misinformation, little evidence exists on the global effectiveness of this approach. We describe fact-checking experiments conducted simultaneously in Argentina, Nigeria, South Africa, and the United Kingdom, in which we studied whether fact-checking can durably reduce belief in misinformation. In total, we evaluated 22 fact-checks, including two that were tested in all four countries. Fact-checking reduced belief in misinformation, with most effects still apparent more than 2 wk later. A meta-analytic procedure indicates that fact-checks reduced belief in misinformation by at least 0.59 points on a 5-point scale. Exposure to misinformation, however, only increased false beliefs by less than 0.07 points on the same scale. Across continents, fact-checks reduce belief in misinformation, often durably so.</p></section>\n<div>\n<h3>Sign up for PNAS alerts.</h3>\n<p>Get alerts for new articles, or get an alert when an article is cited.</p>\n</div>\n</div><div><p>The spread of misinformation is a global phenomenon (<a href=\"#core-r1\">1</a>). Misinformation is said to have played a role in the Myanmar genocide (<a href=\"#core-r2\">2</a>), national elections (<a href=\"#core-r3\">3</a>), and the resurgence of measles (<a href=\"#core-r4\">4</a>). Scholars have investigated various means of reducing belief in misinformation, including, but not limited to, fact-checking (<a href=\"#core-r5\">5</a>–<a href=\"#core-r8\">8</a>). Yet, despite the global scope of the challenge, much of the available evidence about decreasing false beliefs comes from single-country samples gathered in North America, Europe, or Australia. The available evidence also pays scant attention to the durability of accuracy increases that fact-checking may generate. Prior research has shown that fact-checking can reduce false beliefs in single countries (<a href=\"#core-r9\">9</a>, <a href=\"#core-r10\">10</a>). Yet, whether fact-checking can reduce belief in misinformation around the world and whether any such reductions endure are unknown.</p><p>We describe simultaneous experiments conducted in four countries that help resolve both questions. In partnership with fact-checking organizations, we administered experiments in September and October 2020 in Argentina, Nigeria, South Africa, and the United Kingdom. The four countries are diverse along racial, economic, and political lines, but are unified by the presence of fact-checking organizations that have signed on to the standards of the International Fact-Checking Network. The experiments evaluated the effects of fact-checks on beliefs about both country-specific and global misinformation.</p><p>In total, we conducted 28 experiments, evaluating 22 distinct fact-checks. To limit the extent to which differences in timing may have been responsible for differential effects, particularly on the global misinformation items, we fielded all experiments in each country at the same time. In each experiment, participants were randomly assigned to misinformation; misinformation followed by a fact-check; or control. All participants then immediately answered outcome questions about their belief in the false claim advanced by the misinformation. Fact-checking stimuli consisted of fact-checks produced by fact-checking organizations in each country, while misinformation stimuli consisted of brief summaries of the false claims that led to the corresponding fact-checks. This allowed us to estimate misinformation effects (the effect of misinformation on belief accuracy compared to control) and correction effects (the effect of corrections on belief accuracy compared to misinformation).</p><p>The fact-checks targeted a broad swath of misinformation topics, including COVID-19, local politics, crime, and the economy. In Argentina, South Africa, and the United Kingdom, we were able to evaluate the durability of effects by recontacting subjects approximately 2 wk after the first survey. In the second wave, we asked subjects outcome questions once again, without reminding them of earlier stimuli or providing any signal about each claim’s truthfulness.</p><p>The tested fact-checks caused significant gains in factual accuracy. A meta-analytic procedure indicated that, on average, fact-checks increased factual accuracy by 0.59 points on a 5-point scale. In comparison, the same procedure showed that misinformation decreased factual accuracy by less than 0.07 on the same scale and that this decrease was not significant. The observed accuracy increases attributable to fact-checks were durable, with most detectable more than 2 wk after initial exposure to the fact-check. Despite concerns that fact-checking can “backfire” and increase false beliefs (<a href=\"#core-r11\">11</a>), we were unable to identify any instances of such behavior. Instead, in all countries studied, fact-checks reduced belief in misinformation, often for a time beyond immediate exposure.</p><p>Scholars are perennially concerned that their conclusions about human behavior are overly reliant on samples of Western, educated, industrialized, rich, and democratic, or “WEIRD,” populations (<a href=\"#core-r12\">12</a>, <a href=\"#core-r13\">13</a>). WEIRD populations may be distinct from other populations, minimizing the external validity of psychological findings (<a href=\"#core-r14\">14</a>). Our evidence suggests that, when it comes to the effects of fact-checking on belief in misinformation, this is not the case. Although the countries in our study differ starkly along educational, economic, and racial lines, the effects of fact-checking were remarkably similar in all of them.</p><section><h2>Misinformation and Fact-Checking in Global Context</h2><p>Exposure to misinformation is widespread (<a href=\"#core-r3\">3</a>). On social media, misinformation appears to be more appealing to users than factually accurate information (<a href=\"#core-r15\">15</a>). However, research has identified various ways of rebutting the false beliefs that misinformation generates. Relying on crowd-sourcing (<a href=\"#core-r8\">8</a>), delivering news-literacy interventions (<a href=\"#core-r7\">7</a>), and providing fact-checks (<a href=\"#core-r9\">9</a>, <a href=\"#core-r16\">16</a>) have all been shown to have sharp, positive effects on factual accuracy. Our experiments in the present study evaluated fact-checking efforts; for this reason, we hypothesized that exposure to factual corrections would increase subjects’ factual accuracy (H1). (We preregistered our hypotheses, research questions, and research design with the Open Science Framework [OSF]. The preregistration is included in <a href=\"http://www.pnas.org/lookup/doi/10.1073/pnas.2104235118#supplementary-materials\"><i>SI Appendix</i></a>.)</p><p>Little prior work of which we are aware has examined whether national setting affects the size and direction of correction effects. Critical for our purposes, a previous meta-analysis of the effects of attempts to correct misinformation (<a href=\"#core-r9\">9</a>) includes only WEIRD samples, none of which attempted to compare the effects of corrections across countries, let alone non-WEIRD countries. The populations of the four countries studied here are distinct along numerous lines, including aggregate ideological orientation and traditional demographics. It may be the case that the size of accuracy increases generated by fact-checks are different in different national settings. The size of any increases may also vary with different demographics. For these reasons, we studied research questions concerning the relationship between national setting and correction effects; the relationship between participants’ ideology and correction effects; and the relationship between other demographics and correction effects.</p><p>The existing literature is also unclear on the duration of accuracy increases that may follow factual corrections. Even when fact-checks bring about greater accuracy, the initial misinformation can continue to affect reasoning over time (<a href=\"#core-r17\">17</a>). Differences in the duration of effects may be attributable to differences among the topics of misinformation and fact-checks. If the accuracy increases that follow fact-checks are only temporary, this suggests that the increases do not represent meaningful gains in accurate knowledge (<a href=\"#core-r18\">18</a>). While the effects of factual information in general can endure (<a href=\"#core-r19\">19</a>, <a href=\"#core-r20\">20</a>), the durability of accuracy increases prompted by fact-checking in particular is not known. Given the uncertainty of existing findings, we investigate a research question pertaining to the duration of accuracy increases.</p><p>Finally, the existing literature does not systematically investigate whether different topics of political misinformation are more (or less) susceptible to factual correction. Scholars have studied a wide",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)September 10, 2021118 (37) e2104235118SignificanceLittle evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)September 10, 2021118 (37) e2104235118SignificanceLittle evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)September 10, 2021118 (37) e2104235118",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Edited by Gordon Pennycook, University of Regina, Regina, SK, Canada, and accepted by Editorial Board Member Margaret Levi July 23, 2021 (received for review March 4, 2021)",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "September 10, 2021118 (37) e2104235118",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "SignificanceLittle evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in all countries, with most effects detectable more than 2 wk later and with surprisingly little variation by country. Our evidence underscores that fact-checking can serve as a pivotal tool in the fight against ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "SignificanceLittle evidence exists on the global effectiveness, or lack thereof, of potential solutions to misinformation. We conducted simultaneous experiments in four countries to investigate the extent to which fact-checking can reduce false beliefs. Fact-checks reduced false beliefs in all countries, with most effects detectable more than 2 wk later and with surprisingly little variation by country. Our evidence underscores that fact-checking can serve as a pivotal tool in the fight against ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractThe spread of misinformation is a global phenomenon, with implications for elections, state-sanctioned violence, and health outcomes. Yet, even though scholars have investigated the capacity of fact-checking to reduce belief in misinformation, little evidence exists on the global effectiveness of this approach. We describe fact-checking experiments conducted simultaneously in Argentina, Nigeria, South Africa, and the United Kingdom, in which we studied whether fact-checking can durably r",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sign up for PNAS alerts.Get alerts for new articles, or get an alert when an article is cited.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The spread of misinformation is a global phenomenon (1). Misinformation is said to have played a role in the Myanmar genocide (2), national elections (3), and the resurgence of measles (4). Scholars have investigated various means of reducing belief in misinformation, including, but not limited to, fact-checking (5–8). Yet, despite the global scope of the challenge, much of the available evidence about decreasing false beliefs comes from single-country samples gathered in North America, Europe, ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Misinformation and Fact-Checking in Global ContextExposure to misinformation is widespread (3). On social media, misinformation appears to be more appealing to users than factually accurate information (15). However, research has identified various ways of rebutting the false beliefs that misinformation generates. Relying on crowd-sourcing (8), delivering news-literacy interventions (7), and providing fact-checks (9,16) have all been shown to have sharp, positive effects on factual accuracy. Our",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Significance",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Sign up for PNAS alerts.",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Misinformation and Fact-Checking in Global Context",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.tandfonline.com/doi/full/10.1080/1461670X.2024.2357316",
      "title": "What Is the Problem with Misinformation? Fact-checking as a Sociotechnical and Problem-Solving Practice",
      "author": "Steen  Steensen",
      "published_date": "2024-06-10T00:00:00.000Z",
      "content": {
        "text": "<div><div><div><h2>ABSTRACT</h2><p>Misinformation is a complex and global problem of social and technical dimensions. It is a problem that is exacerbated and sought to be solved by using diverse technologies. It is also a problem that flourishes on platforms and can lead to partnerships with platform companies. These sociotechnical dimensions of misinformation as a problem involve different actors. Some actors create or contribute to the problem, while others perceive it as their problem to solve and work to address it. Identifying the problem of misinformation is at the heart of the issue of problem-solving in fact-checking, as different actors have interests in how problems are discursively presented. This article draws on an international interview study conducted throughout 2020–2022 with 46 fact-checking actors (21 fact-checkers, 14 journalists, and 11 newsroom managers). This article analyzes how these actors reflect on “misinformation problems,” and how these problems become “fact-checking problems” for the actors to work with and solve. Ultimately, the article argues that fact-checking must be approached as a sociotechnical and problem-solving-oriented practice. Doing so highlights specific obstacles in information distribution and platform affordances.</p></div><div><p>Misinformation is a complex and global problem of social and technical dimensions. Misinformation can mislead people, contributing to uninformed and bad decisions (Pennycook and Rand <span><a href=\"#\"><span>Citation</span>2021</a></span>). Misinformation may also undermine trust in institutions and expert knowledge (Ognyanova et al. <span><a href=\"#\"><span>Citation</span>2020</a></span>), polarize political discourse (Vicario et al. <span><a href=\"#\"><span>Citation</span>2019</a></span>), influence the outcome of elections (Miró-Llinares and Aguerri <span><a href=\"#\"><span>Citation</span>2021</a></span>), and endanger public health and safety during pandemics and other crises (Bursztyn et al. <span><a href=\"#\"><span>Citation</span>2020</a></span>). The problem of misinformation may be exacerbated and/or mitigated by technological changes related to platform companies, automation, media and information literacy, and various verification technologies (see Creech <span><a href=\"#\"><span>Citation</span>2020</a></span>). Technology plays at least three key roles concerning “solving” misinformation problems. First, actors involved in solving the misinformation problems consider why problems occur (technology as an enabler of problems). Second, actors consider how problems can be solved (technology as an enabler of problem-solving). Third, actors consider what a problem is when technology cannot provide a solution (problems dismissed as non-problems when technology cannot be used).</p><p>These sociotechnical dimensions of misinformation as a problem involve different actors with some creating or contributing to the problem, and with others working to address it. Policymakers, governance bodies, non-governmental organizations (NGOs), journalists, fact-checking organizations, educators, and researchers contribute to efforts to solve the problem of misinformation. Platform companies facilitate misinformation via sociotechnical infrastructures for publishing and distributing misinformation; simultaneously, they are engaged in initiatives to fight misinformation. For instance, these actors support fact-checking organizations, enroll users to flag content, promote media literacy among users, and engage in active content moderation (Lien, Lee, and Tandoc <span><a href=\"#\"><span>Citation</span>2022</a></span>). Understanding the misinformation problems is essential for fact-checking actors to determine if they have the resources necessary for epistemic efforts (Ekström, Ramsälv, and Westlund <span><a href=\"#\"><span>Citation</span>2022</a></span>; Steensen, Kalsnes, and Westlund <span><a href=\"#\"><span>Citation</span>2023</a></span>). Fact-checking actors are situated in a complex platform environment that imposes constant constraints on their work. This article builds on 46 interviews with fact-checking actors, here defined as (1) people who directly produce fact checks as well as (2) their managers (in newsrooms and independent/NGOs), and (3) other journalists writing about fact-checking and misinformation but not producing actual fact-checks (e.g., reporters who cover the misinformation beat).</p><p>The article explores how fact-checking actors reflect on “misinformation problems” and how they gravitate towards “fact-checking problems.” We thus also explore how these fact-checking actors identify and prioritize problems to solve as a community of practice. The article addresses the overall question: <i>how do fact-checking actors understand misinformation as a problem for them to solve?</i></p><p>The article offers insights into the nature of the field when it comes to misinformation, fact-checking practices, and the role of technology and platforms. It argues that fact-checking must be approached as sociotechnical and problem-solving-oriented practices. Doing so highlights obstacles, related for instance to distribution and platform affordances. The fact-checking actors interviewed identify and articulate several problems, which in this article are synthesized into five key areas:\n</p><ol><li><p>Limited affordances of digital technologies;</p></li><li><p>Limited agency on platform infrastructures;</p></li><li><p>Limited expertise and human resources;</p></li><li><p>Hostility towards fact-checking actors; and</p></li><li><p>Fact-checks fueling misinformation.</p></li></ol></div><div><h2>Analytical Framework</h2><p>The vibrant and collaborative global fact-checking community consists of actors including news organizations, civil society organizations, academic institutions, and independent fact-checking startups. The community includes projects that are collaborations between different kinds of institutions, as well as projects attached to news media consortiums and major global news agencies (Graves and Cherubini <span><a href=\"#\"><span>Citation</span>2016</a></span>). The fact-checking movement initially focused primarily on what practitioners call political fact-checking, or identifying and checking truth claims made by politicians and other public figures. Beginning in 2016, emphasis has increasingly shifted toward debunking, or what Graves, Bélair-Gagnon, and Larsen (<span><a href=\"#\"><span>Citation</span>2023</a></span>) refer to as “policing viral misinformation online.” Debunking typically focuses on exposing absurd hoaxes, conspiracy theories, and fakes, as opposed to “elite political discourse”. Debunking is closely linked to third-party fact-checking programs run by platform companies, who have enrolled fact-checkers to clean their platforms from misinformation. Interviewers for this study asked about fact-checking practices including political fact-checking as well as debunking. These practices present overlapping but distinct challenges and rely on different methods and tools (Graves, Bélair-Gagnon, and Larsen <span><a href=\"#\"><span>Citation</span>2023</a></span>; Westlund et al. <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>Fact-checkers regularly communicate within the community, sharing experiences about their fact-checking practices and what technologies to use for different types of problems. For example, International Fact-Checking Network (IFCN) members have access to Slack channels for such exchanges. The global fact-checking movement has thus developed professional standards and practices, becoming a community where actors support each other in problem-solving via experimentation, pooling resources, and information sharing. Such collaborative work is a key characteristic of what scholars refer to as a “community of practice”, marked by having (inter-institutional) collaborative networks in which they seek to improve and learn through experimentation and interactions (Brookes and Waller <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>These exchanges of resources and experiences help fact-checkers in their shared pursuit of solving problems associated with misinformation. They cannot solve all problems on their own and have thus actively worked on partnerships with resourceful actors. IFCN members engage substantially in debunking on Facebook, incentivized and supported through their partnership with Meta (which also includes Instagram and WhatsApp). However, such partnerships take different shapes and do not exist for all platforms, limiting fact-checkers’ ability to identify, track, and effectively combat disinformation on different social networks (e.g., IFCN <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>The partnerships with actors such as platform companies provide legitimacy and help support the work of fact-checkers (Graves and Lauer <span><a href=\"#\"><span>Citation</span>2020</a></span>). Fact-checking actors often operate within platform partnerships that provide financial flows of varying size and importance, access to sociotechnical resources such as systems for identifying potential misinformation, and algorithmic systems for distribution of fact-checks (Bélair-Gagnon et al. <span><a href=\"#\"><span>Citation</span>2023a</a></span>). The following sections will further unpack how fact-checking constitutes a sociotechnical and problem-solving practice.</p><div><h3>Sociotechnical Practice</h3><p>Adopting a sociotechnical approach entails focusing on how social actors and technologies (e.g., machines, digital infrastructures, algorithms) work together. Pickering labeled this dynamic as “mangle of practice,” meaning an “evolving field of human and material agencies reciprocally engaged in a play of resistance and accommodation in which the former seeks to capture the latter” (<span><a href=\"#\"><span>Citation</span>1995</a></span>, 23). Research has applied such a sociotechni",
        "html": "<div><div><div><h2>ABSTRACT</h2><p>Misinformation is a complex and global problem of social and technical dimensions. It is a problem that is exacerbated and sought to be solved by using diverse technologies. It is also a problem that flourishes on platforms and can lead to partnerships with platform companies. These sociotechnical dimensions of misinformation as a problem involve different actors. Some actors create or contribute to the problem, while others perceive it as their problem to solve and work to address it. Identifying the problem of misinformation is at the heart of the issue of problem-solving in fact-checking, as different actors have interests in how problems are discursively presented. This article draws on an international interview study conducted throughout 2020–2022 with 46 fact-checking actors (21 fact-checkers, 14 journalists, and 11 newsroom managers). This article analyzes how these actors reflect on “misinformation problems,” and how these problems become “fact-checking problems” for the actors to work with and solve. Ultimately, the article argues that fact-checking must be approached as a sociotechnical and problem-solving-oriented practice. Doing so highlights specific obstacles in information distribution and platform affordances.</p></div><div><p>Misinformation is a complex and global problem of social and technical dimensions. Misinformation can mislead people, contributing to uninformed and bad decisions (Pennycook and Rand <span><a href=\"#\"><span>Citation</span>2021</a></span>). Misinformation may also undermine trust in institutions and expert knowledge (Ognyanova et al. <span><a href=\"#\"><span>Citation</span>2020</a></span>), polarize political discourse (Vicario et al. <span><a href=\"#\"><span>Citation</span>2019</a></span>), influence the outcome of elections (Miró-Llinares and Aguerri <span><a href=\"#\"><span>Citation</span>2021</a></span>), and endanger public health and safety during pandemics and other crises (Bursztyn et al. <span><a href=\"#\"><span>Citation</span>2020</a></span>). The problem of misinformation may be exacerbated and/or mitigated by technological changes related to platform companies, automation, media and information literacy, and various verification technologies (see Creech <span><a href=\"#\"><span>Citation</span>2020</a></span>). Technology plays at least three key roles concerning “solving” misinformation problems. First, actors involved in solving the misinformation problems consider why problems occur (technology as an enabler of problems). Second, actors consider how problems can be solved (technology as an enabler of problem-solving). Third, actors consider what a problem is when technology cannot provide a solution (problems dismissed as non-problems when technology cannot be used).</p><p>These sociotechnical dimensions of misinformation as a problem involve different actors with some creating or contributing to the problem, and with others working to address it. Policymakers, governance bodies, non-governmental organizations (NGOs), journalists, fact-checking organizations, educators, and researchers contribute to efforts to solve the problem of misinformation. Platform companies facilitate misinformation via sociotechnical infrastructures for publishing and distributing misinformation; simultaneously, they are engaged in initiatives to fight misinformation. For instance, these actors support fact-checking organizations, enroll users to flag content, promote media literacy among users, and engage in active content moderation (Lien, Lee, and Tandoc <span><a href=\"#\"><span>Citation</span>2022</a></span>). Understanding the misinformation problems is essential for fact-checking actors to determine if they have the resources necessary for epistemic efforts (Ekström, Ramsälv, and Westlund <span><a href=\"#\"><span>Citation</span>2022</a></span>; Steensen, Kalsnes, and Westlund <span><a href=\"#\"><span>Citation</span>2023</a></span>). Fact-checking actors are situated in a complex platform environment that imposes constant constraints on their work. This article builds on 46 interviews with fact-checking actors, here defined as (1) people who directly produce fact checks as well as (2) their managers (in newsrooms and independent/NGOs), and (3) other journalists writing about fact-checking and misinformation but not producing actual fact-checks (e.g., reporters who cover the misinformation beat).</p><p>The article explores how fact-checking actors reflect on “misinformation problems” and how they gravitate towards “fact-checking problems.” We thus also explore how these fact-checking actors identify and prioritize problems to solve as a community of practice. The article addresses the overall question: <i>how do fact-checking actors understand misinformation as a problem for them to solve?</i></p><p>The article offers insights into the nature of the field when it comes to misinformation, fact-checking practices, and the role of technology and platforms. It argues that fact-checking must be approached as sociotechnical and problem-solving-oriented practices. Doing so highlights obstacles, related for instance to distribution and platform affordances. The fact-checking actors interviewed identify and articulate several problems, which in this article are synthesized into five key areas:\n</p><ol><li><p>Limited affordances of digital technologies;</p></li><li><p>Limited agency on platform infrastructures;</p></li><li><p>Limited expertise and human resources;</p></li><li><p>Hostility towards fact-checking actors; and</p></li><li><p>Fact-checks fueling misinformation.</p></li></ol></div><div><h2>Analytical Framework</h2><p>The vibrant and collaborative global fact-checking community consists of actors including news organizations, civil society organizations, academic institutions, and independent fact-checking startups. The community includes projects that are collaborations between different kinds of institutions, as well as projects attached to news media consortiums and major global news agencies (Graves and Cherubini <span><a href=\"#\"><span>Citation</span>2016</a></span>). The fact-checking movement initially focused primarily on what practitioners call political fact-checking, or identifying and checking truth claims made by politicians and other public figures. Beginning in 2016, emphasis has increasingly shifted toward debunking, or what Graves, Bélair-Gagnon, and Larsen (<span><a href=\"#\"><span>Citation</span>2023</a></span>) refer to as “policing viral misinformation online.” Debunking typically focuses on exposing absurd hoaxes, conspiracy theories, and fakes, as opposed to “elite political discourse”. Debunking is closely linked to third-party fact-checking programs run by platform companies, who have enrolled fact-checkers to clean their platforms from misinformation. Interviewers for this study asked about fact-checking practices including political fact-checking as well as debunking. These practices present overlapping but distinct challenges and rely on different methods and tools (Graves, Bélair-Gagnon, and Larsen <span><a href=\"#\"><span>Citation</span>2023</a></span>; Westlund et al. <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>Fact-checkers regularly communicate within the community, sharing experiences about their fact-checking practices and what technologies to use for different types of problems. For example, International Fact-Checking Network (IFCN) members have access to Slack channels for such exchanges. The global fact-checking movement has thus developed professional standards and practices, becoming a community where actors support each other in problem-solving via experimentation, pooling resources, and information sharing. Such collaborative work is a key characteristic of what scholars refer to as a “community of practice”, marked by having (inter-institutional) collaborative networks in which they seek to improve and learn through experimentation and interactions (Brookes and Waller <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>These exchanges of resources and experiences help fact-checkers in their shared pursuit of solving problems associated with misinformation. They cannot solve all problems on their own and have thus actively worked on partnerships with resourceful actors. IFCN members engage substantially in debunking on Facebook, incentivized and supported through their partnership with Meta (which also includes Instagram and WhatsApp). However, such partnerships take different shapes and do not exist for all platforms, limiting fact-checkers’ ability to identify, track, and effectively combat disinformation on different social networks (e.g., IFCN <span><a href=\"#\"><span>Citation</span>2022</a></span>).</p><p>The partnerships with actors such as platform companies provide legitimacy and help support the work of fact-checkers (Graves and Lauer <span><a href=\"#\"><span>Citation</span>2020</a></span>). Fact-checking actors often operate within platform partnerships that provide financial flows of varying size and importance, access to sociotechnical resources such as systems for identifying potential misinformation, and algorithmic systems for distribution of fact-checks (Bélair-Gagnon et al. <span><a href=\"#\"><span>Citation</span>2023a</a></span>). The following sections will further unpack how fact-checking constitutes a sociotechnical and problem-solving practice.</p><div><h3>Sociotechnical Practice</h3><p>Adopting a sociotechnical approach entails focusing on how social actors and technologies (e.g., machines, digital infrastructures, algorithms) work together. Pickering labeled this dynamic as “mangle of practice,” meaning an “evolving field of human and material agencies reciprocally engaged in a play of resistance and accommodation in which the former seeks to capture the latter” (<span><a href=\"#\"><span>Citation</span>1995</a></span>, 23). Research has applied such a sociotechni",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "ABSTRACTMisinformation is a complex and global problem of social and technical dimensions. It is a problem that is exacerbated and sought to be solved by using diverse technologies. It is also a problem that flourishes on platforms and can lead to partnerships with platform companies. These sociotechnical dimensions of misinformation as a problem involve different actors. Some actors create or contribute to the problem, while others perceive it as their problem to solve and work to address it. I",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "ABSTRACTMisinformation is a complex and global problem of social and technical dimensions. It is a problem that is exacerbated and sought to be solved by using diverse technologies. It is also a problem that flourishes on platforms and can lead to partnerships with platform companies. These sociotechnical dimensions of misinformation as a problem involve different actors. Some actors create or contribute to the problem, while others perceive it as their problem to solve and work to address it. I",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "ABSTRACTMisinformation is a complex and global problem of social and technical dimensions. It is a problem that is exacerbated and sought to be solved by using diverse technologies. It is also a problem that flourishes on platforms and can lead to partnerships with platform companies. These sociotechnical dimensions of misinformation as a problem involve different actors. Some actors create or contribute to the problem, while others perceive it as their problem to solve and work to address it. I",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Misinformation is a complex and global problem of social and technical dimensions. Misinformation can mislead people, contributing to uninformed and bad decisions (Pennycook and RandCitation2021). Misinformation may also undermine trust in institutions and expert knowledge (Ognyanova et al.Citation2020), polarize political discourse (Vicario et al.Citation2019), influence the outcome of elections (Miró-Llinares and AguerriCitation2021), and endanger public health and safety during pandemics and ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Analytical FrameworkThe vibrant and collaborative global fact-checking community consists of actors including news organizations, civil society organizations, academic institutions, and independent fact-checking startups. The community includes projects that are collaborations between different kinds of institutions, as well as projects attached to news media consortiums and major global news agencies (Graves and CherubiniCitation2016). The fact-checking movement initially focused primarily on w",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sociotechnical PracticeAdopting a sociotechnical approach entails focusing on how social actors and technologies (e.g., machines, digital infrastructures, algorithms) work together. Pickering labeled this dynamic as “mangle of practice,” meaning an “evolving field of human and material agencies reciprocally engaged in a play of resistance and accommodation in which the former seeks to capture the latter” (Citation1995, 23). Research has applied such a sociotechni",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "ABSTRACT",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Analytical Framework",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Sociotechnical Practice",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.edutopia.org/blog/starting-student-feedback-loops-taylor-meredith",
      "title": "Starting Student Feedback Loops",
      "author": "Taylor Meredith",
      "published_date": "2015-05-15T17:00:00.000Z",
      "content": {
        "text": "<div><p>Feedback has been in the spotlight lately. Gone are the days of feedback scrawled below a letter grade, the days of red-inked papers and assignments. What was once final is now formative. As an educator (and person), my feedback approach has changed. I used to provide what I called feedback to my students on final assignments, writing pieces, and projects. Even though I had provided thoughtful suggestions for improvement, I was not seeing visible improvement in their subsequent work. I decided to take a closer look to understand where my feedback process was failing.</p><div><p>Reading this, I was confident that our classroom community of practice and growth would support a culture of feedback, and I was inspired to try student feedback loops.</p><h3>What Are Student Feedback Loops?</h3><p>A feedback loop is a process that aims to move learning forward through feedback. Ideally, this feedback loop would happen frequently, in all subject areas. This is one way to structure the process:</p><h4>1. Begin With an Aim</h4><p>An aim is a learning target or essential question that is unpacked from the standards, a part of a learning progression that is clearly communicated to the students at the beginning of each lesson.</p><h4>2. Feedback Exchange</h4><p>Feedback should be specific, non-evaluative, manageable, and focused on the aim. If the aim for the day is that readers should structure reasons to develop a compelling argument in a research-based essay, all feedback exchanged should be focused on that aim. I used the heart and brain strategy to support effective feedback exchanges (more on that below).</p><h4>3. Revision and Application</h4><p>In order for feedback to be effective, students must be given time to revise and apply their new understandings or ideas. Susan Brookhart and Connie Moss, authors of <em><a href=\"http://shop.ascd.org/Default.aspx?TabID=55&amp;ProductId=1072&amp;Advancing-formative-assessment-in-every-classroom:-a-guide-for-instructional-leaders\">Advancing Formative Assessment in Every Classroom: A Guide for Instructional Leaders</a></em>, speak of the Golden Second Opportunity, that moment when feedback is grasped and applied. When a student takes the feedback, makes changes to his or her work, and as a result moves a step closer to meeting the desired learning of the day's aim, then the loop has started. It is authentic, purposeful learning. The teacher begins the process, but the student owns it.</p><h4>4. Reflection</h4><p>Closing the loop is time to reflect on the aim. Did students meet the desired learning of the day's aim? Could they move to a different level of proficiency? Could they ask for more feedback? Are there any other areas to revise?</p><p>In student feedback loops, students are the ones who drive this process. The teacher supports the students by clearly defining a structure for feedback, modeling effective feedback, highlighting critical student feedback, and participating when necessary.</p><h3>Heart and Brain Feedback</h3><p>The structure that has worked best for my fifth grade students has been the heart and brain strategy. Provide one piece of feedback for each:</p><h4>The Heart</h4><p>This addresses something that you liked or loved, something that really stuck with you in a positive way. Related to the aim, the heart is something that worked well, be used as an example or a mentor to others. For example: \"The way that you structured your reasons from broad to narrow really worked. It created a compelling argument that made me think in a new way.\"</p><h4>The Brain</h4><p>This is something to try or consider revising. Related to the aim, the brain is a suggestion for specific growth, change, or improvement. For example: \"Have you thought about using repetition here to make this stand out?\"</p><p>This strategy keeps things simple and actionable. As a class, we brainstorm the language we can use to ensure that feedback is non-evaluative and feeds forward.</p><h3>Who Benefits?</h3><p>Everyone benefits: students who receive feedback, students who give feedback, and anyone who listens to the feedback. Even when giving feedback, students are focused on the aim of the day. Throughout the process, students may identify areas of growth in their own work, find peer examples as models, and take ownership over their work. As a teacher, my instruction is informed through these feedback loops. Listening to these loops tells me if I need to revisit a certain aim or set my expectations higher. I can pull exemplars and students to model their thinking aloud. All learners benefit from effective feedback loops.</p><h3>Where to Start?</h3><p>Reflect as a teacher. Start small. Think aloud while you model. Ask students for feedback, point out effective attributes, and revise. Start your first feedback loop as a whole class. Reflect as a class. Your classroom community will keep feedback loops in their hearts and brains as they grow and explore.</p></div></div>",
        "html": "<div><p>Feedback has been in the spotlight lately. Gone are the days of feedback scrawled below a letter grade, the days of red-inked papers and assignments. What was once final is now formative. As an educator (and person), my feedback approach has changed. I used to provide what I called feedback to my students on final assignments, writing pieces, and projects. Even though I had provided thoughtful suggestions for improvement, I was not seeing visible improvement in their subsequent work. I decided to take a closer look to understand where my feedback process was failing.</p><div><p>Reading this, I was confident that our classroom community of practice and growth would support a culture of feedback, and I was inspired to try student feedback loops.</p><h3>What Are Student Feedback Loops?</h3><p>A feedback loop is a process that aims to move learning forward through feedback. Ideally, this feedback loop would happen frequently, in all subject areas. This is one way to structure the process:</p><h4>1. Begin With an Aim</h4><p>An aim is a learning target or essential question that is unpacked from the standards, a part of a learning progression that is clearly communicated to the students at the beginning of each lesson.</p><h4>2. Feedback Exchange</h4><p>Feedback should be specific, non-evaluative, manageable, and focused on the aim. If the aim for the day is that readers should structure reasons to develop a compelling argument in a research-based essay, all feedback exchanged should be focused on that aim. I used the heart and brain strategy to support effective feedback exchanges (more on that below).</p><h4>3. Revision and Application</h4><p>In order for feedback to be effective, students must be given time to revise and apply their new understandings or ideas. Susan Brookhart and Connie Moss, authors of <em><a href=\"http://shop.ascd.org/Default.aspx?TabID=55&amp;ProductId=1072&amp;Advancing-formative-assessment-in-every-classroom:-a-guide-for-instructional-leaders\">Advancing Formative Assessment in Every Classroom: A Guide for Instructional Leaders</a></em>, speak of the Golden Second Opportunity, that moment when feedback is grasped and applied. When a student takes the feedback, makes changes to his or her work, and as a result moves a step closer to meeting the desired learning of the day's aim, then the loop has started. It is authentic, purposeful learning. The teacher begins the process, but the student owns it.</p><h4>4. Reflection</h4><p>Closing the loop is time to reflect on the aim. Did students meet the desired learning of the day's aim? Could they move to a different level of proficiency? Could they ask for more feedback? Are there any other areas to revise?</p><p>In student feedback loops, students are the ones who drive this process. The teacher supports the students by clearly defining a structure for feedback, modeling effective feedback, highlighting critical student feedback, and participating when necessary.</p><h3>Heart and Brain Feedback</h3><p>The structure that has worked best for my fifth grade students has been the heart and brain strategy. Provide one piece of feedback for each:</p><h4>The Heart</h4><p>This addresses something that you liked or loved, something that really stuck with you in a positive way. Related to the aim, the heart is something that worked well, be used as an example or a mentor to others. For example: \"The way that you structured your reasons from broad to narrow really worked. It created a compelling argument that made me think in a new way.\"</p><h4>The Brain</h4><p>This is something to try or consider revising. Related to the aim, the brain is a suggestion for specific growth, change, or improvement. For example: \"Have you thought about using repetition here to make this stand out?\"</p><p>This strategy keeps things simple and actionable. As a class, we brainstorm the language we can use to ensure that feedback is non-evaluative and feeds forward.</p><h3>Who Benefits?</h3><p>Everyone benefits: students who receive feedback, students who give feedback, and anyone who listens to the feedback. Even when giving feedback, students are focused on the aim of the day. Throughout the process, students may identify areas of growth in their own work, find peer examples as models, and take ownership over their work. As a teacher, my instruction is informed through these feedback loops. Listening to these loops tells me if I need to revisit a certain aim or set my expectations higher. I can pull exemplars and students to model their thinking aloud. All learners benefit from effective feedback loops.</p><h3>Where to Start?</h3><p>Reflect as a teacher. Start small. Think aloud while you model. Ask students for feedback, point out effective attributes, and revise. Start your first feedback loop as a whole class. Reflect as a class. Your classroom community will keep feedback loops in their hearts and brains as they grow and explore.</p></div></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Feedback has been in the spotlight lately. Gone are the days of feedback scrawled below a letter grade, the days of red-inked papers and assignments. What was once final is now formative. As an educator (and person), my feedback approach has changed. I used to provide what I called feedback to my students on final assignments, writing pieces, and projects. Even though I had provided thoughtful suggestions for improvement, I was not seeing visible improvement in their subsequent work. I decided t",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Reading this, I was confident that our classroom community of practice and growth would support a culture of feedback, and I was inspired to try student feedback loops.What Are Student Feedback Loops?A feedback loop is a process that aims to move learning forward through feedback. Ideally, this feedback loop would happen frequently, in all subject areas. This is one way to structure the process:1. Begin With an AimAn aim is a learning target or essential question that is unpacked from the standa",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h3",
              "text": "What Are Student Feedback Loops?",
              "id": ""
            },
            {
              "level": "h4",
              "text": "1. Begin With an Aim",
              "id": ""
            },
            {
              "level": "h4",
              "text": "2. Feedback Exchange",
              "id": ""
            },
            {
              "level": "h4",
              "text": "3. Revision and Application",
              "id": ""
            },
            {
              "level": "h4",
              "text": "4. Reflection",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Heart and Brain Feedback",
              "id": ""
            },
            {
              "level": "h4",
              "text": "The Heart",
              "id": ""
            },
            {
              "level": "h4",
              "text": "The Brain",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Who Benefits?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Where to Start?",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://www.linkedin.com/pulse/evolving-knowledge-management-navigating-ethical-digital-wakuthii",
      "title": "Evolving Knowledge Management: Navigating Ethical Challenges in the Digital Era",
      "author": "SARAH WAKUTHII",
      "published_date": "2023-09-11T17:08:01.000Z",
      "content": {
        "text": "<div><div>\n<div>\n<article>\n<figure>\n</figure>\n<header>\n<div>\n<p><a href=\"https://ke.linkedin.com/in/sarah-wakuthii-558690199\">\n<span>\nSARAH WAKUTHII\n</span>\n</a></p><div>\n<h3>\nSARAH WAKUTHII\n</h3>\n<h4>\nTransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:\n</h4>\n<p>\nPublished Sep 11, 2023\n</p>\n</div>\n</div>\n</header>\n<div>\n<p>\n<span>In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of knowledge are pivotal in making well-informed decisions, fostering innovation, and gaining a competitive edge. However, as the landscape of knowledge management expands, so do the ethical challenges that accompany it. In this article, we explore the ethical implications of knowledge management in the digital age and how organizations can navigate these challenges to foster responsible and sustainable practices.</span>\n</p>\n<p>\n<span>The Power of Data and Knowledge;</span>\n</p>\n<p>\n<span>In the digital era, data has become a valuable commodity, and organizations strive to collect vast amounts of information to gain insights and drive strategic decision-making. The combination of data and knowledge management has the potential to revolutionize businesses, healthcare, education, and even national security. For instance, data-driven healthcare systems can enhance patient care through personalized treatment plans based on historical patient data and medical research. Similarly, businesses can optimize supply chain operations by leveraging predictive analytics to forecast demand and streamline inventory management.</span>\n</p>\n<p>\n<span>Ethical Challenges in Knowledge Management;</span>\n</p>\n<p>\n<span>Amidst the immense potential of knowledge management in the digital age, several ethical challenges have emerged. The first major concern revolves around data privacy and security. As organizations collect and store vast amounts of personal and sensitive data, they must ensure robust cybersecurity measures are in place to safeguard against breaches and unauthorized access. The misuse of data can lead to severe consequences, such as identity theft, privacy violations, and erosion of customer trust.</span>\n</p>\n<p>\n<span>Another critical ethical issue is data ownership and control. With the rise of big data and the Internet of Things (IoT), the amount of data generated from various sources is vast and diverse. Organizations must address questions surrounding data ownership rights and the responsible use of data. In cases where data is shared or sold to third parties, organizations must be transparent and obtain informed consent from individuals to protect their rights.</span>\n</p>\n<p>\n<span>Transparency and Bias in AI and Machine Learning;</span>\n</p>\n<p>\n<span>As AI and machine learning algorithms play an increasingly prominent role in knowledge management, ensuring transparency and avoiding biases become paramount. AI-driven decision-making processes must be transparent and explainable, especially in sectors such as finance, healthcare, and recruitment, where the impact on individuals' lives is significant. Understanding how AI algorithms arrive at decisions is crucial for accountability and mitigating potential biases.</span>\n</p>\n<p>\n<span>Preventing Intellectual Property Violations;</span>\n</p>\n<p>\n<span>Knowledge management often involves sharing and disseminating information. However, organizations must be cautious about the inadvertent sharing of copyrighted material or intellectual property violations. Properly attributing sources, adhering to licensing agreements, and respecting copyright laws are essential to maintain ethical knowledge management practices.</span>\n</p>\n<p>\n<span>The Challenge of Misinformation and Fake News;</span>\n</p>\n<p>\n<span>The digital era has seen an unprecedented proliferation of misinformation and fake news. As information spreads rapidly through social media and digital channels, knowledge management systems must focus on verifying the accuracy and reliability of sources before incorporating information into decision-making processes. Combating misinformation requires critical thinking and fact-checking mechanisms to ensure the dissemination of credible knowledge.</span>\n</p>\n<p>\n<span>Strategies for Ethical Knowledge Management;</span>\n</p>\n<p>\n<span>To navigate the ethical challenges in the digital era, organizations should adopt several strategies to foster responsible knowledge management practices.</span>\n</p>\n<div>\n<ol>\n<li><span>Establish Ethical Guidelines: Develop clear and comprehensive ethical guidelines that encompass data privacy, transparency, data ownership, and the responsible use of AI and machine learning technologies.</span></li>\n<li><span>Prioritize Data Security: Implement robust cybersecurity measures to safeguard sensitive data from potential breaches and unauthorized access. Encrypt data and limit access to authorized personnel only.</span></li>\n<li><span>Promote Transparency: Ensure that AI and machine learning algorithms are transparent and explainable. Communicate to stakeholders how decisions are made to build trust and accountability.</span></li>\n<li><span>Invest in Ethical Training: Train employees in ethical knowledge management practices, including data privacy, copyright compliance, and responsible AI usage.</span></li>\n<li><span>Fact-Checking and Verification: Implement fact-checking mechanisms and verification processes to ensure the accuracy and reliability of information incorporated into knowledge management systems.</span></li>\n<li><span>Engage in Continuous Ethical Review: Regularly review and update ethical guidelines to stay abreast of emerging ethical challenges in knowledge management.</span></li>\n</ol>\n</div>\n<p>\n<span>Knowledge management is a powerful tool that can transform organizations and societies in the digital era. However, it comes with ethical challenges that require careful consideration and proactive solutions. By prioritizing data privacy, promoting transparency, and investing in ethical training, organizations can navigate the ethical complexities of knowledge management and foster responsible practices. Embracing ethical knowledge management not only ensures compliance with regulations but also builds trust with stakeholders and strengthens the foundation for sustainable growth and innovation in the digital age.</span>\n</p>\n</div>\n</article>\n<div>\n<p><a href=\"https://www.linkedin.com/newsletters/knowledge-management-spotlight-7058855313378074624\">\n<span>\nKnowledge Management Spotlight\n</span>\n</a></p><p>\n</p><h3>\nKnowledge Management Spotlight\n</h3>\n<h4>\n581 followers\n</h4>\n<p></p>\n</div>\n<section>\n<h2>\nMore articles by SARAH WAKUTHII\n</h2>\n<div>\n<ul>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-predictive-analytics-enhances-km-sarah-wakuthii-ax5xf\">\n<span>\nHow Predictive Analytics Enhances KM.\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nHow Predictive Analytics Enhances KM.\n</h3>\n<p>\nPredictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-can-one-implement-knowledge-graphs-sarah-wakuthii-rdjxf\">\n<span>\nHow can one implement knowledge graphs?\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nHow can one implement knowledge graphs?\n</h3>\n<p>\nKnowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-challenges-cross-functional-knowledge-sharing-sarah-wakuthii-yghgf\">\n<span>\nWhat are the challenges of cross-functional knowledge sharing?\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nWhat are the challenges of cross-functional knowledge sharing?\n</h3>\n<p>\nCross-functional knowledge sharing is a pivotal trend in modern knowledge management, emphasizing the breakdown of…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-key-market-gaps-cybersecurity-one-can-fill-sarah-wakuthii-3qamf\">\n<span>\nWhat are the key market gaps in cybersecurity that one can fill?\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nWhat are the key market gaps in cybersecurity that one can fill?\n</h3>\n<p>\nKey Market Gaps in Cybersecurity: Small and Medium-Sized Enterprises (SMEs): Many SMEs lack adequate cybersecurity…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-can-one-leverage-cloud-based-cybersecurity-sarah-wakuthii-7nqnf\">\n<span>\nHow can one leverage cloud-based cybersecurity solutions?\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nHow can one leverage cloud-based cybersecurity solutions?\n</h3>\n<p>\nLeveraging Cloud-Based Cybersecurity Solutions: Implement Robust Identity and Access Management: Ensure that…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-emerging-issues-trends-cybersecurity-you-should-aware-wakuthii-pnlpf\">\n<span>\nWhat are the emerging issues and trends in cybersecurity that you should be aware of\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nWhat are the emerging issues and trends in cybersecurity that you should be aware of\n</h3>\n<p>\nEmerging Trends in Cybersecurity: Here are some of the emerging issues and trends in cybersecurity; Integration of…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-secure-your-ai-knowledge-navigating-risks-sarah-wakuthii-gh0cf\">\n<span>\nHow Secure Is Your AI Knowledge? Navigating Risks and Solutions\n</span>\n</a></p>\n<div>\n<p><span>Jun 29, 2024</span>\n</p>\n<h3>\nHow Secure Is Your",
        "html": "<div><div>\n<div>\n<article>\n<figure>\n</figure>\n<header>\n<div>\n<p><a href=\"https://ke.linkedin.com/in/sarah-wakuthii-558690199\">\n<span>\nSARAH WAKUTHII\n</span>\n</a></p><div>\n<h3>\nSARAH WAKUTHII\n</h3>\n<h4>\nTransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:\n</h4>\n<p>\nPublished Sep 11, 2023\n</p>\n</div>\n</div>\n</header>\n<div>\n<p>\n<span>In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of knowledge are pivotal in making well-informed decisions, fostering innovation, and gaining a competitive edge. However, as the landscape of knowledge management expands, so do the ethical challenges that accompany it. In this article, we explore the ethical implications of knowledge management in the digital age and how organizations can navigate these challenges to foster responsible and sustainable practices.</span>\n</p>\n<p>\n<span>The Power of Data and Knowledge;</span>\n</p>\n<p>\n<span>In the digital era, data has become a valuable commodity, and organizations strive to collect vast amounts of information to gain insights and drive strategic decision-making. The combination of data and knowledge management has the potential to revolutionize businesses, healthcare, education, and even national security. For instance, data-driven healthcare systems can enhance patient care through personalized treatment plans based on historical patient data and medical research. Similarly, businesses can optimize supply chain operations by leveraging predictive analytics to forecast demand and streamline inventory management.</span>\n</p>\n<p>\n<span>Ethical Challenges in Knowledge Management;</span>\n</p>\n<p>\n<span>Amidst the immense potential of knowledge management in the digital age, several ethical challenges have emerged. The first major concern revolves around data privacy and security. As organizations collect and store vast amounts of personal and sensitive data, they must ensure robust cybersecurity measures are in place to safeguard against breaches and unauthorized access. The misuse of data can lead to severe consequences, such as identity theft, privacy violations, and erosion of customer trust.</span>\n</p>\n<p>\n<span>Another critical ethical issue is data ownership and control. With the rise of big data and the Internet of Things (IoT), the amount of data generated from various sources is vast and diverse. Organizations must address questions surrounding data ownership rights and the responsible use of data. In cases where data is shared or sold to third parties, organizations must be transparent and obtain informed consent from individuals to protect their rights.</span>\n</p>\n<p>\n<span>Transparency and Bias in AI and Machine Learning;</span>\n</p>\n<p>\n<span>As AI and machine learning algorithms play an increasingly prominent role in knowledge management, ensuring transparency and avoiding biases become paramount. AI-driven decision-making processes must be transparent and explainable, especially in sectors such as finance, healthcare, and recruitment, where the impact on individuals' lives is significant. Understanding how AI algorithms arrive at decisions is crucial for accountability and mitigating potential biases.</span>\n</p>\n<p>\n<span>Preventing Intellectual Property Violations;</span>\n</p>\n<p>\n<span>Knowledge management often involves sharing and disseminating information. However, organizations must be cautious about the inadvertent sharing of copyrighted material or intellectual property violations. Properly attributing sources, adhering to licensing agreements, and respecting copyright laws are essential to maintain ethical knowledge management practices.</span>\n</p>\n<p>\n<span>The Challenge of Misinformation and Fake News;</span>\n</p>\n<p>\n<span>The digital era has seen an unprecedented proliferation of misinformation and fake news. As information spreads rapidly through social media and digital channels, knowledge management systems must focus on verifying the accuracy and reliability of sources before incorporating information into decision-making processes. Combating misinformation requires critical thinking and fact-checking mechanisms to ensure the dissemination of credible knowledge.</span>\n</p>\n<p>\n<span>Strategies for Ethical Knowledge Management;</span>\n</p>\n<p>\n<span>To navigate the ethical challenges in the digital era, organizations should adopt several strategies to foster responsible knowledge management practices.</span>\n</p>\n<div>\n<ol>\n<li><span>Establish Ethical Guidelines: Develop clear and comprehensive ethical guidelines that encompass data privacy, transparency, data ownership, and the responsible use of AI and machine learning technologies.</span></li>\n<li><span>Prioritize Data Security: Implement robust cybersecurity measures to safeguard sensitive data from potential breaches and unauthorized access. Encrypt data and limit access to authorized personnel only.</span></li>\n<li><span>Promote Transparency: Ensure that AI and machine learning algorithms are transparent and explainable. Communicate to stakeholders how decisions are made to build trust and accountability.</span></li>\n<li><span>Invest in Ethical Training: Train employees in ethical knowledge management practices, including data privacy, copyright compliance, and responsible AI usage.</span></li>\n<li><span>Fact-Checking and Verification: Implement fact-checking mechanisms and verification processes to ensure the accuracy and reliability of information incorporated into knowledge management systems.</span></li>\n<li><span>Engage in Continuous Ethical Review: Regularly review and update ethical guidelines to stay abreast of emerging ethical challenges in knowledge management.</span></li>\n</ol>\n</div>\n<p>\n<span>Knowledge management is a powerful tool that can transform organizations and societies in the digital era. However, it comes with ethical challenges that require careful consideration and proactive solutions. By prioritizing data privacy, promoting transparency, and investing in ethical training, organizations can navigate the ethical complexities of knowledge management and foster responsible practices. Embracing ethical knowledge management not only ensures compliance with regulations but also builds trust with stakeholders and strengthens the foundation for sustainable growth and innovation in the digital age.</span>\n</p>\n</div>\n</article>\n<div>\n<p><a href=\"https://www.linkedin.com/newsletters/knowledge-management-spotlight-7058855313378074624\">\n<span>\nKnowledge Management Spotlight\n</span>\n</a></p><p>\n</p><h3>\nKnowledge Management Spotlight\n</h3>\n<h4>\n581 followers\n</h4>\n<p></p>\n</div>\n<section>\n<h2>\nMore articles by SARAH WAKUTHII\n</h2>\n<div>\n<ul>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-predictive-analytics-enhances-km-sarah-wakuthii-ax5xf\">\n<span>\nHow Predictive Analytics Enhances KM.\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nHow Predictive Analytics Enhances KM.\n</h3>\n<p>\nPredictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-can-one-implement-knowledge-graphs-sarah-wakuthii-rdjxf\">\n<span>\nHow can one implement knowledge graphs?\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nHow can one implement knowledge graphs?\n</h3>\n<p>\nKnowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-challenges-cross-functional-knowledge-sharing-sarah-wakuthii-yghgf\">\n<span>\nWhat are the challenges of cross-functional knowledge sharing?\n</span>\n</a></p>\n<div>\n<p><span>Sep 3, 2024</span>\n</p>\n<h3>\nWhat are the challenges of cross-functional knowledge sharing?\n</h3>\n<p>\nCross-functional knowledge sharing is a pivotal trend in modern knowledge management, emphasizing the breakdown of…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-key-market-gaps-cybersecurity-one-can-fill-sarah-wakuthii-3qamf\">\n<span>\nWhat are the key market gaps in cybersecurity that one can fill?\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nWhat are the key market gaps in cybersecurity that one can fill?\n</h3>\n<p>\nKey Market Gaps in Cybersecurity: Small and Medium-Sized Enterprises (SMEs): Many SMEs lack adequate cybersecurity…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-can-one-leverage-cloud-based-cybersecurity-sarah-wakuthii-7nqnf\">\n<span>\nHow can one leverage cloud-based cybersecurity solutions?\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nHow can one leverage cloud-based cybersecurity solutions?\n</h3>\n<p>\nLeveraging Cloud-Based Cybersecurity Solutions: Implement Robust Identity and Access Management: Ensure that…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/what-emerging-issues-trends-cybersecurity-you-should-aware-wakuthii-pnlpf\">\n<span>\nWhat are the emerging issues and trends in cybersecurity that you should be aware of\n</span>\n</a></p>\n<div>\n<p><span>Sep 2, 2024</span>\n</p>\n<h3>\nWhat are the emerging issues and trends in cybersecurity that you should be aware of\n</h3>\n<p>\nEmerging Trends in Cybersecurity: Here are some of the emerging issues and trends in cybersecurity; Integration of…\n</p>\n</div>\n</div>\n</li>\n<li>\n<div>\n<p><a href=\"https://www.linkedin.com/pulse/how-secure-your-ai-knowledge-navigating-risks-sarah-wakuthii-gh0cf\">\n<span>\nHow Secure Is Your AI Knowledge? Navigating Risks and Solutions\n</span>\n</a></p>\n<div>\n<p><span>Jun 29, 2024</span>\n</p>\n<h3>\nHow Secure Is Your",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "SARAH WAKUTHIISARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of know",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "SARAH WAKUTHIISARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of know",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "SARAH WAKUTHIISARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of know",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "SARAH WAKUTHIISARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of know",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "SARAH WAKUTHIISARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "SARAH WAKUTHIITransformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:Published Sep 11, 2023",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "In the rapidly advancing digital era, knowledge management has become a cornerstone for organizations seeking to thrive in an increasingly competitive and interconnected world. The effective gathering, organization, and utilization of knowledge are pivotal in making well-informed decisions, fostering innovation, and gaining a competitive edge. However, as the landscape of knowledge management expands, so do the ethical challenges that accompany it. In this article, we explore the ethical implica",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Establish Ethical Guidelines: Develop clear and comprehensive ethical guidelines that encompass data privacy, transparency, data ownership, and the responsible use of AI and machine learning technologies.Prioritize Data Security: Implement robust cybersecurity measures to safeguard sensitive data from potential breaches and unauthorized access. Encrypt data and limit access to authorized personnel only.Promote Transparency: Ensure that AI and machine learning algorithms are transparent and expla",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Knowledge Management SpotlightKnowledge Management Spotlight581 followers",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "More articles by SARAH WAKUTHIIHow Predictive Analytics Enhances KM.Sep 3, 2024How Predictive Analytics Enhances KM.Predictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…How can one implement knowledge graphs?Sep 3, 2024How can one implement knowledge graphs?Knowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…What are the challenges of cross-functional knowledge sharing?Sep",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "How Predictive Analytics Enhances KM.Sep 3, 2024How Predictive Analytics Enhances KM.Predictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…How can one implement knowledge graphs?Sep 3, 2024How can one implement knowledge graphs?Knowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…What are the challenges of cross-functional knowledge sharing?Sep 3, 2024What are the challenges",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "How Predictive Analytics Enhances KM.Sep 3, 2024How Predictive Analytics Enhances KM.Predictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 3, 2024How Predictive Analytics Enhances KM.Predictive analytics is increasingly being integrated into knowledge management (KM) to enhance the identification…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "How can one implement knowledge graphs?Sep 3, 2024How can one implement knowledge graphs?Knowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 3, 2024How can one implement knowledge graphs?Knowledge graphs are sophisticated data structures that represent real-world entities and their relationships in a…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "What are the challenges of cross-functional knowledge sharing?Sep 3, 2024What are the challenges of cross-functional knowledge sharing?Cross-functional knowledge sharing is a pivotal trend in modern knowledge management, emphasizing the breakdown of…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 3, 2024What are the challenges of cross-functional knowledge sharing?Cross-functional knowledge sharing is a pivotal trend in modern knowledge management, emphasizing the breakdown of…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "What are the key market gaps in cybersecurity that one can fill?Sep 2, 2024What are the key market gaps in cybersecurity that one can fill?Key Market Gaps in Cybersecurity: Small and Medium-Sized Enterprises (SMEs): Many SMEs lack adequate cybersecurity…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 2, 2024What are the key market gaps in cybersecurity that one can fill?Key Market Gaps in Cybersecurity: Small and Medium-Sized Enterprises (SMEs): Many SMEs lack adequate cybersecurity…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "How can one leverage cloud-based cybersecurity solutions?Sep 2, 2024How can one leverage cloud-based cybersecurity solutions?Leveraging Cloud-Based Cybersecurity Solutions: Implement Robust Identity and Access Management: Ensure that…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 2, 2024How can one leverage cloud-based cybersecurity solutions?Leveraging Cloud-Based Cybersecurity Solutions: Implement Robust Identity and Access Management: Ensure that…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "What are the emerging issues and trends in cybersecurity that you should be aware ofSep 2, 2024What are the emerging issues and trends in cybersecurity that you should be aware ofEmerging Trends in Cybersecurity: Here are some of the emerging issues and trends in cybersecurity; Integration of…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sep 2, 2024What are the emerging issues and trends in cybersecurity that you should be aware ofEmerging Trends in Cybersecurity: Here are some of the emerging issues and trends in cybersecurity; Integration of…",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "How Secure Is Your AI Knowledge? Navigating Risks and SolutionsJun 29, 2024How Secure Is Your",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Jun 29, 2024How Secure Is Your",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h3",
              "text": "SARAH WAKUTHII",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Transformative Information Systems and Technology Expert | Software Engineer | Cybersecurity Master Knowledge Manager | Catalyzing Positive Change, Transforming Societies through Strategic Technology Innovation:",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Knowledge Management Spotlight",
              "id": ""
            },
            {
              "level": "h4",
              "text": "581 followers",
              "id": ""
            },
            {
              "level": "h2",
              "text": "More articles by SARAH WAKUTHII",
              "id": ""
            },
            {
              "level": "h3",
              "text": "How Predictive Analytics Enhances KM.",
              "id": ""
            },
            {
              "level": "h3",
              "text": "How can one implement knowledge graphs?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What are the challenges of cross-functional knowledge sharing?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What are the key market gaps in cybersecurity that one can fill?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "How can one leverage cloud-based cybersecurity solutions?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What are the emerging issues and trends in cybersecurity that you should be aware of",
              "id": ""
            },
            {
              "level": "h3",
              "text": "How Secure Is Your",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "knowledge_management"
    },
    {
      "url": "https://www.ldatschool.ca/introduction-self-regulation/",
      "title": "An Introduction to Self-Regulation",
      "author": "LDAOeng",
      "published_date": "2017-01-27T19:11:54.000Z",
      "content": {
        "text": "<div><div>\n<p><span><a href=\"https://www.ldatschool.ca/introduction-self-regulation/?wpfpaction=add&amp;postid=13362\"><i></i> Add to favorites</a></span></p><p><em>by Michael Fairbrother and Dr. Jessica Whitley</em></p>\n<h2>What is Self-regulation?</h2>\n<p>Self-regulated learning is a process that assists students in<strong> managing their thoughts, behaviours, and emotions</strong> in order to successfully navigate their learning experiences (Zumbrunn, Tadlock, &amp; Roberts, 2011). According to Canadian researcher, Shanker (2012), “self-regulation refers to a child’s ability to <strong>deal with stressors</strong> effectively and efficiently and then return to a baseline of being calmly focused and alert” (p. 5).</p>\n<p>According to many researchers (for e.g., Alexander, Entwistle, &amp; Kabbani, 2001, and O’Shaughnessy et al., 2003) self-regulation (SR) is “absolutely <strong>critical for school readiness</strong>” (Blair &amp; Diamond, 2008, p. 906) and is often linked with meta-cognition. SR and meta-cognition are related but differing constructs. Whereas metacognition has to do with knowledge and awareness of one’s cognitive strengths and weaknesses, SR is the process that creates the conditions to guide this thinking: “the ability to regulate one’s cognitive activities, underlies the executive processes and functions associated with metacognition” (Montague, 2008, p. 37).</p>\n<p>For those students entering school without strong SR skills, <strong>early intervention and instruction</strong> <strong>are</strong> <strong>essential</strong>. These students as a group are more likely to become <strong>increasingly resistant to school work</strong>, school in general and self-investment in school, resulting in a <strong>greater likelihood of dropping out</strong> (Blair &amp; Diamond, 2008). Rimm-Kaufman, Pianta and Cox’s (2001) analysis of the National Center for Early Development and Learning’s <em>Transition Practices Survey </em>(1996) sampling kindergarten teachers across the United States found that 50% of their students were experiencing difficulties that limited their abilities to learn and that most difficulties dealt with SR: “particularly problems with following directions and controlling attention” (Blair &amp; Diamond, 2008, p. 899).</p>\n<p>Developing SR skills in students is not easy. It requires that teachers help students learn how to actively monitor their own thinking, to pause and check when needed, and to make their own decisions as they are engaged in their learning activities (Westwood, 2003). It is widely held view that many learning problems are a result of students’ lack of metacognitive skill/ability: “For self-regulated learning to develop teachers need to demonstrate convincingly how to use appropriate strategies, explain in ways that students can understand, and make frequent and consistent use of metacognition and strategy training in all parts of the school curriculum” (Westwood, p. 63).</p>\n<p></p>\n<h2>General Features of Self-Regulation</h2>\n<p>There has been an explosion of research on SR over the last decade connected to various domains that affect students’ abilities to focus and achieve optimal learning in academic and social situations (Shanker, 2013). Zimmerman (1990) described self-regulated students as “distinguished by their systematic use of metacognitive, motivational, and behavioral strategies; by their systematic use of metacognitive, motivational, and behavioral strategies; by their responsiveness to feedback, regarding the effectiveness of their learning; and by their self-perceptions of academic accomplishment” (p. 14).</p>\n<h2>Domains of Self-regulation</h2>\n<p>Shanker (2013) explored SR through five domains (biological, emotional, cognitive, social, and prosocial) which can be helpful in conceptualizing SR:</p>\n<ul>\n<li>The <strong>biological domain</strong> relates to the <strong>level of energy</strong> in the human nervous system;</li>\n<li>the <strong>emotional domain</strong> is related to <strong>positive and negative feelings</strong>;</li>\n<li>the <strong>cognitive domain</strong> consists of the <strong>mental processes</strong> required for taking in and being able to use information: memory, attention, acquisition and retention of information and problem solving;</li>\n<li>the <strong>social domain</strong> relates to the <strong>child’s ability to use social cues</strong> to act in an appropriate manner and is also known as social intelligence; and</li>\n<li>the<strong> prosocial domain</strong> represents <strong>how individuals act with others in their environment</strong> and ability to promote positive social connections, friendship and empathy.</li>\n</ul>\n<p>The list of elements related to each domain represents what students would present with if they reached optimal levels of SR (see Table 1). When students are not at these levels, it can be challenging for educators to optimize students’ learning.</p>\n<p></p>\n<p><a href=\"https://www.ldatschool.ca/wp-content/uploads/2015/04/General-features-of-Self-Reg.pdf\">Click here to access this list.</a></p>\n<h2>How Self-regulation Affects Learning</h2>\n<p>Creating a Nurturing Classroom Environment (Shanker, 2013) citing recent research, suggests that student<strong> academic success can be predicted based on their ability to self-regulate</strong> (Shanker, 2013 citing Blair &amp; Diamond, 2008; Duckworth &amp; Seligman, 2005). Many students lacking SR skills tend to have learning challenges that persist through their school years, and Bodrovan and Leong (2005) remark that children unable to self-regulate at age four will likely have <strong>difficulties following teacher directions</strong> at age six.<strong> It is a myth believed by many educators that these children are immature and will grow out of their impulsive behaviours; </strong>not only will students not learn or develop SR skills on their own but they will have more opportunities to practice dysregulating behaviours (Bodrovan &amp; Leong, 2005, Shanker, 2013)[1]. Other myths include attributing lack of SR skills to conditions such as ADHD, and believing that dysregulating behaviours cannot be changed.</p>\n<p>It is suggested that even when teachers are covering the curriculum at an appropriate pace, one factor that may contribute to many students not being able to process it efficiently is their inability to effectively self-regulate: they may be not paying attention, are unable to follow instructions, and may have a hard time remembering what they just heard (Bodrovan &amp; Leong, 2005). Often these are the same children that have trouble building relationships with classmates. These difficulties result in teachers “spending more time on classroom management than teaching” (p.55).</p>\n<h2>Tips for Educators</h2>\n<p>As a teacher, focusing first on the biological domain sets the stage for learning by helping students be better prepared (Shanker, 2013) Addressing environmental variables that could cause students to be overly aroused adds an important calming element in the classroom. It is recommended that teachers reduce the stressors causing over arousal among students. The following table lists some suggestions for implementing classroom management strategies that may improve SR (see Table 2).</p>\n<table>\n<tbody>\n<tr>\n<td><strong> Enhancing the Classroom Environment</strong></td>\n<td><strong> Classroom Management</strong></td>\n</tr>\n<tr>\n<td>\n<ul>\n<li>Limit extraneous visual material (bright commercially made borders and posters, mobiles)</li>\n<li>Keep clutter to a minimum</li>\n<li>Cover bottom of desk and chair legs with tennis balls if floors are uncarpeted</li>\n<li>Arrange classroom so that noise making activities are in one corner and quiet activities are in another</li>\n<li>Use a rubber strip along bottom of door to reduce hallway noise</li>\n<li>Avoid use of noisy fans; have computers off when not using</li>\n<li>Use natural light as much as possible</li>\n<li>Plan seating adaptations that help students to remain calm and focused (e.g., using disc cushions for students with sensory-integration issues, allowing fidget toys at desks)</li>\n<li>Provide quiet, calming areas where children can go when they need to down-regulate so that they can focus and be attentive</li>\n</ul>\n</td>\n<td>\n<ul>\n<li>Try to reduce auditory distractions by using chimes or music rather than school bells or buzzers to signify transitions</li>\n<li>Keep classroom schedule predictable to help students anticipate transitions throughout the day</li>\n<li>Consider using fidget toys, exercise bands, or worry beads to help students stay calmly focussed and alert</li>\n<li>Take time to observe class over several days; note times when a number of students have trouble transitioning from learning event to another</li>\n<li>From student observations note those who experience hypo- or hyperarousal more often and for longer periods than peers; keep track of triggers and consider where to make modifications making it easier to self-regulate</li>\n<li>Provide activities that allow for student choice</li>\n<li>Make your self-regulation techniques obvious to your students using age-appropriate vocabulary to help them understand and internalize concept of self-regulation</li>\n<li>Try to establish a connection between students’ parents and caregivers so there is continuity between strategies used in and outside of the classroom</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td><em>Table 2</em>:<strong> Strategies for Enhancing Classroom Environment and Improving Classroom Management (adapted from Shanker, 2013)</strong></td>\n</tr>\n</tbody>\n</table>\n<h2>General Instructional Techniques</h2>\n<p>Instructional strategies for promoting development of academic skills across the curriculum can easily incorporate those focused on developing SR. The following list of instructional strategies were compiled from Mason (2013) and Montague (2007):</p>\n<ul>\n<li><strong>Model use of SR strategies in context</strong> of the",
        "html": "<div><div>\n<p><span><a href=\"https://www.ldatschool.ca/introduction-self-regulation/?wpfpaction=add&amp;postid=13362\"><i></i> Add to favorites</a></span></p><p><em>by Michael Fairbrother and Dr. Jessica Whitley</em></p>\n<h2>What is Self-regulation?</h2>\n<p>Self-regulated learning is a process that assists students in<strong> managing their thoughts, behaviours, and emotions</strong> in order to successfully navigate their learning experiences (Zumbrunn, Tadlock, &amp; Roberts, 2011). According to Canadian researcher, Shanker (2012), “self-regulation refers to a child’s ability to <strong>deal with stressors</strong> effectively and efficiently and then return to a baseline of being calmly focused and alert” (p. 5).</p>\n<p>According to many researchers (for e.g., Alexander, Entwistle, &amp; Kabbani, 2001, and O’Shaughnessy et al., 2003) self-regulation (SR) is “absolutely <strong>critical for school readiness</strong>” (Blair &amp; Diamond, 2008, p. 906) and is often linked with meta-cognition. SR and meta-cognition are related but differing constructs. Whereas metacognition has to do with knowledge and awareness of one’s cognitive strengths and weaknesses, SR is the process that creates the conditions to guide this thinking: “the ability to regulate one’s cognitive activities, underlies the executive processes and functions associated with metacognition” (Montague, 2008, p. 37).</p>\n<p>For those students entering school without strong SR skills, <strong>early intervention and instruction</strong> <strong>are</strong> <strong>essential</strong>. These students as a group are more likely to become <strong>increasingly resistant to school work</strong>, school in general and self-investment in school, resulting in a <strong>greater likelihood of dropping out</strong> (Blair &amp; Diamond, 2008). Rimm-Kaufman, Pianta and Cox’s (2001) analysis of the National Center for Early Development and Learning’s <em>Transition Practices Survey </em>(1996) sampling kindergarten teachers across the United States found that 50% of their students were experiencing difficulties that limited their abilities to learn and that most difficulties dealt with SR: “particularly problems with following directions and controlling attention” (Blair &amp; Diamond, 2008, p. 899).</p>\n<p>Developing SR skills in students is not easy. It requires that teachers help students learn how to actively monitor their own thinking, to pause and check when needed, and to make their own decisions as they are engaged in their learning activities (Westwood, 2003). It is widely held view that many learning problems are a result of students’ lack of metacognitive skill/ability: “For self-regulated learning to develop teachers need to demonstrate convincingly how to use appropriate strategies, explain in ways that students can understand, and make frequent and consistent use of metacognition and strategy training in all parts of the school curriculum” (Westwood, p. 63).</p>\n<p></p>\n<h2>General Features of Self-Regulation</h2>\n<p>There has been an explosion of research on SR over the last decade connected to various domains that affect students’ abilities to focus and achieve optimal learning in academic and social situations (Shanker, 2013). Zimmerman (1990) described self-regulated students as “distinguished by their systematic use of metacognitive, motivational, and behavioral strategies; by their systematic use of metacognitive, motivational, and behavioral strategies; by their responsiveness to feedback, regarding the effectiveness of their learning; and by their self-perceptions of academic accomplishment” (p. 14).</p>\n<h2>Domains of Self-regulation</h2>\n<p>Shanker (2013) explored SR through five domains (biological, emotional, cognitive, social, and prosocial) which can be helpful in conceptualizing SR:</p>\n<ul>\n<li>The <strong>biological domain</strong> relates to the <strong>level of energy</strong> in the human nervous system;</li>\n<li>the <strong>emotional domain</strong> is related to <strong>positive and negative feelings</strong>;</li>\n<li>the <strong>cognitive domain</strong> consists of the <strong>mental processes</strong> required for taking in and being able to use information: memory, attention, acquisition and retention of information and problem solving;</li>\n<li>the <strong>social domain</strong> relates to the <strong>child’s ability to use social cues</strong> to act in an appropriate manner and is also known as social intelligence; and</li>\n<li>the<strong> prosocial domain</strong> represents <strong>how individuals act with others in their environment</strong> and ability to promote positive social connections, friendship and empathy.</li>\n</ul>\n<p>The list of elements related to each domain represents what students would present with if they reached optimal levels of SR (see Table 1). When students are not at these levels, it can be challenging for educators to optimize students’ learning.</p>\n<p></p>\n<p><a href=\"https://www.ldatschool.ca/wp-content/uploads/2015/04/General-features-of-Self-Reg.pdf\">Click here to access this list.</a></p>\n<h2>How Self-regulation Affects Learning</h2>\n<p>Creating a Nurturing Classroom Environment (Shanker, 2013) citing recent research, suggests that student<strong> academic success can be predicted based on their ability to self-regulate</strong> (Shanker, 2013 citing Blair &amp; Diamond, 2008; Duckworth &amp; Seligman, 2005). Many students lacking SR skills tend to have learning challenges that persist through their school years, and Bodrovan and Leong (2005) remark that children unable to self-regulate at age four will likely have <strong>difficulties following teacher directions</strong> at age six.<strong> It is a myth believed by many educators that these children are immature and will grow out of their impulsive behaviours; </strong>not only will students not learn or develop SR skills on their own but they will have more opportunities to practice dysregulating behaviours (Bodrovan &amp; Leong, 2005, Shanker, 2013)[1]. Other myths include attributing lack of SR skills to conditions such as ADHD, and believing that dysregulating behaviours cannot be changed.</p>\n<p>It is suggested that even when teachers are covering the curriculum at an appropriate pace, one factor that may contribute to many students not being able to process it efficiently is their inability to effectively self-regulate: they may be not paying attention, are unable to follow instructions, and may have a hard time remembering what they just heard (Bodrovan &amp; Leong, 2005). Often these are the same children that have trouble building relationships with classmates. These difficulties result in teachers “spending more time on classroom management than teaching” (p.55).</p>\n<h2>Tips for Educators</h2>\n<p>As a teacher, focusing first on the biological domain sets the stage for learning by helping students be better prepared (Shanker, 2013) Addressing environmental variables that could cause students to be overly aroused adds an important calming element in the classroom. It is recommended that teachers reduce the stressors causing over arousal among students. The following table lists some suggestions for implementing classroom management strategies that may improve SR (see Table 2).</p>\n<table>\n<tbody>\n<tr>\n<td><strong> Enhancing the Classroom Environment</strong></td>\n<td><strong> Classroom Management</strong></td>\n</tr>\n<tr>\n<td>\n<ul>\n<li>Limit extraneous visual material (bright commercially made borders and posters, mobiles)</li>\n<li>Keep clutter to a minimum</li>\n<li>Cover bottom of desk and chair legs with tennis balls if floors are uncarpeted</li>\n<li>Arrange classroom so that noise making activities are in one corner and quiet activities are in another</li>\n<li>Use a rubber strip along bottom of door to reduce hallway noise</li>\n<li>Avoid use of noisy fans; have computers off when not using</li>\n<li>Use natural light as much as possible</li>\n<li>Plan seating adaptations that help students to remain calm and focused (e.g., using disc cushions for students with sensory-integration issues, allowing fidget toys at desks)</li>\n<li>Provide quiet, calming areas where children can go when they need to down-regulate so that they can focus and be attentive</li>\n</ul>\n</td>\n<td>\n<ul>\n<li>Try to reduce auditory distractions by using chimes or music rather than school bells or buzzers to signify transitions</li>\n<li>Keep classroom schedule predictable to help students anticipate transitions throughout the day</li>\n<li>Consider using fidget toys, exercise bands, or worry beads to help students stay calmly focussed and alert</li>\n<li>Take time to observe class over several days; note times when a number of students have trouble transitioning from learning event to another</li>\n<li>From student observations note those who experience hypo- or hyperarousal more often and for longer periods than peers; keep track of triggers and consider where to make modifications making it easier to self-regulate</li>\n<li>Provide activities that allow for student choice</li>\n<li>Make your self-regulation techniques obvious to your students using age-appropriate vocabulary to help them understand and internalize concept of self-regulation</li>\n<li>Try to establish a connection between students’ parents and caregivers so there is continuity between strategies used in and outside of the classroom</li>\n</ul>\n</td>\n</tr>\n<tr>\n<td><em>Table 2</em>:<strong> Strategies for Enhancing Classroom Environment and Improving Classroom Management (adapted from Shanker, 2013)</strong></td>\n</tr>\n</tbody>\n</table>\n<h2>General Instructional Techniques</h2>\n<p>Instructional strategies for promoting development of academic skills across the curriculum can easily incorporate those focused on developing SR. The following list of instructional strategies were compiled from Mason (2013) and Montague (2007):</p>\n<ul>\n<li><strong>Model use of SR strategies in context</strong> of the",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Add to favoritesby Michael Fairbrother and Dr. Jessica WhitleyWhat is Self-regulation?Self-regulated learning is a process that assists students inmanaging their thoughts, behaviours, and emotionsin order to successfully navigate their learning experiences (Zumbrunn, Tadlock, & Roberts, 2011). According to Canadian researcher, Shanker (2012), “self-regulation refers to a child’s ability todeal with stressorseffectively and efficiently and then return to a baseline of being calmly focused and ale",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Add to favoritesby Michael Fairbrother and Dr. Jessica WhitleyWhat is Self-regulation?Self-regulated learning is a process that assists students inmanaging their thoughts, behaviours, and emotionsin order to successfully navigate their learning experiences (Zumbrunn, Tadlock, & Roberts, 2011). According to Canadian researcher, Shanker (2012), “self-regulation refers to a child’s ability todeal with stressorseffectively and efficiently and then return to a baseline of being calmly focused and ale",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "What is Self-regulation?",
              "id": ""
            },
            {
              "level": "h2",
              "text": "General Features of Self-Regulation",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Domains of Self-regulation",
              "id": ""
            },
            {
              "level": "h2",
              "text": "How Self-regulation Affects Learning",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Tips for Educators",
              "id": ""
            },
            {
              "level": "h2",
              "text": "General Instructional Techniques",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3056504/",
      "title": "Neuroscience of Self and Self-Regulation",
      "author": "",
      "published_date": "2010-03-01T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<main>\n<article><section><div>\n<p>. Author manuscript; available in PMC: 2011 Mar 14.</p></div></section><section><section><h2>Abstract</h2>\n<p>As a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal norms. Second, people need to understand how others are reacting to their behavior so as to predict how others will respond to them. This necessitates a third mechanism, which detects threat, especially in complex social situations. Finally, there needs to be a mechanism for resolving discrepancies between self-knowledge and social expectations or norms, thereby motivating behavior to resolve any conflict that exists. This article reviews recent social neuroscience research on the psychological components that support the human capacity for self-regulation.</p>\n<section><p><strong>Keywords:</strong> self-awareness, theory of mind, need to belong, social neuroscience, neuroimaging, addiction</p></section></section><section><h2>Introduction</h2>\n<p>Many of the adaptive challenges facing our earliest ancestors were social in nature, such as differentiating friends from foes, identifying and evaluating potential mates, understanding the nature and structure of group relations, and so on. Those ancestors who were able to solve survival problems and adapt to their social environments were most likely to reproduce and pass along their genes. As such, humans have evolved a fundamental need to belong that encourages behaviors consistent with being a good group member (<a href=\"#R17\">Baumeister &amp; Leary 1995</a>). Belonging to a good group had considerable value, including access to shared resources, security from various threats, and even assistance with daily chores. Hence, the human brain has adapted within a complex social environment and is likely to have evolved dedicated neural mechanisms that are acutely sensitive to social context, especially for any signs that group membership is imperiled (<a href=\"#R102\">Heatherton &amp; Wheatley 2010</a>, <a href=\"#R154\">Mitchell &amp; Heatherton 2009</a>).</p>\n<section><h3>The Need for Inhibition</h3>\n<p>Being a good group member is not always easy, however. There is an inherent conflict between what is enjoyable for the individual and what is best for the group. From an individual perspective, basic motivational reward processes encourage behaviors that bring pleasure. Left to our own devices and without fear of social evaluation, we might indulge our appetites without restraint: eat as much fattening tasty food as our stomachs can hold, ingest chemical substances that activate dopamine receptors, and generally follow the hedonistic rule of doing whatever feels good. But eating more than a fair share of food or otherwise monopolizing group resources comes with a cost to other group members and thus can threaten our status in the group. Inhibitions are therefore important for harmonious social relations, and evolution has undoubtedly favored those who could control undesirable impulses.</p>\n<p>Inhibition is a core feature of self-regulation, which refers to the process by which people initiate, adjust, interrupt, stop, or otherwise change thoughts, feelings, or actions in order to effect realization of personal goals or plans or to maintain current standards (<a href=\"#R16\">Baumeister et al. 1994a</a>, <a href=\"#R15\">Baumeister &amp; Heatherton 1996</a>, <a href=\"#R37\">Carver &amp; Scheier 1998</a>). At the broadest level, self-regulation refers to intentional or purposeful acts that are directed from within the person (<a href=\"#R10\">Bandura 1989</a>). From this perspective, learning, physiology, and culture predispose certain behaviors, thoughts, or emotions in specific circumstances, but self-regulation allows people to change or overcome them. Although all humans have an impressive capacity for self-regulation, failures are common, and people lose control of their behavior in a wide variety of circumstances (<a href=\"#R15\">Baumeister &amp; Heatherton 1996</a>, <a href=\"#R16\">Baumeister et al. 1994a</a>). Such failures are an important cause of several contemporary societal problems—obesity, sexual predation, addiction, and sexual infidelity, to name but a few. That even revered figures, including Catholic priests, celebrity/sports role models, and respected political leaders, have been publicly castigated for their spectacular failures of self-control is testament to the difficulties inherent in trying to control the self. This article discusses the neural bases of fundamental components of the social brain, focusing on how having a “self” serves the basic social skills necessary for maintaining effective relations with group members.</p>\n<p>There are, of course, other important features of self-regulation, such as initiating self-regulatory efforts in order to achieve personal goals (<a href=\"#R198\">Shah 2005</a>). For example, <a href=\"#R105\">Higgins (1997)</a> distinguished self-regulatory efforts aimed at achieving desirable outcomes from those aimed at avoiding undesirable outcomes. Promotion goals are those in which people approach ideal goals with aspiration and a sense of accomplishment, focusing on potential gains. By contrast, prevention goals are those in which people try to avoid losses by playing it safe or doing what they ought to do. This framework has proven useful for understanding a great deal of social behavior, from how people behave in intergroup contexts (<a href=\"#R199\">Shah et al. 2004</a>) to how they respond to awkward interracial interactions (<a href=\"#R213\">Trawalter &amp; Richeson 2006</a>). Although understanding how people initiate behavior to attain personal goals is clearly important for many aspects of human behavior, particularly health behavior (<a href=\"#R11\">Bandura 1991</a>, <a href=\"#R37\">Carver &amp; Scheier 1998</a>, <a href=\"#R188\">Rothman et al. 2004</a>), there is not yet a substantial body of neuroscience research addressing this aspect of self-regulation (for exceptions, see <a href=\"#R46\">Cunningham et al. 2005</a>, <a href=\"#R64\">Eddington et al. 2007</a>). Accordingly, much of the focus of this article is on regulation and control of ongoing psychological activity.</p></section></section><section><h2>Components of the Social Brain</h2>\n<p>Controlling oneself to be a good group member involves an awareness of how one is thinking, feeling, or behaving and the ability to alter any of these to satisfy the standards or expectations of the group. This implies the need for at least four psychological components, the failure of any of which can lead to poor outcomes and censure from the group (<a href=\"#R96\">Heatherton 2010</a>, <a href=\"#R128\">Krendl &amp; Heatherton 2009</a>, <a href=\"#R154\">Mitchell &amp; Heatherton 2009</a>, <a href=\"#R222\">Wagner &amp; Heatherton 2010b</a>).</p>\n<section><h3>Self-Awareness</h3>\n<p>First, people need self-awareness to reflect on their behaviors, including their emotional displays, so as to judge them against group norms. An empirical understanding of the self has a long history in psychology (see <a href=\"#R14\">Baumeister 1998</a>), dating back to William James' important distinction between the self as the knower (“I”) and the self as the object that is known (“me”). In the sense of the knower, the self is the subject doing the thinking, feeling, and acting. In the sense of the objectified self, the self consists of the knowledge that people hold about themselves, as when they contemplate their best and worst qualities. The experience of self as the object of attention is the psychological state known as self-awareness, which encourages people to reflect on their actions and understand the extent to which those actions match both personal values and beliefs as well as group standards (<a href=\"#R36\">Carver &amp; Scheier 1981</a>, <a href=\"#R63\">Duval &amp; Wicklund 1972</a>). Whether certain aspects of the self, such as self-serving biases and motivations, truly are adaptive is open to some debate (<a href=\"#R131\">Leary 2004</a>), although there is considerable evidence that a symbolically representational self provided considerable advantages to humans over the course of evolution, such as facilitating communication and cooperation with group members (<a href=\"#R196\">Sedikides &amp; Skowronski 1997</a>).</p></section><section><h3>Mentalizing</h3>\n<p>Understanding that violating social norms is problematic requires people to appreciate that they are the objects of social evaluation, which in turn necessitates knowing that others are capable of making such evaluations. That is, people need the ability to infer the mental states of others to predict their actions, a skill referred to as mentalizing or having “theory of mind” (<a href=\"#R5\">Amodio &amp; Frith 2006</a>, <a href=\"#R73\">Gallagher &amp; Frith 2003</a>, <a href=\"#R151\">Mitchell 2006</a>). Mentalizing allows people to be aware that other people have thoughts and also attempt to understand the content of those thoughts. Ultimately, this allows people to empathize with observers to be able to predict their judgments or behaviors.</p></section><section><h3>Threat Detection</h3>\n<p>The ability to mentalize is crucial for the third mechanism, threat detection, which monitors the environment for any cues or other evidence of possible group exclusion. If humans have a fundamental need to belong, then there needs to be a mechanism for detecting inclusionary status (<a href=\"#R132\">Leary et al. 1995</a>, <a href=\"#R140\">Macdonald &amp; Leary 2005</a>). Indeed, feeling socially anxious or worrying about potential rejection should lead to heightened soc",
        "html": "<div><div>\n<main>\n<article><section><div>\n<p>. Author manuscript; available in PMC: 2011 Mar 14.</p></div></section><section><section><h2>Abstract</h2>\n<p>As a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal norms. Second, people need to understand how others are reacting to their behavior so as to predict how others will respond to them. This necessitates a third mechanism, which detects threat, especially in complex social situations. Finally, there needs to be a mechanism for resolving discrepancies between self-knowledge and social expectations or norms, thereby motivating behavior to resolve any conflict that exists. This article reviews recent social neuroscience research on the psychological components that support the human capacity for self-regulation.</p>\n<section><p><strong>Keywords:</strong> self-awareness, theory of mind, need to belong, social neuroscience, neuroimaging, addiction</p></section></section><section><h2>Introduction</h2>\n<p>Many of the adaptive challenges facing our earliest ancestors were social in nature, such as differentiating friends from foes, identifying and evaluating potential mates, understanding the nature and structure of group relations, and so on. Those ancestors who were able to solve survival problems and adapt to their social environments were most likely to reproduce and pass along their genes. As such, humans have evolved a fundamental need to belong that encourages behaviors consistent with being a good group member (<a href=\"#R17\">Baumeister &amp; Leary 1995</a>). Belonging to a good group had considerable value, including access to shared resources, security from various threats, and even assistance with daily chores. Hence, the human brain has adapted within a complex social environment and is likely to have evolved dedicated neural mechanisms that are acutely sensitive to social context, especially for any signs that group membership is imperiled (<a href=\"#R102\">Heatherton &amp; Wheatley 2010</a>, <a href=\"#R154\">Mitchell &amp; Heatherton 2009</a>).</p>\n<section><h3>The Need for Inhibition</h3>\n<p>Being a good group member is not always easy, however. There is an inherent conflict between what is enjoyable for the individual and what is best for the group. From an individual perspective, basic motivational reward processes encourage behaviors that bring pleasure. Left to our own devices and without fear of social evaluation, we might indulge our appetites without restraint: eat as much fattening tasty food as our stomachs can hold, ingest chemical substances that activate dopamine receptors, and generally follow the hedonistic rule of doing whatever feels good. But eating more than a fair share of food or otherwise monopolizing group resources comes with a cost to other group members and thus can threaten our status in the group. Inhibitions are therefore important for harmonious social relations, and evolution has undoubtedly favored those who could control undesirable impulses.</p>\n<p>Inhibition is a core feature of self-regulation, which refers to the process by which people initiate, adjust, interrupt, stop, or otherwise change thoughts, feelings, or actions in order to effect realization of personal goals or plans or to maintain current standards (<a href=\"#R16\">Baumeister et al. 1994a</a>, <a href=\"#R15\">Baumeister &amp; Heatherton 1996</a>, <a href=\"#R37\">Carver &amp; Scheier 1998</a>). At the broadest level, self-regulation refers to intentional or purposeful acts that are directed from within the person (<a href=\"#R10\">Bandura 1989</a>). From this perspective, learning, physiology, and culture predispose certain behaviors, thoughts, or emotions in specific circumstances, but self-regulation allows people to change or overcome them. Although all humans have an impressive capacity for self-regulation, failures are common, and people lose control of their behavior in a wide variety of circumstances (<a href=\"#R15\">Baumeister &amp; Heatherton 1996</a>, <a href=\"#R16\">Baumeister et al. 1994a</a>). Such failures are an important cause of several contemporary societal problems—obesity, sexual predation, addiction, and sexual infidelity, to name but a few. That even revered figures, including Catholic priests, celebrity/sports role models, and respected political leaders, have been publicly castigated for their spectacular failures of self-control is testament to the difficulties inherent in trying to control the self. This article discusses the neural bases of fundamental components of the social brain, focusing on how having a “self” serves the basic social skills necessary for maintaining effective relations with group members.</p>\n<p>There are, of course, other important features of self-regulation, such as initiating self-regulatory efforts in order to achieve personal goals (<a href=\"#R198\">Shah 2005</a>). For example, <a href=\"#R105\">Higgins (1997)</a> distinguished self-regulatory efforts aimed at achieving desirable outcomes from those aimed at avoiding undesirable outcomes. Promotion goals are those in which people approach ideal goals with aspiration and a sense of accomplishment, focusing on potential gains. By contrast, prevention goals are those in which people try to avoid losses by playing it safe or doing what they ought to do. This framework has proven useful for understanding a great deal of social behavior, from how people behave in intergroup contexts (<a href=\"#R199\">Shah et al. 2004</a>) to how they respond to awkward interracial interactions (<a href=\"#R213\">Trawalter &amp; Richeson 2006</a>). Although understanding how people initiate behavior to attain personal goals is clearly important for many aspects of human behavior, particularly health behavior (<a href=\"#R11\">Bandura 1991</a>, <a href=\"#R37\">Carver &amp; Scheier 1998</a>, <a href=\"#R188\">Rothman et al. 2004</a>), there is not yet a substantial body of neuroscience research addressing this aspect of self-regulation (for exceptions, see <a href=\"#R46\">Cunningham et al. 2005</a>, <a href=\"#R64\">Eddington et al. 2007</a>). Accordingly, much of the focus of this article is on regulation and control of ongoing psychological activity.</p></section></section><section><h2>Components of the Social Brain</h2>\n<p>Controlling oneself to be a good group member involves an awareness of how one is thinking, feeling, or behaving and the ability to alter any of these to satisfy the standards or expectations of the group. This implies the need for at least four psychological components, the failure of any of which can lead to poor outcomes and censure from the group (<a href=\"#R96\">Heatherton 2010</a>, <a href=\"#R128\">Krendl &amp; Heatherton 2009</a>, <a href=\"#R154\">Mitchell &amp; Heatherton 2009</a>, <a href=\"#R222\">Wagner &amp; Heatherton 2010b</a>).</p>\n<section><h3>Self-Awareness</h3>\n<p>First, people need self-awareness to reflect on their behaviors, including their emotional displays, so as to judge them against group norms. An empirical understanding of the self has a long history in psychology (see <a href=\"#R14\">Baumeister 1998</a>), dating back to William James' important distinction between the self as the knower (“I”) and the self as the object that is known (“me”). In the sense of the knower, the self is the subject doing the thinking, feeling, and acting. In the sense of the objectified self, the self consists of the knowledge that people hold about themselves, as when they contemplate their best and worst qualities. The experience of self as the object of attention is the psychological state known as self-awareness, which encourages people to reflect on their actions and understand the extent to which those actions match both personal values and beliefs as well as group standards (<a href=\"#R36\">Carver &amp; Scheier 1981</a>, <a href=\"#R63\">Duval &amp; Wicklund 1972</a>). Whether certain aspects of the self, such as self-serving biases and motivations, truly are adaptive is open to some debate (<a href=\"#R131\">Leary 2004</a>), although there is considerable evidence that a symbolically representational self provided considerable advantages to humans over the course of evolution, such as facilitating communication and cooperation with group members (<a href=\"#R196\">Sedikides &amp; Skowronski 1997</a>).</p></section><section><h3>Mentalizing</h3>\n<p>Understanding that violating social norms is problematic requires people to appreciate that they are the objects of social evaluation, which in turn necessitates knowing that others are capable of making such evaluations. That is, people need the ability to infer the mental states of others to predict their actions, a skill referred to as mentalizing or having “theory of mind” (<a href=\"#R5\">Amodio &amp; Frith 2006</a>, <a href=\"#R73\">Gallagher &amp; Frith 2003</a>, <a href=\"#R151\">Mitchell 2006</a>). Mentalizing allows people to be aware that other people have thoughts and also attempt to understand the content of those thoughts. Ultimately, this allows people to empathize with observers to be able to predict their judgments or behaviors.</p></section><section><h3>Threat Detection</h3>\n<p>The ability to mentalize is crucial for the third mechanism, threat detection, which monitors the environment for any cues or other evidence of possible group exclusion. If humans have a fundamental need to belong, then there needs to be a mechanism for detecting inclusionary status (<a href=\"#R132\">Leary et al. 1995</a>, <a href=\"#R140\">Macdonald &amp; Leary 2005</a>). Indeed, feeling socially anxious or worrying about potential rejection should lead to heightened soc",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2011 Mar 14.AbstractAs a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2011 Mar 14.AbstractAs a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal ",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": ". Author manuscript; available in PMC: 2011 Mar 14.AbstractAs a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": ". Author manuscript; available in PMC: 2011 Mar 14.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": ". Author manuscript; available in PMC: 2011 Mar 14.",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractAs a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal norms. Second, people need to understand how others",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "AbstractAs a social species, humans have a fundamental need to belong that encourages behaviors consistent with being a good group member. Being a good group member requires the capacity for self-regulation, which allows people to alter or inhibit behaviors that would place them at risk for group exclusion. Self-regulation requires four psychological components. First, people need to be aware of their behavior so as to gauge it against societal norms. Second, people need to understand how others",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Keywords:self-awareness, theory of mind, need to belong, social neuroscience, neuroimaging, addiction",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "IntroductionMany of the adaptive challenges facing our earliest ancestors were social in nature, such as differentiating friends from foes, identifying and evaluating potential mates, understanding the nature and structure of group relations, and so on. Those ancestors who were able to solve survival problems and adapt to their social environments were most likely to reproduce and pass along their genes. As such, humans have evolved a fundamental need to belong that encourages behaviors consiste",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "The Need for InhibitionBeing a good group member is not always easy, however. There is an inherent conflict between what is enjoyable for the individual and what is best for the group. From an individual perspective, basic motivational reward processes encourage behaviors that bring pleasure. Left to our own devices and without fear of social evaluation, we might indulge our appetites without restraint: eat as much fattening tasty food as our stomachs can hold, ingest chemical substances that ac",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Components of the Social BrainControlling oneself to be a good group member involves an awareness of how one is thinking, feeling, or behaving and the ability to alter any of these to satisfy the standards or expectations of the group. This implies the need for at least four psychological components, the failure of any of which can lead to poor outcomes and censure from the group (Heatherton 2010,Krendl & Heatherton 2009,Mitchell & Heatherton 2009,Wagner & Heatherton 2010b).Self-AwarenessFirst, ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Self-AwarenessFirst, people need self-awareness to reflect on their behaviors, including their emotional displays, so as to judge them against group norms. An empirical understanding of the self has a long history in psychology (seeBaumeister 1998), dating back to William James' important distinction between the self as the knower (“I”) and the self as the object that is known (“me”). In the sense of the knower, the self is the subject doing the thinking, feeling, and acting. In the sense of the",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "MentalizingUnderstanding that violating social norms is problematic requires people to appreciate that they are the objects of social evaluation, which in turn necessitates knowing that others are capable of making such evaluations. That is, people need the ability to infer the mental states of others to predict their actions, a skill referred to as mentalizing or having “theory of mind” (Amodio & Frith 2006,Gallagher & Frith 2003,Mitchell 2006). Mentalizing allows people to be aware that other ",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Threat DetectionThe ability to mentalize is crucial for the third mechanism, threat detection, which monitors the environment for any cues or other evidence of possible group exclusion. If humans have a fundamental need to belong, then there needs to be a mechanism for detecting inclusionary status (Leary et al. 1995,Macdonald & Leary 2005). Indeed, feeling socially anxious or worrying about potential rejection should lead to heightened soc",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction",
              "id": ""
            },
            {
              "level": "h3",
              "text": "The Need for Inhibition",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Components of the Social Brain",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Self-Awareness",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Mentalizing",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Threat Detection",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://tll.mit.edu/teaching-resources/how-people-learn/self-regulation/",
      "title": "regulation | Teaching + Learning Lab",
      "author": "",
      "published_date": "2024-01-24T00:00:00.000Z",
      "content": {
        "text": "<div><div>\n<nav><ol><li><a href=\"https://tll.mit.edu\">Home</a></li><li><a href=\"https://tll.mit.edu/teaching-resources/how-people-learn/\">How People Learn</a></li><li>Self-regulation</li></ol></nav>\n<figure></figure>\n<p>Self-regulated learning is not an innate ability, but rather a skill set that can be developed to help students direct themselves through the process of learning. Developing this skill set allows students to learn more effectively because they are able to set clear goals for themselves and monitor their progress based on their goals and strategies. Self-regulation allows students to become less reactive and more proactive in their learning.</p>\n<p>The self-regulated learner typically engages in a 3-part thought process:</p>\n<ol><li><strong>Plan</strong>: Set sub-goals, such as thinking about when and where to study or choosing strategies for a given assignment, exam, or assessment format.</li><li><strong>Monitor</strong>: Reflect on how effective certain locations or strategies are during studying, think about what to do when obstacles arise, and determine how good one’s understanding of the content is.</li><li><strong>Evaluate</strong>: Think about both the outcome on the assignment/assessment and the effectiveness of the strategies used, while attributing performance to effort/strategies rather than external influences.</li></ol>\n<h2><strong>Why is self-regulation important?</strong></h2>\n<p>It has been shown that students who have become more proficient in regulating their learning are able to improve their performance. This occurs through the self-reflection encouraged by self-regulated learning, which allows students to have a better idea of how to approach learning and use available resources effectively. Students who develop the skill set of regulating their learning can also gain psychological benefits, such as greater perceived control over performance and having less negative affect towards exams. Employing self-regulation techniques helps students course correct and become more independent learners, as they are able to realize which strategies are actually effective or not as they monitor their own studying.</p>\n<p>At the college level, self-regulation is even more important as classes are both more demanding and have less supervision than in secondary education. Many students enter college without a good understanding of how to learn effectively, and continuing in that way may affect their mindset towards college-level classes and lead them to feel discouraged that they just don’t “get it” unlike their peers. This is especially true as MIT is known for its rigorous academics, and it is all too easy for students to compare themselves to others. Encouraging students to develop their self-regulated learning skills can help students feel that they are on an equal playing field as their peers.</p>\n<p>During this period of remote learning, self-regulation can help students learn effectively while they are also under stress from a number of other factors not within their control amidst the pandemic. Students may also have to do more planning on their own than on-campus, due to the more unstructured nature of remote learning. Having a process like self-regulation can help students feel a sense of self-efficacy towards their learning, and this mindset is beneficial even after they graduate.</p>\n<h2><strong>Strategies</strong> to teach self-regulation</h2>\n<p>Help students actively think about strategies for more effective learning in your discipline and show how they can be used. For example, if there is an upcoming assessment, have students think about the assessment format, what resources would help them study, why they would help, and when/how students plan to use those resources.</p>\n<p>Model goal-oriented behavior for students to deliberately plan how to reach their goals, such as building in sub-steps for projects, clear deadlines, and estimated time for completing tasks. For instance, if you have a video that students will watch asynchronously, you can set a date by which students should have watched the video, and then have a small quiz or exercise for students to complete after watching the video; this will help students better plan and monitor their learning.</p>\n<p>Develop everyday discourse about self-regulation in the classroom to provide students with a foundation to talk about their own self-regulation and become more self-aware. Making it part of the discourse makes the process more transparent, instead of something where some students seem to get it, and others don’t. Below are some ways you can do this:</p>\n<ul><li>Conduct a pre-class survey to gauge how students currently approach learning. This can provide you a natural way to transition into talking about self-regulated learning strategies in the classroom and explaining how they are connected to improved learning.</li><li>State improved self-regulated learning skills as a learning goal in the class syllabus may also help make it clearer to students that you intend to work on this skillset throughout the class.</li><li>Encourage students to plan out their classwork, such as working on an assignment a bit each day before it is due rather than doing it all the day before. One way to do this is with a timesheet. An example of a timesheet can be found here in the 9.68 OCW. The idea here is that students will consciously monitor their time spent on classwork at least a few times a week and be honest with how they spent their time. By actively observing themselves, students can recognize both the amount and quality of the work they put into a class and continuously evaluate their performance. </li><li>Explicitly label and discuss strategies in the context of a classroom activity or discussion. For example, when discussing the solution to a problem or having a discussion on readings, you can have students talk about different strategies to review the material or strategies they used to read the assigned texts.</li></ul>\n<p>Give students feedback on how they are meeting course goals, and encourage them to actively think about what strategies have and haven’t worked in learning certain kinds of material. You can ask them to reflect on how they can improve their learning strategies, and provide feedback on their reflection as well.</p>\n<p>Provide students with rubrics can also develop their self-regulated learning skills, as it will allow them to be aware of what constitutes a successful assignment submission. This can then influence the learning strategies they choose to employ for an assignment and self-evaluate their progress. Additionally, if they do not get the result they had expected based on the rubric, students can reevaluate their current strategies to better achieve their goals.</p>\n<h2>References</h2>\n<p>Pintrich, Paul R. (2002). “The Role of Metacognitive Knowledge in Learning, Teaching, and Assessing”, <em>Theory Into Practice</em>, 41:4, 219-225.</p>\n<p>Chen et al. (2017). “Strategic Resource Use for Learning: A Self-Administered Intervention That Guides Self-Reflection on Effective Resource Use Enhances Academic Performance”, <em>Psychological Science</em>, Vol. 28(6), 774-785</p>\n<p>Ertmer, Peggy A. and Newby, Timothy J. (1996). “The expert learner: Strategic, self-regulated, and reflective”, <em>Instructional Science</em>, 24, 1-24. </p>\n<p>Azvedo, Roger and Cromley, Jennifer G. (2004). “Does Training on Self-Regulated Learning Facilitate Students’ Learning With Hypermedia?” <em>Journal of Educational Psychology</em>, Vol. 96, No. 3, 523-535.</p>\n<p>Zimmerman, Barry J. (2002). “Becoming a Self-Regulated Learner: An Overview”, <em>Theory Into Practice</em>, Vol. 41, No. 2, 64-70.</p>\n</div></div>",
        "html": "<div><div>\n<nav><ol><li><a href=\"https://tll.mit.edu\">Home</a></li><li><a href=\"https://tll.mit.edu/teaching-resources/how-people-learn/\">How People Learn</a></li><li>Self-regulation</li></ol></nav>\n<figure></figure>\n<p>Self-regulated learning is not an innate ability, but rather a skill set that can be developed to help students direct themselves through the process of learning. Developing this skill set allows students to learn more effectively because they are able to set clear goals for themselves and monitor their progress based on their goals and strategies. Self-regulation allows students to become less reactive and more proactive in their learning.</p>\n<p>The self-regulated learner typically engages in a 3-part thought process:</p>\n<ol><li><strong>Plan</strong>: Set sub-goals, such as thinking about when and where to study or choosing strategies for a given assignment, exam, or assessment format.</li><li><strong>Monitor</strong>: Reflect on how effective certain locations or strategies are during studying, think about what to do when obstacles arise, and determine how good one’s understanding of the content is.</li><li><strong>Evaluate</strong>: Think about both the outcome on the assignment/assessment and the effectiveness of the strategies used, while attributing performance to effort/strategies rather than external influences.</li></ol>\n<h2><strong>Why is self-regulation important?</strong></h2>\n<p>It has been shown that students who have become more proficient in regulating their learning are able to improve their performance. This occurs through the self-reflection encouraged by self-regulated learning, which allows students to have a better idea of how to approach learning and use available resources effectively. Students who develop the skill set of regulating their learning can also gain psychological benefits, such as greater perceived control over performance and having less negative affect towards exams. Employing self-regulation techniques helps students course correct and become more independent learners, as they are able to realize which strategies are actually effective or not as they monitor their own studying.</p>\n<p>At the college level, self-regulation is even more important as classes are both more demanding and have less supervision than in secondary education. Many students enter college without a good understanding of how to learn effectively, and continuing in that way may affect their mindset towards college-level classes and lead them to feel discouraged that they just don’t “get it” unlike their peers. This is especially true as MIT is known for its rigorous academics, and it is all too easy for students to compare themselves to others. Encouraging students to develop their self-regulated learning skills can help students feel that they are on an equal playing field as their peers.</p>\n<p>During this period of remote learning, self-regulation can help students learn effectively while they are also under stress from a number of other factors not within their control amidst the pandemic. Students may also have to do more planning on their own than on-campus, due to the more unstructured nature of remote learning. Having a process like self-regulation can help students feel a sense of self-efficacy towards their learning, and this mindset is beneficial even after they graduate.</p>\n<h2><strong>Strategies</strong> to teach self-regulation</h2>\n<p>Help students actively think about strategies for more effective learning in your discipline and show how they can be used. For example, if there is an upcoming assessment, have students think about the assessment format, what resources would help them study, why they would help, and when/how students plan to use those resources.</p>\n<p>Model goal-oriented behavior for students to deliberately plan how to reach their goals, such as building in sub-steps for projects, clear deadlines, and estimated time for completing tasks. For instance, if you have a video that students will watch asynchronously, you can set a date by which students should have watched the video, and then have a small quiz or exercise for students to complete after watching the video; this will help students better plan and monitor their learning.</p>\n<p>Develop everyday discourse about self-regulation in the classroom to provide students with a foundation to talk about their own self-regulation and become more self-aware. Making it part of the discourse makes the process more transparent, instead of something where some students seem to get it, and others don’t. Below are some ways you can do this:</p>\n<ul><li>Conduct a pre-class survey to gauge how students currently approach learning. This can provide you a natural way to transition into talking about self-regulated learning strategies in the classroom and explaining how they are connected to improved learning.</li><li>State improved self-regulated learning skills as a learning goal in the class syllabus may also help make it clearer to students that you intend to work on this skillset throughout the class.</li><li>Encourage students to plan out their classwork, such as working on an assignment a bit each day before it is due rather than doing it all the day before. One way to do this is with a timesheet. An example of a timesheet can be found here in the 9.68 OCW. The idea here is that students will consciously monitor their time spent on classwork at least a few times a week and be honest with how they spent their time. By actively observing themselves, students can recognize both the amount and quality of the work they put into a class and continuously evaluate their performance. </li><li>Explicitly label and discuss strategies in the context of a classroom activity or discussion. For example, when discussing the solution to a problem or having a discussion on readings, you can have students talk about different strategies to review the material or strategies they used to read the assigned texts.</li></ul>\n<p>Give students feedback on how they are meeting course goals, and encourage them to actively think about what strategies have and haven’t worked in learning certain kinds of material. You can ask them to reflect on how they can improve their learning strategies, and provide feedback on their reflection as well.</p>\n<p>Provide students with rubrics can also develop their self-regulated learning skills, as it will allow them to be aware of what constitutes a successful assignment submission. This can then influence the learning strategies they choose to employ for an assignment and self-evaluate their progress. Additionally, if they do not get the result they had expected based on the rubric, students can reevaluate their current strategies to better achieve their goals.</p>\n<h2>References</h2>\n<p>Pintrich, Paul R. (2002). “The Role of Metacognitive Knowledge in Learning, Teaching, and Assessing”, <em>Theory Into Practice</em>, 41:4, 219-225.</p>\n<p>Chen et al. (2017). “Strategic Resource Use for Learning: A Self-Administered Intervention That Guides Self-Reflection on Effective Resource Use Enhances Academic Performance”, <em>Psychological Science</em>, Vol. 28(6), 774-785</p>\n<p>Ertmer, Peggy A. and Newby, Timothy J. (1996). “The expert learner: Strategic, self-regulated, and reflective”, <em>Instructional Science</em>, 24, 1-24. </p>\n<p>Azvedo, Roger and Cromley, Jennifer G. (2004). “Does Training on Self-Regulated Learning Facilitate Students’ Learning With Hypermedia?” <em>Journal of Educational Psychology</em>, Vol. 96, No. 3, 523-535.</p>\n<p>Zimmerman, Barry J. (2002). “Becoming a Self-Regulated Learner: An Overview”, <em>Theory Into Practice</em>, Vol. 41, No. 2, 64-70.</p>\n</div></div>",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "HomeHow People LearnSelf-regulationSelf-regulated learning is not an innate ability, but rather a skill set that can be developed to help students direct themselves through the process of learning. Developing this skill set allows students to learn more effectively because they are able to set clear goals for themselves and monitor their progress based on their goals and strategies. Self-regulation allows students to become less reactive and more proactive in their learning.The self-regulated le",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "HomeHow People LearnSelf-regulationSelf-regulated learning is not an innate ability, but rather a skill set that can be developed to help students direct themselves through the process of learning. Developing this skill set allows students to learn more effectively because they are able to set clear goals for themselves and monitor their progress based on their goals and strategies. Self-regulation allows students to become less reactive and more proactive in their learning.The self-regulated le",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Why is self-regulation important?",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Strategiesto teach self-regulation",
              "id": ""
            },
            {
              "level": "h2",
              "text": "References",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
      "title": "Error detection and correction - Wikipedia",
      "author": null,
      "published_date": "2023-02-06T17:16:17.391Z",
      "content": {
        "text": "<div><div>\n<div><p><a href=\"/wiki/File:Reed%E2%80%93Solomon_error_correction_Mona_Lisa_LroLrLasercomFig4.jpg\"></a></p><div><p>To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.</p></div></div>\n<p>In <a href=\"/wiki/Information_theory\">information theory</a> and <a href=\"/wiki/Coding_theory\">coding theory</a> with applications in <a href=\"/wiki/Computer_science\">computer science</a> and <a href=\"/wiki/Telecommunication\">telecommunication</a>, <b>error detection and correction</b> (<b>EDAC</b>) or <b>error control</b> are techniques that enable <a href=\"/wiki/Reliability_(computer_networking)\">reliable delivery</a> of <a href=\"/wiki/Digital_data\">digital data</a> over unreliable <a href=\"/wiki/Communication_channel\">communication channels</a>. Many communication channels are subject to <a href=\"/wiki/Channel_noise\">channel noise</a>, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.\n</p>\n<h2>Definitions[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=1\">edit</a>]</h2>\n<p><i>Error detection</i> is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver.\n</p><p><i>Error correction</i> is the detection of errors and reconstruction of the original, error-free data.\n</p>\n<h2>History[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=2\">edit</a>]</h2>\n<p>In classical antiquity, <a href=\"/wiki/Copyist\">copyists</a> of the <a href=\"/wiki/Hebrew_Bible\">Hebrew Bible</a> were paid for their work according to the number of <a href=\"https://en.wiktionary.org/wiki/stich\">stichs</a> (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters.<sup><a href=\"#cite_note-Jewish-1\">[1]</a></sup> This also helped ensure accuracy in the transmission of the text with the production of subsequent copies.<sup><a href=\"#cite_note-2\">[2]</a></sup><sup><a href=\"#cite_note-3\">[3]</a></sup> Between the 7th and 10th centuries CE a <a href=\"/wiki/Masoretes\">group of Jewish scribes</a> formalized and expanded this to create the <a href=\"/wiki/Masoretic_Text#Numerical_Masorah\">Numerical Masorah</a> to ensure accurate reproduction of the sacred text. It included counts of the number of words in a line, section, book and groups of books, noting the middle stich of a book, word use statistics, and commentary.<sup><a href=\"#cite_note-Jewish-1\">[1]</a></sup> Standards became such that a deviation in even a single letter in a Torah scroll was considered unacceptable.<sup><a href=\"#cite_note-4\">[4]</a></sup> The effectiveness of their error correction method was verified by the accuracy of copying through the centuries demonstrated by discovery of the <a href=\"/wiki/Dead_Sea_Scrolls\">Dead Sea Scrolls</a> in 1947–1956, dating from c.150 BCE-75 CE.<sup><a href=\"#cite_note-5\">[5]</a></sup>\n</p><p>The modern development of <a href=\"/wiki/Error_correction_code\">error correction codes</a> is credited to <a href=\"/wiki/Richard_Hamming\">Richard Hamming</a> in 1947.<sup><a href=\"#cite_note-Thompson-6\">[6]</a></sup> A description of <a href=\"/wiki/Hamming_code\">Hamming's code</a> appeared in <a href=\"/wiki/Claude_Shannon\">Claude Shannon</a>'s <i>A Mathematical Theory of Communication</i><sup><a href=\"#cite_note-7\">[7]</a></sup> and was quickly generalized by <a href=\"/wiki/Marcel_J._E._Golay\">Marcel J. E. Golay</a>.<sup><a href=\"#cite_note-8\">[8]</a></sup>\n</p>\n<h2>Introduction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=3\">edit</a>]</h2>\n<p>All error-detection and correction schemes add some <a href=\"/wiki/Redundancy_(information_theory)\">redundancy</a> (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message, and to recover data that has been determined to be corrupted. Error-detection and correction schemes can be either <a href=\"/wiki/Systematic_code\">systematic</a> or non-systematic. In a systematic scheme, the transmitter sends the original data, and attaches a fixed number of <i>check bits</i> (or <i>parity data</i>), which are derived from the data bits by some <a href=\"/wiki/Deterministic_algorithm\">deterministic algorithm</a>. If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. In a system that uses a non-systematic code, the original message is transformed into an encoded message carrying the same information and that has at least as many bits as the original message.\n</p><p>Good error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common <a href=\"/wiki/Channel_model\">channel models</a> include <a href=\"/wiki/Memoryless\">memoryless</a> models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in <a href=\"/wiki/Burst_error\">bursts</a>. Consequently, error-detecting and correcting codes can be generally distinguished between <i>random-error-detecting/correcting</i> and <i>burst-error-detecting/correcting</i>. Some codes can also be suitable for a mixture of random errors and burst errors.\n</p><p>If the channel characteristics cannot be determined, or are highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as <a href=\"/wiki/Automatic_repeat_request\">automatic repeat request</a> (ARQ), and is most notably used in the Internet. An alternate approach for error control is <a href=\"/wiki/Hybrid_automatic_repeat_request\">hybrid automatic repeat request</a> (HARQ), which is a combination of ARQ and error-correction coding.\n</p>\n<h2>Types of error correction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=4\">edit</a>]</h2>\n<p>There are three major types of error correction.<sup><a href=\"#cite_note-9\">[9]</a></sup>\n</p>\n<h3>Automatic repeat request[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=5\">edit</a>]</h3>\n<p><a href=\"/wiki/Automatic_repeat_request\">Automatic repeat request</a> (ARQ) is an error control method for data transmission that makes use of error-detection codes, acknowledgment and/or negative acknowledgment messages, and <a href=\"/wiki/Timeout_(computing)\">timeouts</a> to achieve reliable data transmission. An <i>acknowledgment</i> is a message sent by the receiver to indicate that it has correctly received a <a href=\"/wiki/Frame_(networking)\">data frame</a>.\n</p><p>Usually, when the transmitter does not receive the acknowledgment before the timeout occurs (i.e., within a reasonable amount of time after sending the data frame), it retransmits the frame until it is either correctly received or the error persists beyond a predetermined number of retransmissions.\n</p><p>Three types of ARQ protocols are <a href=\"/wiki/Stop-and-wait_ARQ\">Stop-and-wait ARQ</a>, <a href=\"/wiki/Go-Back-N_ARQ\">Go-Back-N ARQ</a>, and <a href=\"/wiki/Selective_Repeat_ARQ\">Selective Repeat ARQ</a>.\n</p><p>ARQ is appropriate if the communication channel has varying or unknown <a href=\"/wiki/Channel_capacity\">capacity</a>, such as is the case on the Internet. However, ARQ requires the availability of a <a href=\"/wiki/Backward_channel\">back channel</a>, results in possibly increased <a href=\"/wiki/Latency_(engineering)\">latency</a> due to retransmissions, and requires the maintenance of buffers and timers for retransmissions, which in the case of <a href=\"/wiki/Network_congestion\">network congestion</a> can put a strain on the server and overall network capacity.<sup><a href=\"#cite_note-reliable-erasure-code-10\">[10]</a></sup>\n</p><p>For example, ARQ is used on shortwave radio data links in the form of <a href=\"/wiki/ARQ-E\">ARQ-E</a>, or combined with multiplexing as <a href=\"/wiki/ARQ-M\">ARQ-M</a>.\n</p>\n<h3>Forward error correction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=6\">edit</a>]</h3>\n<p><a href=\"/wiki/Forward_error_correction\">Forward error correction</a> (FEC) is a process of adding <a href=\"/wiki/Redundancy_(information_theory)\">redundant data</a> such as an <a href=\"/wiki/Error-correcting_code\">error-correcting code</a> (ECC) to a message so that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) are introduced, either during the process of transmission or on storage. Since the receiver does not have to ask the sender for retransmission of the data, a <a href=\"/wiki/Backchannel\">backchannel</a> is not required in forward error correction. Error-correcting codes are used in <a href=\"/wiki/Physical_layer\">lower-layer</a> communication such as <a href=\"/wiki/Cellular_network\">cellular network</a>, high-speed <a href=\"/wiki/Fiber-optic_communication\">fiber-optic communication</a> and <a href=\"/wiki/Wi-Fi\">Wi-Fi</a>,<sup><a href=\"#cite_note-11\">[11]</a></sup><sup><a href=\"#cite_note-12\">[12]</a></sup> as well as for reliable storage in media such as <a href=\"/wiki/Flash_memory\">flash memory</a>, <a href=\"/wiki/Hard_disk\">hard disk</a> and <a href=\"/wiki/ECC_memory\">RAM</a>.<sup><a href=\"#cite_note-13\">[13]</a></sup>\n</p><p>Error-correcting codes are usually distinguished between <a href=\"/wiki/Convolutional_code\">convolutional codes<",
        "html": "<div><div>\n<div><p><a href=\"/wiki/File:Reed%E2%80%93Solomon_error_correction_Mona_Lisa_LroLrLasercomFig4.jpg\"></a></p><div><p>To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.</p></div></div>\n<p>In <a href=\"/wiki/Information_theory\">information theory</a> and <a href=\"/wiki/Coding_theory\">coding theory</a> with applications in <a href=\"/wiki/Computer_science\">computer science</a> and <a href=\"/wiki/Telecommunication\">telecommunication</a>, <b>error detection and correction</b> (<b>EDAC</b>) or <b>error control</b> are techniques that enable <a href=\"/wiki/Reliability_(computer_networking)\">reliable delivery</a> of <a href=\"/wiki/Digital_data\">digital data</a> over unreliable <a href=\"/wiki/Communication_channel\">communication channels</a>. Many communication channels are subject to <a href=\"/wiki/Channel_noise\">channel noise</a>, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.\n</p>\n<h2>Definitions[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=1\">edit</a>]</h2>\n<p><i>Error detection</i> is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver.\n</p><p><i>Error correction</i> is the detection of errors and reconstruction of the original, error-free data.\n</p>\n<h2>History[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=2\">edit</a>]</h2>\n<p>In classical antiquity, <a href=\"/wiki/Copyist\">copyists</a> of the <a href=\"/wiki/Hebrew_Bible\">Hebrew Bible</a> were paid for their work according to the number of <a href=\"https://en.wiktionary.org/wiki/stich\">stichs</a> (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters.<sup><a href=\"#cite_note-Jewish-1\">[1]</a></sup> This also helped ensure accuracy in the transmission of the text with the production of subsequent copies.<sup><a href=\"#cite_note-2\">[2]</a></sup><sup><a href=\"#cite_note-3\">[3]</a></sup> Between the 7th and 10th centuries CE a <a href=\"/wiki/Masoretes\">group of Jewish scribes</a> formalized and expanded this to create the <a href=\"/wiki/Masoretic_Text#Numerical_Masorah\">Numerical Masorah</a> to ensure accurate reproduction of the sacred text. It included counts of the number of words in a line, section, book and groups of books, noting the middle stich of a book, word use statistics, and commentary.<sup><a href=\"#cite_note-Jewish-1\">[1]</a></sup> Standards became such that a deviation in even a single letter in a Torah scroll was considered unacceptable.<sup><a href=\"#cite_note-4\">[4]</a></sup> The effectiveness of their error correction method was verified by the accuracy of copying through the centuries demonstrated by discovery of the <a href=\"/wiki/Dead_Sea_Scrolls\">Dead Sea Scrolls</a> in 1947–1956, dating from c.150 BCE-75 CE.<sup><a href=\"#cite_note-5\">[5]</a></sup>\n</p><p>The modern development of <a href=\"/wiki/Error_correction_code\">error correction codes</a> is credited to <a href=\"/wiki/Richard_Hamming\">Richard Hamming</a> in 1947.<sup><a href=\"#cite_note-Thompson-6\">[6]</a></sup> A description of <a href=\"/wiki/Hamming_code\">Hamming's code</a> appeared in <a href=\"/wiki/Claude_Shannon\">Claude Shannon</a>'s <i>A Mathematical Theory of Communication</i><sup><a href=\"#cite_note-7\">[7]</a></sup> and was quickly generalized by <a href=\"/wiki/Marcel_J._E._Golay\">Marcel J. E. Golay</a>.<sup><a href=\"#cite_note-8\">[8]</a></sup>\n</p>\n<h2>Introduction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=3\">edit</a>]</h2>\n<p>All error-detection and correction schemes add some <a href=\"/wiki/Redundancy_(information_theory)\">redundancy</a> (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message, and to recover data that has been determined to be corrupted. Error-detection and correction schemes can be either <a href=\"/wiki/Systematic_code\">systematic</a> or non-systematic. In a systematic scheme, the transmitter sends the original data, and attaches a fixed number of <i>check bits</i> (or <i>parity data</i>), which are derived from the data bits by some <a href=\"/wiki/Deterministic_algorithm\">deterministic algorithm</a>. If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. In a system that uses a non-systematic code, the original message is transformed into an encoded message carrying the same information and that has at least as many bits as the original message.\n</p><p>Good error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common <a href=\"/wiki/Channel_model\">channel models</a> include <a href=\"/wiki/Memoryless\">memoryless</a> models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in <a href=\"/wiki/Burst_error\">bursts</a>. Consequently, error-detecting and correcting codes can be generally distinguished between <i>random-error-detecting/correcting</i> and <i>burst-error-detecting/correcting</i>. Some codes can also be suitable for a mixture of random errors and burst errors.\n</p><p>If the channel characteristics cannot be determined, or are highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as <a href=\"/wiki/Automatic_repeat_request\">automatic repeat request</a> (ARQ), and is most notably used in the Internet. An alternate approach for error control is <a href=\"/wiki/Hybrid_automatic_repeat_request\">hybrid automatic repeat request</a> (HARQ), which is a combination of ARQ and error-correction coding.\n</p>\n<h2>Types of error correction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=4\">edit</a>]</h2>\n<p>There are three major types of error correction.<sup><a href=\"#cite_note-9\">[9]</a></sup>\n</p>\n<h3>Automatic repeat request[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=5\">edit</a>]</h3>\n<p><a href=\"/wiki/Automatic_repeat_request\">Automatic repeat request</a> (ARQ) is an error control method for data transmission that makes use of error-detection codes, acknowledgment and/or negative acknowledgment messages, and <a href=\"/wiki/Timeout_(computing)\">timeouts</a> to achieve reliable data transmission. An <i>acknowledgment</i> is a message sent by the receiver to indicate that it has correctly received a <a href=\"/wiki/Frame_(networking)\">data frame</a>.\n</p><p>Usually, when the transmitter does not receive the acknowledgment before the timeout occurs (i.e., within a reasonable amount of time after sending the data frame), it retransmits the frame until it is either correctly received or the error persists beyond a predetermined number of retransmissions.\n</p><p>Three types of ARQ protocols are <a href=\"/wiki/Stop-and-wait_ARQ\">Stop-and-wait ARQ</a>, <a href=\"/wiki/Go-Back-N_ARQ\">Go-Back-N ARQ</a>, and <a href=\"/wiki/Selective_Repeat_ARQ\">Selective Repeat ARQ</a>.\n</p><p>ARQ is appropriate if the communication channel has varying or unknown <a href=\"/wiki/Channel_capacity\">capacity</a>, such as is the case on the Internet. However, ARQ requires the availability of a <a href=\"/wiki/Backward_channel\">back channel</a>, results in possibly increased <a href=\"/wiki/Latency_(engineering)\">latency</a> due to retransmissions, and requires the maintenance of buffers and timers for retransmissions, which in the case of <a href=\"/wiki/Network_congestion\">network congestion</a> can put a strain on the server and overall network capacity.<sup><a href=\"#cite_note-reliable-erasure-code-10\">[10]</a></sup>\n</p><p>For example, ARQ is used on shortwave radio data links in the form of <a href=\"/wiki/ARQ-E\">ARQ-E</a>, or combined with multiplexing as <a href=\"/wiki/ARQ-M\">ARQ-M</a>.\n</p>\n<h3>Forward error correction[<a href=\"/w/index.php?title=Error_detection_and_correction&amp;action=edit&amp;section=6\">edit</a>]</h3>\n<p><a href=\"/wiki/Forward_error_correction\">Forward error correction</a> (FEC) is a process of adding <a href=\"/wiki/Redundancy_(information_theory)\">redundant data</a> such as an <a href=\"/wiki/Error-correcting_code\">error-correcting code</a> (ECC) to a message so that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) are introduced, either during the process of transmission or on storage. Since the receiver does not have to ask the sender for retransmission of the data, a <a href=\"/wiki/Backchannel\">backchannel</a> is not required in forward error correction. Error-correcting codes are used in <a href=\"/wiki/Physical_layer\">lower-layer</a> communication such as <a href=\"/wiki/Cellular_network\">cellular network</a>, high-speed <a href=\"/wiki/Fiber-optic_communication\">fiber-optic communication</a> and <a href=\"/wiki/Wi-Fi\">Wi-Fi</a>,<sup><a href=\"#cite_note-11\">[11]</a></sup><sup><a href=\"#cite_note-12\">[12]</a></sup> as well as for reliable storage in media such as <a href=\"/wiki/Flash_memory\">flash memory</a>, <a href=\"/wiki/Hard_disk\">hard disk</a> and <a href=\"/wiki/ECC_memory\">RAM</a>.<sup><a href=\"#cite_note-13\">[13]</a></sup>\n</p><p>Error-correcting codes are usually distinguished between <a href=\"/wiki/Convolutional_code\">convolutional codes<",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.Ininformation theoryandcoding theorywith applications incomputer scienceandtelecommunication,error detection and correction(EDAC) orerror controlare techniques that enabler",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.Ininformation theoryandcoding theorywith applications incomputer scienceandtelecommunication,error detection and correction(EDAC) orerror controlare techniques that enabler",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "To clean up transmission errors introduced by Earth's atmosphere (left), Goddard scientists applied Reed–Solomon error correction (right), which is commonly used in CDs and DVDs. Typical errors include missing pixels (white) and false signals (black). The white stripe indicates a brief period when transmission was interrupted.",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Definitions[edit]",
              "id": ""
            },
            {
              "level": "h2",
              "text": "History[edit]",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Introduction[edit]",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Types of error correction[edit]",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Automatic repeat request[edit]",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Forward error correction[edit]",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00454/109469/A-Survey-on-Automated-Fact-Checking",
      "title": "A Survey on Automated Fact-Checking",
      "author": "Guo, Zhijiang, Schlichtkrull, Michael, Vlachos, Andreas",
      "published_date": "2022-12-23T00:00:00.000Z",
      "content": {
        "text": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00454/109469/A-Survey-on-Automated-Fact-Checking\nA Survey on Automated Fact-Checking\n2022-12-23T00:00:00Z\nGuo, Zhijiang, Schlichtkrull, Michael, Vlachos, Andreas\n<html><body><div><div>\n<p><a href=\"#\">Skip Nav Destination</a></p><div>\n<div>\n<div>\n<p><span>February 09 2022</span>\n</p>\n<div>\n<div>\n<p>Zhijiang Guo<span>, </span></p><div>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:zg283@cam.ac.uk\">zg283@cam.ac.uk</a></p>\n</div>\n<p>\nSearch for other works by this author on:\n</p>\n</div>\n</div>\n<p><span>\n</span>\n</p>\n<div>\n<div>\n<div>\n<p>\nZhijiang Guo\n<span><a>*</a></span>\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:zg283@cam.ac.uk\">zg283@cam.ac.uk</a></p>\n</div>\n</div>\n<div>\n<p>\nMichael Schlichtkrull\n<span><a>*</a></span>\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:mss84@cam.ac.uk\">mss84@cam.ac.uk</a></p>\n</div>\n</div>\n<div>\n<p>\nAndreas Vlachos\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:av308@cam.ac.uk\">av308@cam.ac.uk</a></p>\n</div>\n</div>\n</div>\n<div><p><span>*</span></p><p>The first two authors have contributed equally.</p></div>\n<div>\n<p><span>Received:</span>\n<span>June 01 2021</span>\n</p>\n<p><span>Revision Received:</span>\n<span>September 01 2021</span>\n</p>\n</div>\n<div>\n<p>© 2022 Association for Computational Linguistics.\nDistributed under a CC-BY 4.0 license.</p><p>2022</p><p>Association for Computational Linguistics. Distributed under a\nCC-BY 4.0 license.</p>\n</div>\n</div>\n<div>\n<p><em>Transactions of the Association for Computational Linguistics</em> (2022) 10: 178–206.</p>\n</div>\n</div>\n</div>\n<div>\n<ul>\n<li>\n</li>\n<li>\n<a href=\"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00454/1987018/tacl_a_00454.pdf\">\n<span>Open the </span>\n<i>\n</i><span>PDF <span>for in another window</span></span>\n</a>\n</li>\n<li>\n</li>\n<li>\n<span>\n<i><span>Views Icon</span></i>\n<span>\n<span>Views</span>\n<i></i>\n</span>\n</span>\n<ul>\n<li><span><span>Article contents</span></span></li>\n<li><span><span>Figures &amp; tables</span></span></li>\n<li><span><span>Video</span></span></li>\n<li><span><span>Audio</span></span></li>\n<li><span><span>Supplementary Data</span></span></li>\n<li><span><span>Peer Review</span></span></li>\n</ul>\n</li>\n<li>\n<span>\n<i></i>\n<span>Search Site</span>\n</span>\n</li>\n</ul>\n</div>\n<div>\n<h2>Abstract</h2>\n<section><p>Fact-checking has become increasingly important due to the speed with which both\ninformation and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity of\nclaims. In this paper, we survey automated fact-checking stemming from natural\nlanguage processing, and discuss its connections to related tasks and\ndisciplines. In this process, we present an overview of existing datasets and\nmodels, aiming to unify the various definitions given and identify common\nconcepts. Finally, we highlight challenges for future research.</p></section>\n<h2>1 Introduction</h2>\n<div>\n<p>Fact-checking is the task of assessing whether claims made in written or spoken\nlanguage are true. This is an essential task in journalism, and is commonly\nconducted manually by dedicated organizations such as PolitiFact. In addition to <em>external</em> fact-checking, <em>internal</em> fact-checking\nis also performed by publishers of newspapers, magazines, and books prior to\npublishing in order to promote truthful reporting. Figure 1 shows an example from PolitiFact, together with the evidence\n(summarized) and the verdict.</p>\n<div>\n<div><p>Figure 1: </p><div><p><a></a></p></div><p>An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said <em>“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”</em> The fact-checker gave\nthe verdict <em>False</em> based on the collected evidence.</p></div><div><p>Figure 1: </p><div><p><a></a></p></div><p>An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said <em>“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”</em> The fact-checker gave\nthe verdict <em>False</em> based on the collected evidence.</p><p><span><i> </i><span>Close modal</span></span></p></div>\n</div>\n<p>Fact-checking is a time-consuming task. To assess the claim in Figure 1, a journalist would need to search through potentially\nmany sources to find job gains under Trump and Obama, evaluate the reliability of\neach source, and make a comparison. This process can take professional fact-checkers\nseveral hours or days (Hassan et al., 2015;\nAdair et al., 2017). Compounding the\nproblem, fact-checkers often work under strict and tight deadlines, especially in\nthe case of internal processes (Borel, 2016; Godler and Reich, 2017), and\nsome studies have shown that less than half of all published articles have been\nsubject to verification (Lewis et al., 2008). Given the amount of new information that appears and the speed with\nwhich it spreads, manual validation is insufficient.</p>\n<p>Automating the fact-checking process has been discussed in the context of\ncomputational journalism (Flew et al., 2010; Cohen et al., 2011; Graves, 2018), and has received significant\nattention in the artificial intelligence community. Vlachos and Riedel (2014) proposed structuring it as a sequence\nof components—identifying claims to be checked, finding appropriate evidence,\nproducing verdicts—that can be modeled as natural language processing (NLP)\ntasks. This motivated the development of automated pipelines consisting of subtasks\nthat can be mapped to tasks well-explored in the NLP community. Advances were made\npossible by the development of datasets, consisting of either claims collected from\nfact-checking websites, for example Liar (Wang, 2017), or purpose-made for research, for example, FEVER (Thorne et al., 2018a).</p>\n<p>A growing body of research is exploring the various tasks and subtasks necessary for\nthe automation of fact checking, and to meet the need for new methods to address\nemerging challenges. Early developments were surveyed in Thorne and Vlachos (2018), which remains the closest to an\nexhaustive overview of the subject. However, their proposed framework does not\ninclude work on determining <em>which</em> claims to verify (i.e., claim\ndetection), nor does their survey include the recent work on producing explainable,\nconvincing verdicts (i.e., justification production).</p>\n<p>Several recent papers have surveyed research focusing on individual components of the\ntask. Zubiaga et al. (2018) and Islam et\nal. (2020) focus on identifying rumors on\nsocial media, Küçük and Can (2020) and Hardalov et al. (2021) on detecting the stance of a given piece of evidence towards a claim,\nand Kotonya and Toni (2020a) on producing\nexplanations and justifications for fact-checks. Finally, Nakov et al. (2021a) surveyed automated approaches to\nassist fact-checking by humans. While these surveys are extremely useful in\nunderstanding various aspects of fact-checking technology, they are fragmented and\nfocused on specific subtasks and components; our aim is to give a comprehensive and\nexhaustive birds-eye view of the subject as a whole.</p>\n<p>A number of papers have surveyed related tasks. Lazer et al. (2018) and Zhou and Zafarani (2020) surveyed work on fake news, including descriptive\nwork on the problem, as well as work seeking to counteract fake news through\ncomputational means. A comprehensive review of NLP approaches to fake news detection\nwas also provided in Oshikawa et al. (2020). However, fake news detection differs in scope from fact checking, as\nthe former focuses on assessing news articles, and includes labeling items based on\naspects not related to veracity, such as satire detection (Oshikawa et al., 2020; Zhou and Zafarani, 2020). Furthermore, other factors—such as the\naudience reached by the claim, and the intentions and forms of the claim—are\noften considered. These factors also feature in the context of propaganda detection,\nrecently surveyed by Da San Martino et al. (2020b). Unlike these efforts, the works discussed in this survey\nconcentrate on assessing veracity of general-domain claims. Finally, Shu et al.\n(2017) and da Silva et al. (2019) surveyed research on fake news\ndetection and fact checking with a focus on social media data, while this survey\ncovers fact checking across domains and sources, including newswire, science,\netc.</p>\n<p>In this survey, we present a comprehensive and up-to-date survey of automated\nfact-checking, unifying various definitions developed in previous research into a\ncommon framework. We begin by defining the three stages of our fact-checking\nframework—claim detection, evidence retrieval, and claim verification, the\nlatter consisting of verdict prediction and justification production. We then give\nan overview of the existing datasets and modeling strategies, taxonomizing these and\ncontextualizing them with respect to our framework. We finally discuss key research\nchallenges that have been addressed, and give directions for challenges that we\nbelieve should be tackled by future research. We accompany the survey with a\nrepository,<sup><span><sup>1</sup></span></sup> which lists the\nresources mentioned in our survey.</p>\n</div>\n<h2>2 Task Definition</h2>\n<div>\n<p>Figure 2 shows a NLP framework for automated\nfact-checking consisting of three stages: (i) <em>claim detection</em> to\nidentify claims that require verification; (ii) <em>evidence retrieval</em> to find sources supporting or refuting the claim; and (iii) <em>claim\nverification</em> to assess the veracity of the claim based on the ",
        "html": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00454/109469/A-Survey-on-Automated-Fact-Checking\nA Survey on Automated Fact-Checking\n2022-12-23T00:00:00Z\nGuo, Zhijiang, Schlichtkrull, Michael, Vlachos, Andreas\n<html><body><div><div>\n<p><a href=\"#\">Skip Nav Destination</a></p><div>\n<div>\n<div>\n<p><span>February 09 2022</span>\n</p>\n<div>\n<div>\n<p>Zhijiang Guo<span>, </span></p><div>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:zg283@cam.ac.uk\">zg283@cam.ac.uk</a></p>\n</div>\n<p>\nSearch for other works by this author on:\n</p>\n</div>\n</div>\n<p><span>\n</span>\n</p>\n<div>\n<div>\n<div>\n<p>\nZhijiang Guo\n<span><a>*</a></span>\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:zg283@cam.ac.uk\">zg283@cam.ac.uk</a></p>\n</div>\n</div>\n<div>\n<p>\nMichael Schlichtkrull\n<span><a>*</a></span>\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:mss84@cam.ac.uk\">mss84@cam.ac.uk</a></p>\n</div>\n</div>\n<div>\n<p>\nAndreas Vlachos\n</p>\n<div>\n<p>Department of Computer Science and Technology, University of Cambridge,\nUK. <a href=\"mailto:av308@cam.ac.uk\">av308@cam.ac.uk</a></p>\n</div>\n</div>\n</div>\n<div><p><span>*</span></p><p>The first two authors have contributed equally.</p></div>\n<div>\n<p><span>Received:</span>\n<span>June 01 2021</span>\n</p>\n<p><span>Revision Received:</span>\n<span>September 01 2021</span>\n</p>\n</div>\n<div>\n<p>© 2022 Association for Computational Linguistics.\nDistributed under a CC-BY 4.0 license.</p><p>2022</p><p>Association for Computational Linguistics. Distributed under a\nCC-BY 4.0 license.</p>\n</div>\n</div>\n<div>\n<p><em>Transactions of the Association for Computational Linguistics</em> (2022) 10: 178–206.</p>\n</div>\n</div>\n</div>\n<div>\n<ul>\n<li>\n</li>\n<li>\n<a href=\"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00454/1987018/tacl_a_00454.pdf\">\n<span>Open the </span>\n<i>\n</i><span>PDF <span>for in another window</span></span>\n</a>\n</li>\n<li>\n</li>\n<li>\n<span>\n<i><span>Views Icon</span></i>\n<span>\n<span>Views</span>\n<i></i>\n</span>\n</span>\n<ul>\n<li><span><span>Article contents</span></span></li>\n<li><span><span>Figures &amp; tables</span></span></li>\n<li><span><span>Video</span></span></li>\n<li><span><span>Audio</span></span></li>\n<li><span><span>Supplementary Data</span></span></li>\n<li><span><span>Peer Review</span></span></li>\n</ul>\n</li>\n<li>\n<span>\n<i></i>\n<span>Search Site</span>\n</span>\n</li>\n</ul>\n</div>\n<div>\n<h2>Abstract</h2>\n<section><p>Fact-checking has become increasingly important due to the speed with which both\ninformation and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity of\nclaims. In this paper, we survey automated fact-checking stemming from natural\nlanguage processing, and discuss its connections to related tasks and\ndisciplines. In this process, we present an overview of existing datasets and\nmodels, aiming to unify the various definitions given and identify common\nconcepts. Finally, we highlight challenges for future research.</p></section>\n<h2>1 Introduction</h2>\n<div>\n<p>Fact-checking is the task of assessing whether claims made in written or spoken\nlanguage are true. This is an essential task in journalism, and is commonly\nconducted manually by dedicated organizations such as PolitiFact. In addition to <em>external</em> fact-checking, <em>internal</em> fact-checking\nis also performed by publishers of newspapers, magazines, and books prior to\npublishing in order to promote truthful reporting. Figure 1 shows an example from PolitiFact, together with the evidence\n(summarized) and the verdict.</p>\n<div>\n<div><p>Figure 1: </p><div><p><a></a></p></div><p>An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said <em>“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”</em> The fact-checker gave\nthe verdict <em>False</em> based on the collected evidence.</p></div><div><p>Figure 1: </p><div><p><a></a></p></div><p>An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said <em>“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”</em> The fact-checker gave\nthe verdict <em>False</em> based on the collected evidence.</p><p><span><i> </i><span>Close modal</span></span></p></div>\n</div>\n<p>Fact-checking is a time-consuming task. To assess the claim in Figure 1, a journalist would need to search through potentially\nmany sources to find job gains under Trump and Obama, evaluate the reliability of\neach source, and make a comparison. This process can take professional fact-checkers\nseveral hours or days (Hassan et al., 2015;\nAdair et al., 2017). Compounding the\nproblem, fact-checkers often work under strict and tight deadlines, especially in\nthe case of internal processes (Borel, 2016; Godler and Reich, 2017), and\nsome studies have shown that less than half of all published articles have been\nsubject to verification (Lewis et al., 2008). Given the amount of new information that appears and the speed with\nwhich it spreads, manual validation is insufficient.</p>\n<p>Automating the fact-checking process has been discussed in the context of\ncomputational journalism (Flew et al., 2010; Cohen et al., 2011; Graves, 2018), and has received significant\nattention in the artificial intelligence community. Vlachos and Riedel (2014) proposed structuring it as a sequence\nof components—identifying claims to be checked, finding appropriate evidence,\nproducing verdicts—that can be modeled as natural language processing (NLP)\ntasks. This motivated the development of automated pipelines consisting of subtasks\nthat can be mapped to tasks well-explored in the NLP community. Advances were made\npossible by the development of datasets, consisting of either claims collected from\nfact-checking websites, for example Liar (Wang, 2017), or purpose-made for research, for example, FEVER (Thorne et al., 2018a).</p>\n<p>A growing body of research is exploring the various tasks and subtasks necessary for\nthe automation of fact checking, and to meet the need for new methods to address\nemerging challenges. Early developments were surveyed in Thorne and Vlachos (2018), which remains the closest to an\nexhaustive overview of the subject. However, their proposed framework does not\ninclude work on determining <em>which</em> claims to verify (i.e., claim\ndetection), nor does their survey include the recent work on producing explainable,\nconvincing verdicts (i.e., justification production).</p>\n<p>Several recent papers have surveyed research focusing on individual components of the\ntask. Zubiaga et al. (2018) and Islam et\nal. (2020) focus on identifying rumors on\nsocial media, Küçük and Can (2020) and Hardalov et al. (2021) on detecting the stance of a given piece of evidence towards a claim,\nand Kotonya and Toni (2020a) on producing\nexplanations and justifications for fact-checks. Finally, Nakov et al. (2021a) surveyed automated approaches to\nassist fact-checking by humans. While these surveys are extremely useful in\nunderstanding various aspects of fact-checking technology, they are fragmented and\nfocused on specific subtasks and components; our aim is to give a comprehensive and\nexhaustive birds-eye view of the subject as a whole.</p>\n<p>A number of papers have surveyed related tasks. Lazer et al. (2018) and Zhou and Zafarani (2020) surveyed work on fake news, including descriptive\nwork on the problem, as well as work seeking to counteract fake news through\ncomputational means. A comprehensive review of NLP approaches to fake news detection\nwas also provided in Oshikawa et al. (2020). However, fake news detection differs in scope from fact checking, as\nthe former focuses on assessing news articles, and includes labeling items based on\naspects not related to veracity, such as satire detection (Oshikawa et al., 2020; Zhou and Zafarani, 2020). Furthermore, other factors—such as the\naudience reached by the claim, and the intentions and forms of the claim—are\noften considered. These factors also feature in the context of propaganda detection,\nrecently surveyed by Da San Martino et al. (2020b). Unlike these efforts, the works discussed in this survey\nconcentrate on assessing veracity of general-domain claims. Finally, Shu et al.\n(2017) and da Silva et al. (2019) surveyed research on fake news\ndetection and fact checking with a focus on social media data, while this survey\ncovers fact checking across domains and sources, including newswire, science,\netc.</p>\n<p>In this survey, we present a comprehensive and up-to-date survey of automated\nfact-checking, unifying various definitions developed in previous research into a\ncommon framework. We begin by defining the three stages of our fact-checking\nframework—claim detection, evidence retrieval, and claim verification, the\nlatter consisting of verdict prediction and justification production. We then give\nan overview of the existing datasets and modeling strategies, taxonomizing these and\ncontextualizing them with respect to our framework. We finally discuss key research\nchallenges that have been addressed, and give directions for challenges that we\nbelieve should be tackled by future research. We accompany the survey with a\nrepository,<sup><span><sup>1</sup></span></sup> which lists the\nresources mentioned in our survey.</p>\n</div>\n<h2>2 Task Definition</h2>\n<div>\n<p>Figure 2 shows a NLP framework for automated\nfact-checking consisting of three stages: (i) <em>claim detection</em> to\nidentify claims that require verification; (ii) <em>evidence retrieval</em> to find sources supporting or refuting the claim; and (iii) <em>claim\nverification</em> to assess the veracity of the claim based on the ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Skip Nav DestinationFebruary 09 2022Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Skip Nav DestinationFebruary 09 2022Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "February 09 2022Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk*The first two autho",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "February 09 2022Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk*The first two autho",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "February 09 2022Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk*The first two autho",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk*The first two authors have contribu",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Zhijiang Guo,Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukSearch for other works by this author on:",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk*The first two authors have contributed equally.Received:June 01 2021Revision Received:September 01 2021© 2022 Association for Computational Linguistics.\nDistributed under a CC-BY ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.ukMichael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.ukAndreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Zhijiang Guo*Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Department of Computer Science and Technology, University of Cambridge,\nUK.zg283@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Michael Schlichtkrull*Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Department of Computer Science and Technology, University of Cambridge,\nUK.mss84@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Andreas VlachosDepartment of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Department of Computer Science and Technology, University of Cambridge,\nUK.av308@cam.ac.uk",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "*The first two authors have contributed equally.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Received:June 01 2021Revision Received:September 01 2021",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "© 2022 Association for Computational Linguistics.\nDistributed under a CC-BY 4.0 license.2022Association for Computational Linguistics. Distributed under a\nCC-BY 4.0 license.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Transactions of the Association for Computational Linguistics(2022) 10: 178–206.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Open thePDFfor in another windowViews IconViewsArticle contentsFigures & tablesVideoAudioSupplementary DataPeer ReviewSearch Site",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "AbstractFact-checking has become increasingly important due to the speed with which both\ninformation and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity of\nclaims. In this paper, we survey automated fact-checking stemming from natural\nlanguage processing, and d",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Fact-checking has become increasingly important due to the speed with which both\ninformation and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity of\nclaims. In this paper, we survey automated fact-checking stemming from natural\nlanguage processing, and discuss i",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Fact-checking is the task of assessing whether claims made in written or spoken\nlanguage are true. This is an essential task in journalism, and is commonly\nconducted manually by dedicated organizations such as PolitiFact. In addition toexternalfact-checking,internalfact-checking\nis also performed by publishers of newspapers, magazines, and books prior to\npublishing in order to promote truthful reporting. Figure 1 shows an example from PolitiFact, together with the evidence\n(summarized) and the v",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure 1:An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”The fact-checker gave\nthe verdictFalsebased on the collected evidence.Figure 1:An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”The fact-checker gave\nthe verdictFalsebased on the collected evidence.Close modal",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure 1:An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”The fact-checker gave\nthe verdictFalsebased on the collected evidence.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure 1:An example of a fact-checked statement. Referring to the manufacturing\nsector, Donald Trump said“I brought back 700,000 jobs. Obama\nand Biden brought back nothing.”The fact-checker gave\nthe verdictFalsebased on the collected evidence.Close modal",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Figure 2 shows a NLP framework for automated\nfact-checking consisting of three stages: (i)claim detectionto\nidentify claims that require verification; (ii)evidence retrievalto find sources supporting or refuting the claim; and (iii)claim\nverificationto assess the veracity of the claim based on the",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Abstract",
              "id": ""
            },
            {
              "level": "h2",
              "text": "1 Introduction",
              "id": ""
            },
            {
              "level": "h2",
              "text": "2 Task Definition",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "transcription"
    },
    {
      "url": "https://faculty.washington.edu/ajko/papers/Loksa2016SelfRegulation.pdf",
      "title": "Microsoft Word - FinalCamEdit_Medians_The Role of Self-Regulation in Programming Problem Solving Process and Success.docx",
      "author": "",
      "published_date": "",
      "content": {
        "text": "The Role of Self-Regulation in Programming Problem\nSolving Process and Success\nDastyni Loksa and Amy J. Ko\nThe Information School • DUB\nUniversity of Washington\n{dloksa, ajko}@uw.edu\nABSTRACT\nWhile prior work has investigated many aspects of programming\nproblem solving, the role of self-regulation in problem solving\nsuccess has received little attention. In this paper we contribute a\nframework for reasoning about self-regulation in programming\nproblem solving. We then use this framework to investigate how\n37 novice programmers of varying experience used self-regulation\nduring a sequence of programming problems. We analyzed the\nextent to which novices engaged in five kinds of self-regulation\nduring their problem solving, how this self-regulation varied\nbetween students enrolled in CS1 and CS2, and how self-regulation\nplayed a role in structuring problem solving. We then investigated\nthe relationship between self-regulation and programming errors.\nOur results indicate that while most novices engage in self\u0002regulation to navigate and inform their problem solving efforts,\nthese self-regulation efforts are only effective when accompanied\nby programming knowledge adequate to succeed at solving a given\nproblem, and only some types of self-regulation appeared related to\nerrors. We discuss the implications of these findings on problem\nsolving pedagogy in computing education.\nCCS Concepts\n• Social and professional topics~Computer science\neducation • Social and professional topics~CS1\nKeywords\nProgramming, Problem Solving, Self-Regulation, Think-Aloud.\n1. INTRODUCTION\nProgramming problem solving is a complex activity that poses\nmany diverse cognitive demands on learners. As Elliot Soloway\nargued thirty years ago, expert programmers “have built up large\nlibraries of stereotypical solutions to problems as well as strategies\nfor coordinating and composing them. Students should be taught\nexplicitly about these libraries and strategies for using them.” [30].\nStudents are often left to develop these strategies on their own, and\nwhen they fail to do so, they quit [2].\nPrior work has investigated a wide range of materials, pedagogies,\nand techniques for teaching programming problem solving\nstrategies. For example, recent studies have explored worked\nexamples and the effect of sub-goal labels, finding that examples\nand sub-goal labels can promote greater problem solving success\n[20,22,23]. Other efforts such as the Idea Garden have investigated\nstrategy hints, giving learners suggestions about how to approach a\nproblem (e.g., divide and conquer), finding that hints can promote\nindependence and self-efficacy [5]. Similarly, Linn & Clancy found\nthat case studies including code and expert explanations can lead to\na more integrated understanding of programming process and some\ngains in problem solving success [18].\nWhile these pedagogies and materials improve learner’s content\nknowledge for programming, prior work in the learning sciences\nliterature suggests process skills, and in particular self-regulation,\nare equally critical. Self-regulation is the ability to be aware of\none’s thoughts and actions and evaluate how well they are moving\none closer towards a goal [28]. Several studies have investigated\nself-regulation in learning, finding, for example, that successful\nlearners generate self-explanations of material and use self\u0002explanations to monitor for misconceptions [21]; that self\u0002explanation prompts can improve problem-solving skill and self\u0002efficacy [8]; that high performing CS students use more\nmetacognitive and resource management strategies [3]; and that\ngeneral metacognitive training can promote improvements in\ndomain-specific skills such as listening and science inquiry [10].\nOnly a handful of studies have explicitly investigated self\u0002regulation in the context of programming. One of the earliest was\nconducted by Clements & Gullo, investigating the effect of\nteaching programming problem solving [7]. They found that\nteaching programming via Logo, relative to teaching computer use,\nsubjectively promoted greater “reflectivity.” Pea and Kurland\nreviewed this and other work on the effects of learning to code,\nfinding little evidence that learning to code promoted self\u0002regulation or metacognition. However, they they did draw upon\nlearning sciences literature to argue that programming itself\nrequires self-regulation for planning programming solutions [24].\nThis is consistent with more recent work, identifying self-regulated\nlearning strategies [11,12], and showing that programming\nexpertise demands a high degree of self-awareness and self\u0002monitoring [9,17].\nAcknowledging that programming requires self-regulation, more\nrecent studies have investigated ways of teaching self-regulation\nfor programming. Bielaczyc et al. investigated the impact of\nteaching self-explanation, finding that students who received\nexplicit training on self-explanation strategies used these strategies\nmore than those without the training, increasing problem solving\nsuccess [4]. More recently, Loksa et al. found that combining\nsimilar self-regulation instruction with a framework for\nprogramming problem solving activities promoted not only greater\nproblem solving success, but also gains in productivity, self\u0002efficacy and growth mindset [19].\nWhile prior work provides compelling evidence that self-regulation\nis key to successful programming, it leaves several open questions:\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for\ncomponents of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to\npost on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from Permissions@acm.org.\nICER '16, September 08-12, 2016, Melbourne, VIC, Australia\n© 2016 ACM. ISBN 978-1-4503-4449-4/16/09…$15.00\nDOI: http://dx.doi.org/10.1145/2960310.2960334\n83\nMost up-to-date version: 06/24/2021\n• To what extent do novices self-regulate when programming?\n• To what extent does programming experience in CS1 and\nCS2 promote self-regulation in programming?\n• To what extent is self-regulation related to successful\nprogramming problem solving?\nIn this paper, we investigate these questions, first proposing a\ntheoretical framework for self-regulation in the context of\nprogramming. We then present an empirical analysis of novice\nprogrammers’ self-regulation activities and explore how variation\nin self-regulation was associated with problem solving success. We\nend with a discussion our findings on computing education, with\nseveral ideas for how to promote self-regulation through teaching.\n2. THEORETICAL FRAMEWORK\nLacking an existing theoretical framework of self-regulation in\ncomputing, we derive our framework from key self-regulation\nelements which are common across prior work. Prior work frames\nself-regulation as the ability to monitor and control one’s behaviors,\nthoughts, and emotions for the demands of the moment, and\nmonitoring progress toward goals [16]. In the context of learning,\nself-regulation involves metacognition (thinking about one’s\nthoughts) [10], planning (evaluating progress toward a learning\ngoal), and motivation (manipulating one’s intrinsic and extrinsic\ngoals to make progress toward learning) [31].\nIn the context of programming, we propose that self-regulation\nhelps plan and evaluate progress toward writing a program that\nsolves some computational problem. Our hypothesis is that the\nmore a participant self-regulates their programming activities, the\nmore successful they will be at solving the problem.\nThis hypothesis, however, demands a more granular view of what\n“progress” means in programming, and how self-regulation is\nrelated to progress. Loksa et al. recently defined progress in\nprogramming problem solving as six distinct and nominally\nsequential but iterative activities [19]; we propose these are related\nto self-regulation as follows:\n• Reinterpreting the problem prompt. Programming tasks\nbegin with some problem that programmers must interpret\nand clarify. As with any problem solving, this understanding\nis a cognitive representation of the problem used to organize\none’s “continuing work” [13]. The more explicitly one\nengages in regulating this understanding (e.g., by reflecting\non whether their understanding is correct), the more likely\nthey will correct misconceptions of it.\n• Searching for analogous problems. Programmers draw upon\nproblems they have encountered in the past, either in past\nprogramming efforts or even in algorithmic activities from\neveryday life [14,30]. By reusing knowledge of related\nproblems, programmers can better conceptualize a problem’s\ncomputational nuances. Learners may self-regulate by being\naware of limitations in their knowledge of related problems.\n• Searching for solutions. With some understanding of a\nproblem, programmers seek solutions that will solve the\nproblem by adapting solutions to related problems or by\nfinding solutions in textbooks, online, or from classmates or\nteachers [15]. During solution search, learners may monitor\nthe extent to which they have searched and the degree to\nwhich the search was satisfactory.\n• Evaluating a potential solution. With a solution in mind,\nprogrammers must evaluate how well it will address the\nproblem. This includes feasibility assessments, mental\nsimulations of algorithm behavior, or other techniques of\nprototyping before implementation. Self-regulation may help\nlearners to decide whether their evaluation of a potential\nsolution is adequate, or whether they need to more certainty.\n• Implementing a solution. With a solution in mind,\nprogrammers must translate the solution into code using their\nprogramming languages and tools. Learners ",
        "html": "The Role of Self-Regulation in Programming Problem\nSolving Process and Success\nDastyni Loksa and Amy J. Ko\nThe Information School • DUB\nUniversity of Washington\n{dloksa, ajko}@uw.edu\nABSTRACT\nWhile prior work has investigated many aspects of programming\nproblem solving, the role of self-regulation in problem solving\nsuccess has received little attention. In this paper we contribute a\nframework for reasoning about self-regulation in programming\nproblem solving. We then use this framework to investigate how\n37 novice programmers of varying experience used self-regulation\nduring a sequence of programming problems. We analyzed the\nextent to which novices engaged in five kinds of self-regulation\nduring their problem solving, how this self-regulation varied\nbetween students enrolled in CS1 and CS2, and how self-regulation\nplayed a role in structuring problem solving. We then investigated\nthe relationship between self-regulation and programming errors.\nOur results indicate that while most novices engage in self\u0002regulation to navigate and inform their problem solving efforts,\nthese self-regulation efforts are only effective when accompanied\nby programming knowledge adequate to succeed at solving a given\nproblem, and only some types of self-regulation appeared related to\nerrors. We discuss the implications of these findings on problem\nsolving pedagogy in computing education.\nCCS Concepts\n• Social and professional topics~Computer science\neducation • Social and professional topics~CS1\nKeywords\nProgramming, Problem Solving, Self-Regulation, Think-Aloud.\n1. INTRODUCTION\nProgramming problem solving is a complex activity that poses\nmany diverse cognitive demands on learners. As Elliot Soloway\nargued thirty years ago, expert programmers “have built up large\nlibraries of stereotypical solutions to problems as well as strategies\nfor coordinating and composing them. Students should be taught\nexplicitly about these libraries and strategies for using them.” [30].\nStudents are often left to develop these strategies on their own, and\nwhen they fail to do so, they quit [2].\nPrior work has investigated a wide range of materials, pedagogies,\nand techniques for teaching programming problem solving\nstrategies. For example, recent studies have explored worked\nexamples and the effect of sub-goal labels, finding that examples\nand sub-goal labels can promote greater problem solving success\n[20,22,23]. Other efforts such as the Idea Garden have investigated\nstrategy hints, giving learners suggestions about how to approach a\nproblem (e.g., divide and conquer), finding that hints can promote\nindependence and self-efficacy [5]. Similarly, Linn & Clancy found\nthat case studies including code and expert explanations can lead to\na more integrated understanding of programming process and some\ngains in problem solving success [18].\nWhile these pedagogies and materials improve learner’s content\nknowledge for programming, prior work in the learning sciences\nliterature suggests process skills, and in particular self-regulation,\nare equally critical. Self-regulation is the ability to be aware of\none’s thoughts and actions and evaluate how well they are moving\none closer towards a goal [28]. Several studies have investigated\nself-regulation in learning, finding, for example, that successful\nlearners generate self-explanations of material and use self\u0002explanations to monitor for misconceptions [21]; that self\u0002explanation prompts can improve problem-solving skill and self\u0002efficacy [8]; that high performing CS students use more\nmetacognitive and resource management strategies [3]; and that\ngeneral metacognitive training can promote improvements in\ndomain-specific skills such as listening and science inquiry [10].\nOnly a handful of studies have explicitly investigated self\u0002regulation in the context of programming. One of the earliest was\nconducted by Clements & Gullo, investigating the effect of\nteaching programming problem solving [7]. They found that\nteaching programming via Logo, relative to teaching computer use,\nsubjectively promoted greater “reflectivity.” Pea and Kurland\nreviewed this and other work on the effects of learning to code,\nfinding little evidence that learning to code promoted self\u0002regulation or metacognition. However, they they did draw upon\nlearning sciences literature to argue that programming itself\nrequires self-regulation for planning programming solutions [24].\nThis is consistent with more recent work, identifying self-regulated\nlearning strategies [11,12], and showing that programming\nexpertise demands a high degree of self-awareness and self\u0002monitoring [9,17].\nAcknowledging that programming requires self-regulation, more\nrecent studies have investigated ways of teaching self-regulation\nfor programming. Bielaczyc et al. investigated the impact of\nteaching self-explanation, finding that students who received\nexplicit training on self-explanation strategies used these strategies\nmore than those without the training, increasing problem solving\nsuccess [4]. More recently, Loksa et al. found that combining\nsimilar self-regulation instruction with a framework for\nprogramming problem solving activities promoted not only greater\nproblem solving success, but also gains in productivity, self\u0002efficacy and growth mindset [19].\nWhile prior work provides compelling evidence that self-regulation\nis key to successful programming, it leaves several open questions:\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for\ncomponents of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to\npost on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from Permissions@acm.org.\nICER '16, September 08-12, 2016, Melbourne, VIC, Australia\n© 2016 ACM. ISBN 978-1-4503-4449-4/16/09…$15.00\nDOI: http://dx.doi.org/10.1145/2960310.2960334\n83\nMost up-to-date version: 06/24/2021\n• To what extent do novices self-regulate when programming?\n• To what extent does programming experience in CS1 and\nCS2 promote self-regulation in programming?\n• To what extent is self-regulation related to successful\nprogramming problem solving?\nIn this paper, we investigate these questions, first proposing a\ntheoretical framework for self-regulation in the context of\nprogramming. We then present an empirical analysis of novice\nprogrammers’ self-regulation activities and explore how variation\nin self-regulation was associated with problem solving success. We\nend with a discussion our findings on computing education, with\nseveral ideas for how to promote self-regulation through teaching.\n2. THEORETICAL FRAMEWORK\nLacking an existing theoretical framework of self-regulation in\ncomputing, we derive our framework from key self-regulation\nelements which are common across prior work. Prior work frames\nself-regulation as the ability to monitor and control one’s behaviors,\nthoughts, and emotions for the demands of the moment, and\nmonitoring progress toward goals [16]. In the context of learning,\nself-regulation involves metacognition (thinking about one’s\nthoughts) [10], planning (evaluating progress toward a learning\ngoal), and motivation (manipulating one’s intrinsic and extrinsic\ngoals to make progress toward learning) [31].\nIn the context of programming, we propose that self-regulation\nhelps plan and evaluate progress toward writing a program that\nsolves some computational problem. Our hypothesis is that the\nmore a participant self-regulates their programming activities, the\nmore successful they will be at solving the problem.\nThis hypothesis, however, demands a more granular view of what\n“progress” means in programming, and how self-regulation is\nrelated to progress. Loksa et al. recently defined progress in\nprogramming problem solving as six distinct and nominally\nsequential but iterative activities [19]; we propose these are related\nto self-regulation as follows:\n• Reinterpreting the problem prompt. Programming tasks\nbegin with some problem that programmers must interpret\nand clarify. As with any problem solving, this understanding\nis a cognitive representation of the problem used to organize\none’s “continuing work” [13]. The more explicitly one\nengages in regulating this understanding (e.g., by reflecting\non whether their understanding is correct), the more likely\nthey will correct misconceptions of it.\n• Searching for analogous problems. Programmers draw upon\nproblems they have encountered in the past, either in past\nprogramming efforts or even in algorithmic activities from\neveryday life [14,30]. By reusing knowledge of related\nproblems, programmers can better conceptualize a problem’s\ncomputational nuances. Learners may self-regulate by being\naware of limitations in their knowledge of related problems.\n• Searching for solutions. With some understanding of a\nproblem, programmers seek solutions that will solve the\nproblem by adapting solutions to related problems or by\nfinding solutions in textbooks, online, or from classmates or\nteachers [15]. During solution search, learners may monitor\nthe extent to which they have searched and the degree to\nwhich the search was satisfactory.\n• Evaluating a potential solution. With a solution in mind,\nprogrammers must evaluate how well it will address the\nproblem. This includes feasibility assessments, mental\nsimulations of algorithm behavior, or other techniques of\nprototyping before implementation. Self-regulation may help\nlearners to decide whether their evaluation of a potential\nsolution is adequate, or whether they need to more certainty.\n• Implementing a solution. With a solution in mind,\nprogrammers must translate the solution into code using their\nprogramming languages and tools. Learners ",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://warwick.ac.uk/fac/soc/wbs/conf/olkc/archive/oklc3/papers/id322.pdf",
      "title": "Microsoft Word - ID322.doc",
      "author": "gpis",
      "published_date": "2002-03-15T00:00:00.000Z",
      "content": {
        "text": "1\nImproving the feedback loop between the\nknowledge business and the core business1\nMattias Strand2\nDepartment of Computer Science\nUniversity of Skövde\nBox 408, 542 28 Skövde\nSweden\nPhone: +46- (0)500–44 83 20\nmattias.strand@ida.his.se\nEva Söderström\nDepartment of Computer Science\nUniversity of Skövde\nBox 408, 542 28 Skövde\nSweden\nPhone: +46- (0)500–44 83 47\neva.soderstrom@ida.his.se\nAbstract\nKnowledge management (KM) and process orientation (PO) constitute two important parts of\ndeveloping an organisation, being the knowledge business part and the core process part. In order\nto stay competitive and to increase the customer value of their products, organisations must\nmaintain close links between these two parts. This can be made by establishing a feedback loop,\nand this loop must be bi-directed. Knowledge and KM concepts must be introduced in\norganisations, but the way of working in organisations must also affect the knowledge residing in\nthem. This paper analyses one approach taken to relate the core business and the knowledge\nbusiness, and identifies where improvements can be made to ensure the bi-directional nature of\nthe feedback loop. The use of data warehouse technology is suggested as an enabler of business\nprocess evolvement, since the feedback loop between the business processes and the KM\napplication data warehouse may be seen as a subset of activities in the feedback loop between the\ncore business and the knowledge business.\n1. Introduction\nIn order for organizations to survive in an ever-changing environment, it is important for them to\nbe competitive and to develop routines for continuous improvement of their competitiveness.\nDifferent approaches have emerged during the years to support organizations in this strive\n(Carneiro, 2000), and one approach that is currently getting a lot of attention is knowledge\nmanagement (KM). Purely introducing the KM concept in organizations will not increase the\nlevel of competitiveness in itself, but there is a need to apply the knowledge in an efficient and\n1\nThe work of this paper is sponsored by the Knowledge Foundation (Sv. KK-stiftelsen).\n2\nAuthors are given in alphabetical order.\n2\nappropriate way to enable KM activities to be successful and to contribute to the competitiveness\nof the organization. The importance of this matter is illustrated by a quotation from Claycomb et\nal. (2001, p 266):\n“Knowledge has no value if it is not applied in some way. It is only in the\napplication that it becomes valuable”\nThe quotation says that the knowledge must be applied to be valuable, and the value concept is\nstrongly related to the creation of products or services that serve to make the organization\ncompetitive. Without the creation of value, organizations would soon find themselves in a\nsituation with higher costs than incomes and this more or less automatically leads to the ruin of\nthe organization. Fortunately, much work has been devoted to knowledge applications, and there\nare numerous descriptions of it in literature pointing to different applications for KM (e.g. Meso\nand Smith, 2000; Binney, 2001). There is also much literature about how to design and\nimplement systems to support these applications (e.g. Tiwana, 2000; Tiwana and\nBalasubramaniam, 2001). However, applying knowledge in business functions and routines is\nonly one side of the “KM-coin”. The other side concerns input to the KM activities from the\nvalue-producing business processes and functions. Unfortunately, less KM literature seems to be\ndevoted to this aspect. Still, as inspired by Braf (2000), the two sides of the KM-coin may be\ndescribed as feedback loops between the core business and the knowledge business (Figure 1), in\nwhich the activities related to each side of the coin is depicted by the arrows relating the\nknowledge business with the core business and vice versa.\nFigure 1: The feedback loop between the knowledge business and the core business\nAs can be seen in Figure 1, the arrows are given different formats. The reason is to show the\npresent discrepancies in the amount of work devoted to the different sides of the KM-coin. As\ndescribed above, the application of KM in the core business seems to attract more research than\nthe inputs acquired from the core business and transferred into the knowledge business.\nTherefore, this paper suggests a way of achieving a clear connection between the core business\nand the knowledge business, with a particular focus on linking business processes with related\nknowledge management activities. We do so by describing and analyzing one existing effort that\ndescribes how the knowledge business and the core business may be linked, and by identifying\nwhere complements to this approach are needed. Furthermore, we describe how the complements\nmay be introduced through the use of data warehouse technology to support business process\nredesign, as a means to on a detailed level improve the feedback loop between the core business\nand the knowledge business.\n3\n2. Background\nThis chapter includes a brief description of the KM concept, along with a division of KM\nactivities into distinct elements as influenced by Binney (2001). This separation is important for\nthe ability to delimit the KM activities in focus in this work. Furthermore, the chapter includes a\ndescription of business processes process orientation in organizations, as well as a description of\nan approach describing the interrelationship the knowledge business and the core business is\nprovided.\n2.1. Knowledge Management\nKnowledge management is difficult to describe, since there exists many different descriptions and\ndefinitions thereof (e.g. Nonaka and Takeushi, 1995; Davenport and Prusak, 1998; and Wiig,\n1993). The fluid mix of concepts, technologies and approaches in knowledge management also\ncontributes in making the whole area almost indefinable. Instead of attempting to define KM, we\nwill describe the different elements constituting KM and how they, in different ways, contribute\nto improve the value-creating work in organizations. The motivation for this approach is that the\naim of the paper is not to define KM, but to describe different approaches on how to relate KM\napplications to the core business, i.e. to take the knowledge into action and thereby increase the\ncompetitiveness of organizations. The description will be based on the work conducted by Binney\n(2001), since his separation of the knowledge management spectrum into different elements is\nconsidered a necessity for being able to comprehend the area and the activities involved. Binney\n(2001) claims that the KM concept includes six distinct elements, each having a particular aim to\nfulfil, in order to allow organisations to cover the whole KM-spectrum. Along with the distinct\nelements, Binney (2001) exemplifies on different applications that may be included to support the\nactivities in each element (Figure 2).\nFigure 2. KM applications mapped to the elements of the KM spectrum (From Binney, 2001, p. 35)\nHowever, Figure 2 only exemplifies on suitable applications for a particular element, and does\nnot describe the core of the included elements. Therefore, the six elements (the column headings\nin Figure 2) will be briefly described below with respect to their aim and role, as described by\nBinney (2001).\nTransactional KM is focused on supporting the user in day-to-day tasks, such as completing a\ntransaction or handling a customer query, by reusing already existing knowledge. Often, the\napplication supports the user by supplying the user, which is confronted with a problem, with the\nsolution of a similar problem. Through this support, the user is able to solve regular problems in\nless time and may therefore be able to handle more transactions or to increase the quality of the\ntransactions handled.\n4\nAnalytical KM is focused on the creation of new knowledge. The core of analytical KM is the\nintegration of large amounts of data and information, from both internal and external sources,\nwhich is then used to derive trends and patterns. Those trends and patterns are previously not\nknown, due to the complexity of the sources and the diversity of data and information. Without\nthe use of data integrating applications, the user should be forced to manually acquire and\nintegrate the data, which is a time-consuming activity. In fact, the ability to automatically\nintegrate data from various types of sources serves as one of the main motivations for data\nwarehouses, since the users may perform more value-added work, instead of performing costly\nand time-consuming integration work (Connolly et al., 1999).\nAsset management KM concerns the processes associated with the management of knowledge\nassets. Asset management involves one of the following:\n1. The management of codified explicit knowledge.\n2. The management of intellectual property.\nWhen these assets have been captured, they are made available to the users in the organization.\nBinney (2001) uses the analogy of a library, since the knowledge assets, just as the books in a\nlibrary, are catalogued and made available to users. These knowledge assets are often bi-products\nto the ordinary business. This is an example of a KM activity type that is strongly related to the\nknowledge business, since it is a bi-product of the ordinary business. Still, by organizing the\nassets of the organization, it strongly contributes to decrease the time spent by users trying to find\nrelevant documents or key competencies distributed throughout the organization.\nProcess-based KM covers, as the name implies, business processes. More specifically, this\nelement is focused on the codification and improvement of processes and procedures and\nmethodology. Furthermore, process-based KM activities often origin from total quality\nmanagement (TQM) and process reengineering activities, since such activities are creating the\nbase line for improving the effectiveness of the business processes.\nDevelopmental KM focuses on increasing the compet",
        "html": "1\nImproving the feedback loop between the\nknowledge business and the core business1\nMattias Strand2\nDepartment of Computer Science\nUniversity of Skövde\nBox 408, 542 28 Skövde\nSweden\nPhone: +46- (0)500–44 83 20\nmattias.strand@ida.his.se\nEva Söderström\nDepartment of Computer Science\nUniversity of Skövde\nBox 408, 542 28 Skövde\nSweden\nPhone: +46- (0)500–44 83 47\neva.soderstrom@ida.his.se\nAbstract\nKnowledge management (KM) and process orientation (PO) constitute two important parts of\ndeveloping an organisation, being the knowledge business part and the core process part. In order\nto stay competitive and to increase the customer value of their products, organisations must\nmaintain close links between these two parts. This can be made by establishing a feedback loop,\nand this loop must be bi-directed. Knowledge and KM concepts must be introduced in\norganisations, but the way of working in organisations must also affect the knowledge residing in\nthem. This paper analyses one approach taken to relate the core business and the knowledge\nbusiness, and identifies where improvements can be made to ensure the bi-directional nature of\nthe feedback loop. The use of data warehouse technology is suggested as an enabler of business\nprocess evolvement, since the feedback loop between the business processes and the KM\napplication data warehouse may be seen as a subset of activities in the feedback loop between the\ncore business and the knowledge business.\n1. Introduction\nIn order for organizations to survive in an ever-changing environment, it is important for them to\nbe competitive and to develop routines for continuous improvement of their competitiveness.\nDifferent approaches have emerged during the years to support organizations in this strive\n(Carneiro, 2000), and one approach that is currently getting a lot of attention is knowledge\nmanagement (KM). Purely introducing the KM concept in organizations will not increase the\nlevel of competitiveness in itself, but there is a need to apply the knowledge in an efficient and\n1\nThe work of this paper is sponsored by the Knowledge Foundation (Sv. KK-stiftelsen).\n2\nAuthors are given in alphabetical order.\n2\nappropriate way to enable KM activities to be successful and to contribute to the competitiveness\nof the organization. The importance of this matter is illustrated by a quotation from Claycomb et\nal. (2001, p 266):\n“Knowledge has no value if it is not applied in some way. It is only in the\napplication that it becomes valuable”\nThe quotation says that the knowledge must be applied to be valuable, and the value concept is\nstrongly related to the creation of products or services that serve to make the organization\ncompetitive. Without the creation of value, organizations would soon find themselves in a\nsituation with higher costs than incomes and this more or less automatically leads to the ruin of\nthe organization. Fortunately, much work has been devoted to knowledge applications, and there\nare numerous descriptions of it in literature pointing to different applications for KM (e.g. Meso\nand Smith, 2000; Binney, 2001). There is also much literature about how to design and\nimplement systems to support these applications (e.g. Tiwana, 2000; Tiwana and\nBalasubramaniam, 2001). However, applying knowledge in business functions and routines is\nonly one side of the “KM-coin”. The other side concerns input to the KM activities from the\nvalue-producing business processes and functions. Unfortunately, less KM literature seems to be\ndevoted to this aspect. Still, as inspired by Braf (2000), the two sides of the KM-coin may be\ndescribed as feedback loops between the core business and the knowledge business (Figure 1), in\nwhich the activities related to each side of the coin is depicted by the arrows relating the\nknowledge business with the core business and vice versa.\nFigure 1: The feedback loop between the knowledge business and the core business\nAs can be seen in Figure 1, the arrows are given different formats. The reason is to show the\npresent discrepancies in the amount of work devoted to the different sides of the KM-coin. As\ndescribed above, the application of KM in the core business seems to attract more research than\nthe inputs acquired from the core business and transferred into the knowledge business.\nTherefore, this paper suggests a way of achieving a clear connection between the core business\nand the knowledge business, with a particular focus on linking business processes with related\nknowledge management activities. We do so by describing and analyzing one existing effort that\ndescribes how the knowledge business and the core business may be linked, and by identifying\nwhere complements to this approach are needed. Furthermore, we describe how the complements\nmay be introduced through the use of data warehouse technology to support business process\nredesign, as a means to on a detailed level improve the feedback loop between the core business\nand the knowledge business.\n3\n2. Background\nThis chapter includes a brief description of the KM concept, along with a division of KM\nactivities into distinct elements as influenced by Binney (2001). This separation is important for\nthe ability to delimit the KM activities in focus in this work. Furthermore, the chapter includes a\ndescription of business processes process orientation in organizations, as well as a description of\nan approach describing the interrelationship the knowledge business and the core business is\nprovided.\n2.1. Knowledge Management\nKnowledge management is difficult to describe, since there exists many different descriptions and\ndefinitions thereof (e.g. Nonaka and Takeushi, 1995; Davenport and Prusak, 1998; and Wiig,\n1993). The fluid mix of concepts, technologies and approaches in knowledge management also\ncontributes in making the whole area almost indefinable. Instead of attempting to define KM, we\nwill describe the different elements constituting KM and how they, in different ways, contribute\nto improve the value-creating work in organizations. The motivation for this approach is that the\naim of the paper is not to define KM, but to describe different approaches on how to relate KM\napplications to the core business, i.e. to take the knowledge into action and thereby increase the\ncompetitiveness of organizations. The description will be based on the work conducted by Binney\n(2001), since his separation of the knowledge management spectrum into different elements is\nconsidered a necessity for being able to comprehend the area and the activities involved. Binney\n(2001) claims that the KM concept includes six distinct elements, each having a particular aim to\nfulfil, in order to allow organisations to cover the whole KM-spectrum. Along with the distinct\nelements, Binney (2001) exemplifies on different applications that may be included to support the\nactivities in each element (Figure 2).\nFigure 2. KM applications mapped to the elements of the KM spectrum (From Binney, 2001, p. 35)\nHowever, Figure 2 only exemplifies on suitable applications for a particular element, and does\nnot describe the core of the included elements. Therefore, the six elements (the column headings\nin Figure 2) will be briefly described below with respect to their aim and role, as described by\nBinney (2001).\nTransactional KM is focused on supporting the user in day-to-day tasks, such as completing a\ntransaction or handling a customer query, by reusing already existing knowledge. Often, the\napplication supports the user by supplying the user, which is confronted with a problem, with the\nsolution of a similar problem. Through this support, the user is able to solve regular problems in\nless time and may therefore be able to handle more transactions or to increase the quality of the\ntransactions handled.\n4\nAnalytical KM is focused on the creation of new knowledge. The core of analytical KM is the\nintegration of large amounts of data and information, from both internal and external sources,\nwhich is then used to derive trends and patterns. Those trends and patterns are previously not\nknown, due to the complexity of the sources and the diversity of data and information. Without\nthe use of data integrating applications, the user should be forced to manually acquire and\nintegrate the data, which is a time-consuming activity. In fact, the ability to automatically\nintegrate data from various types of sources serves as one of the main motivations for data\nwarehouses, since the users may perform more value-added work, instead of performing costly\nand time-consuming integration work (Connolly et al., 1999).\nAsset management KM concerns the processes associated with the management of knowledge\nassets. Asset management involves one of the following:\n1. The management of codified explicit knowledge.\n2. The management of intellectual property.\nWhen these assets have been captured, they are made available to the users in the organization.\nBinney (2001) uses the analogy of a library, since the knowledge assets, just as the books in a\nlibrary, are catalogued and made available to users. These knowledge assets are often bi-products\nto the ordinary business. This is an example of a KM activity type that is strongly related to the\nknowledge business, since it is a bi-product of the ordinary business. Still, by organizing the\nassets of the organization, it strongly contributes to decrease the time spent by users trying to find\nrelevant documents or key competencies distributed throughout the organization.\nProcess-based KM covers, as the name implies, business processes. More specifically, this\nelement is focused on the codification and improvement of processes and procedures and\nmethodology. Furthermore, process-based KM activities often origin from total quality\nmanagement (TQM) and process reengineering activities, since such activities are creating the\nbase line for improving the effectiveness of the business processes.\nDevelopmental KM focuses on increasing the compet",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "knowledge_management"
    },
    {
      "url": "https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2018-02/graves_factsheet_180226%20FINAL.pdf",
      "title": "",
      "author": "",
      "published_date": null,
      "content": {
        "text": "| 1 |\nF A C T S H E E T\nFebruary 2018\nUnderstanding the Promise and\nLimits of Automated Fact-Checking\nAuthor: Lucas Graves\nThe last year has seen growing attention among\njournalists, policymakers, and technology companies\nto the problem of finding effective, large-scale\nresponses to online misinformation. The furore over\nso-called ‘fake news’ has exacerbated long-standing\nconcerns about political lying and online rumours\nin a fragmented media environment, sharpening\ncalls for technological solutions to what is often\nseen as a technological problem. This factsheet\ngives an overview of efforts to automatically police\nfalse political claims and misleading content online,\nhighlighting central research challenges in this area\nas well as current initiatives involving professional\nfact-checkers, platform companies, and artificial\nintelligence researchers.\nThe influence of ‘fake news’ in different parts of the\nworld remains poorly understood. Initial evidence\nfrom the US and Europe suggests that the share\nof online users who visit false news sites directly is\nquite limited, and that people exposed to these sites\nvisit mainstream news sources far more (Allcott and\nGentzkow 2017; Guess et al. 2018; Fletcher et al. 2018).\nHowever, the same studies indicate fabricated news\nstories may draw disproportionate attention on social\nnetworks, outperforming conventional news, and\nsome partisans (e.g. Trump voters in the US) appear\nto be regular users of false news sites. Little is known\nabout the dynamics by which individual viral stories\nmay influence the opinions and behaviour of specific,\ntargeted audiences around particular events or issues.\nIn the US and Europe, concern about commercially\nor politically motivated misinformation online – in\nparticular about mounting evidence of sophisticated,\nstate-backed campaigns operating from Russia –\nhas fuelled a vigorous debate over policy options.\nThese include a raft of proposals to regulate platform\ncompanies like Facebook and Google in new ways, a\nquestion under review in the European Commission.\nSeveral countries, notably Germany, France, and\nIreland, have passed or are considering legislation that\npenalises the distribution of false information.\nThese concerns have also drawn new attention to the\npotential of various automated fact-checking (AFC)\ntechnologies to combat false information online.\nHowever, deciding the truth of public claims and\nseparating legitimate views from misinformation\nis difficult and often controversial work (see Graves\n2016), challenges that carry over into AFC. Based on a\nreview of current efforts and interviews with both fact\u0002checkers and computer scientists working in this area,\nthis survey of the AFC landscape finds that:\n• Much of the terrain covered by human fact\u0002checkers requires a kind of judgement and\nsensitivity to context that remains far out of reach\nfor fully automated verification.\n• Rapid progress is being made in automatic\nverification of a narrow range of simple factual\nclaims for which authoritative data are available.\nEven here, though, AFC systems will require\nhuman supervision for the foreseeable future.\n• Both researchers and practitioners agree that\nthe real promise of AFC technologies for now\nDOI: 10.60625/risj-nqnx-bg89\nUNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING\n| 2 |\nlies in tools to assist fact-checkers to identify and\ninvestigate claims, and to deliver their conclusions\nas effectively as possible.\n• So far independent, nonprofit fact-checking\norganizations have led the way in developing\nand implementing AFC, with little activity from\ntraditional media outlets.\n• Some individual AFC tools have been built\ninexpensively by fact-checking groups. However,\nadvancing capabilities and developing large\u0002scale systems requires continuing support from\nfoundations, universities, and platform companies.\nOverview\nAFC initiatives and research generally focus on one\nor more of three overlapping objectives: to spot false\nor questionable claims circulating online and in other\nmedia; to authoritatively verify claims or stories\nthat are in doubt, or to facilitate their verification by\njournalists and members of the public; and to deliver\ncorrections instantaneously, across different media,\nto audiences exposed to misinformation. End-to\u0002end systems aim to address all three elements –\nidentification, verification, and correction (see chart).\nThe first proposals to automate online fact-checking\nappeared nearly a decade ago. Over the last several\nyears a growing research literature has embraced AFC\nas an interesting problem in artificial intelligence,\nintersecting with practical experiments by fact\u0002checkers.1\nTwo recent programming competitions,\nthe ‘Fast & Furious Fact Check Challenge’ and the\n‘Fake News Challenge’, allowed research teams from\naround the world to test different AFC techniques\non common problem sets.2 Dr Andreas Vlachos, a\nlecturer at University of Sheffield, remarks on the\nincreased attention:\nWe published our first paper in 2014. To us, apart\nfrom our interest in politics, we thought it was a great\nchallenge for artificial intelligence to actually work on\nthis problem. [But] for better or worse, Trump’s election\nincreased the interest.\nMeanwhile, real-world AFC initiatives have enjoyed a\nwave of additional funding in the last two years. Full\nFact, a London-based fact-checking charity, began\ndeveloping AFC tools in 2016 with a €50,000 grant\nfrom Google and recently announced £500,000\nadditional funding from the Omidyar Foundation and\nthe Open Society Foundations. The Duke Reporters\nLab, based at Duke University, received $1.2m in late\n2017 to launch the Tech & Check Cooperative, a hub\nfor AFC projects, from the Knight Foundation, the\nFacebook Journalism Project, and the Craig Newmark\nFoundation. In January, Factmata, a London-based\nstartup developing an AFC platform, announced $1m\nin seed funding.\n1 Useful research overviews are in Cohen et al. 2011; Hassan et al. 2017; Vlachos and Riedel 2014.\n2 See https://www.herox.com/factcheck/guidelines; http://www.fakenewschallenge.org\nCORE ELEMENTS OF AUTOMATED FACT-CHECKING\nidentification\n• monitoring media &\npolitical sources\n• identifying factual\nstatements\n• prioritising claims to\ncheck\nverification\n• checking against\nexisting fact-checks\n• checking against\nauthoritative sources\n• unstructured (e.g.\ncredibility scoring)\ncorrection\n• flagging repeated\nfalsehoods\n• providing contextual\ndata\n• publishing new fact\u0002checks\nUNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING\n| 3 |\nApproaches to AFC\nReal-world AFC efforts begin with systems to monitor\nvarious forms of public discourse – speeches, debates,\ncommentary, news reports, and so on – online and\nin traditional media. This is a difficult problem that\nmay involve scraping transcripts and other material\nfrom media or political pages, monitoring live subtitle\nfeeds, or using automatic transcription.3\nOnce monitoring is in place, the central research and\ndesign challenge revolves around the closely linked\nproblems of identifying and verifying factual claims,\nexplored below. A tension exists in that success in\nthe first complicates the second, widening the range\nof claims that must be verified. In practice, AFC\nimplementations constrain the problem by drawing\non the work of human fact-checkers and/or by sharply\nlimiting the kinds of claims being checked.\nIdentifying Claims\nThe greatest success in AFC research has come\nin the area of extracting discrete factual claims\nfrom a text such as a speech or an article. The most\ncommon approach relies on a combination of\nnatural language processing and machine learning\nto identify and prioritise claims to be checked. For\ninstance, ClaimBuster, an AFC platform developed at\nthe University of Texas-Arlington, at a cost of roughly\n$150,000 so far, trained on about 20,000 sentences\nfrom past US presidential debates, classified by paid\nhuman coders, to learn to distinguish ‘check-worthy’\nfactual claims from opinions and boring statements\n(Hassan et al. 2017). In a test during a US primary\ndebate in 2016, more than 70% of actual claims\nchecked by fact-checkers at PolitiFact and CNN\nwere among the top fifth of statements identified by\nClaimBuster.4\nA number of fact-checking outlets around the world\nhave begun relying on software to help spot claims\nto check. In the US, for instance, the Duke Reporters\nLab recently deployed a tool that uses ClaimBuster to\ndeliver potentially interesting claims to fact-checkers\nat PolitiFact, FactCheck.org, the Washington Post, and\nthe Associated Press (see the box). However, so far\nthese systems can only identify simple declarative\nstatements, missing implied claims or claims\nembedded in complex sentences which humans\nrecognise easily. This is a particular challenge with\nconversational sources, like discussion programmes,\nin which people often use pronouns and refer back to\nearlier points.\nIt is also important to note that the ‘ground truth’\nestablished by training algorithms on human work\nis neither universal not permanent. For instance,\nClaimBuster has been optimised to detect debate\nclaims and does somewhat less well harvesting\nstatements on Twitter. More broadly, the meaning and\nOne hub for automated fact-checking projects is the Duke Reporters Lab\nat Duke University. Tech & Check Alerts, in beta testing since early 2018,\nautomatically generates a daily email newsletter neatly listing 15 promising\npolitical claims harvested from transcripts of CNN programming. The\nprogramme uses the ClaimBuster API but identifies the speaker and strips\nout statements by journalists; modules are being developed to pull claims\nfrom the Congressional Record, the California legislature, and the Facebook\nfeeds of candidates in contested congressional races. Today the email goes\nout at 10 a.m. EST to PolitiFact, FactCheck.org, the Washington Post, and the\nAssociated Press. Another new project, FactStream, offers live, ‘second\u0002screen’ fact-checking of major political ",
        "html": "| 1 |\nF A C T S H E E T\nFebruary 2018\nUnderstanding the Promise and\nLimits of Automated Fact-Checking\nAuthor: Lucas Graves\nThe last year has seen growing attention among\njournalists, policymakers, and technology companies\nto the problem of finding effective, large-scale\nresponses to online misinformation. The furore over\nso-called ‘fake news’ has exacerbated long-standing\nconcerns about political lying and online rumours\nin a fragmented media environment, sharpening\ncalls for technological solutions to what is often\nseen as a technological problem. This factsheet\ngives an overview of efforts to automatically police\nfalse political claims and misleading content online,\nhighlighting central research challenges in this area\nas well as current initiatives involving professional\nfact-checkers, platform companies, and artificial\nintelligence researchers.\nThe influence of ‘fake news’ in different parts of the\nworld remains poorly understood. Initial evidence\nfrom the US and Europe suggests that the share\nof online users who visit false news sites directly is\nquite limited, and that people exposed to these sites\nvisit mainstream news sources far more (Allcott and\nGentzkow 2017; Guess et al. 2018; Fletcher et al. 2018).\nHowever, the same studies indicate fabricated news\nstories may draw disproportionate attention on social\nnetworks, outperforming conventional news, and\nsome partisans (e.g. Trump voters in the US) appear\nto be regular users of false news sites. Little is known\nabout the dynamics by which individual viral stories\nmay influence the opinions and behaviour of specific,\ntargeted audiences around particular events or issues.\nIn the US and Europe, concern about commercially\nor politically motivated misinformation online – in\nparticular about mounting evidence of sophisticated,\nstate-backed campaigns operating from Russia –\nhas fuelled a vigorous debate over policy options.\nThese include a raft of proposals to regulate platform\ncompanies like Facebook and Google in new ways, a\nquestion under review in the European Commission.\nSeveral countries, notably Germany, France, and\nIreland, have passed or are considering legislation that\npenalises the distribution of false information.\nThese concerns have also drawn new attention to the\npotential of various automated fact-checking (AFC)\ntechnologies to combat false information online.\nHowever, deciding the truth of public claims and\nseparating legitimate views from misinformation\nis difficult and often controversial work (see Graves\n2016), challenges that carry over into AFC. Based on a\nreview of current efforts and interviews with both fact\u0002checkers and computer scientists working in this area,\nthis survey of the AFC landscape finds that:\n• Much of the terrain covered by human fact\u0002checkers requires a kind of judgement and\nsensitivity to context that remains far out of reach\nfor fully automated verification.\n• Rapid progress is being made in automatic\nverification of a narrow range of simple factual\nclaims for which authoritative data are available.\nEven here, though, AFC systems will require\nhuman supervision for the foreseeable future.\n• Both researchers and practitioners agree that\nthe real promise of AFC technologies for now\nDOI: 10.60625/risj-nqnx-bg89\nUNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING\n| 2 |\nlies in tools to assist fact-checkers to identify and\ninvestigate claims, and to deliver their conclusions\nas effectively as possible.\n• So far independent, nonprofit fact-checking\norganizations have led the way in developing\nand implementing AFC, with little activity from\ntraditional media outlets.\n• Some individual AFC tools have been built\ninexpensively by fact-checking groups. However,\nadvancing capabilities and developing large\u0002scale systems requires continuing support from\nfoundations, universities, and platform companies.\nOverview\nAFC initiatives and research generally focus on one\nor more of three overlapping objectives: to spot false\nor questionable claims circulating online and in other\nmedia; to authoritatively verify claims or stories\nthat are in doubt, or to facilitate their verification by\njournalists and members of the public; and to deliver\ncorrections instantaneously, across different media,\nto audiences exposed to misinformation. End-to\u0002end systems aim to address all three elements –\nidentification, verification, and correction (see chart).\nThe first proposals to automate online fact-checking\nappeared nearly a decade ago. Over the last several\nyears a growing research literature has embraced AFC\nas an interesting problem in artificial intelligence,\nintersecting with practical experiments by fact\u0002checkers.1\nTwo recent programming competitions,\nthe ‘Fast & Furious Fact Check Challenge’ and the\n‘Fake News Challenge’, allowed research teams from\naround the world to test different AFC techniques\non common problem sets.2 Dr Andreas Vlachos, a\nlecturer at University of Sheffield, remarks on the\nincreased attention:\nWe published our first paper in 2014. To us, apart\nfrom our interest in politics, we thought it was a great\nchallenge for artificial intelligence to actually work on\nthis problem. [But] for better or worse, Trump’s election\nincreased the interest.\nMeanwhile, real-world AFC initiatives have enjoyed a\nwave of additional funding in the last two years. Full\nFact, a London-based fact-checking charity, began\ndeveloping AFC tools in 2016 with a €50,000 grant\nfrom Google and recently announced £500,000\nadditional funding from the Omidyar Foundation and\nthe Open Society Foundations. The Duke Reporters\nLab, based at Duke University, received $1.2m in late\n2017 to launch the Tech & Check Cooperative, a hub\nfor AFC projects, from the Knight Foundation, the\nFacebook Journalism Project, and the Craig Newmark\nFoundation. In January, Factmata, a London-based\nstartup developing an AFC platform, announced $1m\nin seed funding.\n1 Useful research overviews are in Cohen et al. 2011; Hassan et al. 2017; Vlachos and Riedel 2014.\n2 See https://www.herox.com/factcheck/guidelines; http://www.fakenewschallenge.org\nCORE ELEMENTS OF AUTOMATED FACT-CHECKING\nidentification\n• monitoring media &\npolitical sources\n• identifying factual\nstatements\n• prioritising claims to\ncheck\nverification\n• checking against\nexisting fact-checks\n• checking against\nauthoritative sources\n• unstructured (e.g.\ncredibility scoring)\ncorrection\n• flagging repeated\nfalsehoods\n• providing contextual\ndata\n• publishing new fact\u0002checks\nUNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING\n| 3 |\nApproaches to AFC\nReal-world AFC efforts begin with systems to monitor\nvarious forms of public discourse – speeches, debates,\ncommentary, news reports, and so on – online and\nin traditional media. This is a difficult problem that\nmay involve scraping transcripts and other material\nfrom media or political pages, monitoring live subtitle\nfeeds, or using automatic transcription.3\nOnce monitoring is in place, the central research and\ndesign challenge revolves around the closely linked\nproblems of identifying and verifying factual claims,\nexplored below. A tension exists in that success in\nthe first complicates the second, widening the range\nof claims that must be verified. In practice, AFC\nimplementations constrain the problem by drawing\non the work of human fact-checkers and/or by sharply\nlimiting the kinds of claims being checked.\nIdentifying Claims\nThe greatest success in AFC research has come\nin the area of extracting discrete factual claims\nfrom a text such as a speech or an article. The most\ncommon approach relies on a combination of\nnatural language processing and machine learning\nto identify and prioritise claims to be checked. For\ninstance, ClaimBuster, an AFC platform developed at\nthe University of Texas-Arlington, at a cost of roughly\n$150,000 so far, trained on about 20,000 sentences\nfrom past US presidential debates, classified by paid\nhuman coders, to learn to distinguish ‘check-worthy’\nfactual claims from opinions and boring statements\n(Hassan et al. 2017). In a test during a US primary\ndebate in 2016, more than 70% of actual claims\nchecked by fact-checkers at PolitiFact and CNN\nwere among the top fifth of statements identified by\nClaimBuster.4\nA number of fact-checking outlets around the world\nhave begun relying on software to help spot claims\nto check. In the US, for instance, the Duke Reporters\nLab recently deployed a tool that uses ClaimBuster to\ndeliver potentially interesting claims to fact-checkers\nat PolitiFact, FactCheck.org, the Washington Post, and\nthe Associated Press (see the box). However, so far\nthese systems can only identify simple declarative\nstatements, missing implied claims or claims\nembedded in complex sentences which humans\nrecognise easily. This is a particular challenge with\nconversational sources, like discussion programmes,\nin which people often use pronouns and refer back to\nearlier points.\nIt is also important to note that the ‘ground truth’\nestablished by training algorithms on human work\nis neither universal not permanent. For instance,\nClaimBuster has been optimised to detect debate\nclaims and does somewhat less well harvesting\nstatements on Twitter. More broadly, the meaning and\nOne hub for automated fact-checking projects is the Duke Reporters Lab\nat Duke University. Tech & Check Alerts, in beta testing since early 2018,\nautomatically generates a daily email newsletter neatly listing 15 promising\npolitical claims harvested from transcripts of CNN programming. The\nprogramme uses the ClaimBuster API but identifies the speaker and strips\nout statements by journalists; modules are being developed to pull claims\nfrom the Congressional Record, the California legislature, and the Facebook\nfeeds of candidates in contested congressional races. Today the email goes\nout at 10 a.m. EST to PolitiFact, FactCheck.org, the Washington Post, and the\nAssociated Press. Another new project, FactStream, offers live, ‘second\u0002screen’ fact-checking of major political ",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://www.youtube.com/watch?v=idmrNQy09Aw",
      "title": "Michael Vasmer: Single-shot quantum error correction with the three-dimensional subsystem toric code",
      "author": "Jens Eisert",
      "published_date": "2021-07-22T09:34:43.000Z",
      "content": {
        "text": "This is a talk on single-shot quantum error correction with the three-dimensional subsystem toric code held by Michael Vasmer in the quantum error correction reading group of the QMIO team in Berlin on July 13, 2021.\n| view_count: 424 views | short_view_count: 424 views | num_likes: 11 | num_subscribers: 875",
        "html": "This is a talk on single-shot quantum error correction with the three-dimensional subsystem toric code held by Michael Vasmer in the quantum error correction reading group of the QMIO team in Berlin on July 13, 2021.\n| view_count: 424 views | short_view_count: 424 views | num_likes: 11 | num_subscribers: 875",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "technical"
    },
    {
      "url": "https://tevera.com/blog/effective-feedback-loops-in-college-programs/",
      "title": "Effective Feedback Loops in College Programs",
      "author": "Tevera",
      "published_date": "2023-08-14T16:35:32.000Z",
      "content": {
        "text": "<div><div>\n<div><ul><li><span></span></li></ul></div><div><div><div><div><p></p><h2>Effective Feedback Loops in College Programs: Bridging the Gaps</h2>\n<p></p></div><div><h3>What are Feedback Loops?</h3>\n<p><span>Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and program administrators.</span></p>\n</div></div><div><h3><i></i>Feedback Loop Stakeholders</h3>\n<p>Feedback loops create communication bridges between all of your program’s key stakeholders. To be effective, everyone must participate. Here’s a look at the roles each stakeholder plays along with their feedback mechanisms and tasks.</p>\n<h4>Program Leaders (Deans and Program Chairs)</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role</b><span>: Program leaders set the vision, goals, and standards for the program.</span></li>\n<li><b>Feedback Mechanism:</b><span> Feedback mechanisms include regular meetings with faculty, field supervisors, and program administrators to gather insights on program performance, challenges in fieldwork, and potential areas of improvement.</span></li>\n<li><b>Tasks:</b><span> Program leaders regularly adjust program content, liaise with accrediting bodies, and mobilize resources based on feedback.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Faculty</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role: </b><span>Faculty are the builders and project managers of feedback loops as they design and deliver the curriculum and guide students before, during, and after their field experience.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include surveys and direct interactions with students pre and post fieldwork. Collaborative meetings with field educators to understand practical challenges and curriculum alignment.</span></li>\n<li><b>Tasks: </b><span>Faculty codify curriculum based on fieldwork insights, integrate real-world examples in lectures, and address students’ concerns regarding fieldwork.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Students</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Students are the main beneficiaries of the program and the bridge between theoretical knowledge and its practical application.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include reflection journals, surveys, and debrief sessions post fieldwork to share their experiences, learnings, and challenges with faculty and external site supervisors.</span></li>\n<li><b>Tasks:</b><span> Students incorporate feedback from university supervisors and external site supervisors to improve their practice while also sharing their experiences with the programs to elevate the field and curricular experiences for future students. </span></li>\n</ul>\n</li>\n</ul>\n<h4>Field Educators, Supervisors, Cooperating Teacher<span>s</span></h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Field educators, supervisors, and cooperating teachers oversee students during fieldwork, ensuring the application of academic knowledge and skill development.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include periodic reports on student performance, challenges faced, and curriculum alignment with real-world needs.</span></li>\n<li><b>Tasks: </b><span>Field educators, supervisors, and cooperating teachers</span> <span>mentor students on site, liaise with faculty for curriculum adjustments, and provide resources for students to overcome challenges while growing their professional skillset.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Program Administrators</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Program administrators bring everything together including the compilation and analysis of data pertinent to student and program performance and accreditation requirements.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include analyzing feedback from all stakeholders, compiling data on program outcomes and standards adherence, and preparing reports for program leaders and accrediting bodies.</span></li>\n<li><b>Tasks:</b><span> Program administrators highlight areas of improvement, ensuring compliance with accreditation standards, and providing data-driven insights to influence program development.</span></li>\n</ul>\n</li>\n</ul>\n<p><span>Incorporating feedback loops in college programs with field experience components ensures holistic development, aligning academic teachings with real-world application, and meeting accreditation standards.</span></p>\n</div><div><h3><i></i>The Key to Effective Feedback Loops: Rubrics</h3>\n<p>Rubrics play an integral role in feedback loops, primarily because they provide a clear and structured method for evaluating performance, be it in assignments, fieldwork, or other assessments. Here’s a breakdown of their importance in feedback loops:</p>\n<h4>Clarifying Expectations and Standards</h4>\n<p><span>Rubrics clearly outline what is expected from students or participants, ensuring that they understand the criteria against which they will be assessed. This transparency promotes a shared understanding between the evaluator and the evaluated.</span></p>\n<h4>Promoting Consistency and Fairness in Evaluation</h4>\n<p><span>By providing specific criteria and descriptors for each performance level, rubrics ensure that evaluations are consistent across different evaluators or over time. This consistency helps in mitigating biases and ensuring fairness.</span></p>\n<h4>Enhancing the Quality of Feedback</h4>\n<p><span>Rubrics offer detailed feedback, highlighting areas of strength and areas needing improvement. This detailed breakdown allows students or participants to understand where they excelled and where they fell short, promoting targeted improvement.</span></p>\n<h4>Improving Course Design and Teaching Strategies</h4>\n<p><span>Based on the collective feedback derived from rubrics, educators can identify areas where many students are facing challenges. This insight can then be used to modify teaching strategies or curriculum components.</span></p>\n<h4>Facilitating Self Assessments</h4>\n<p><span>Rubrics can also be used by students themselves for self-assessment. This promotes critical self-reflection which helps students internalize their learning and better-collaborate with their external supervisors and university faculty. </span></p>\n<h4>Saving Time while Maintaining Quality</h4>\n<p><span>The initial creation of a rubric might be time-consuming, but once developed it streamlines the feedback process. Educators can provide comprehensive feedback in a shorter time frame without compromising on its quality.</span></p>\n<h4>Aligning with Learning Goals</h4>\n<p><span>Rubrics, by their very design, align with specific learning goals or outcomes. This alignment ensures that the feedback provided is directly related to the objectives of the course or program.</span></p>\n<p><span>In the context of the feedback loops described earlier, rubrics offer a structured and consistent mechanism for gathering feedback from various stakeholders, be it students, field educators, or faculty. The insights derived from these rubrics can then be used to modify and enhance the program, creating a continuous loop of improvement.</span></p>\n</div><div><div><h3><i></i>Obstacles to Effective Feedback Loops</h3>\n<p><span>Developing effective feedback loops, as previously described, is fundamental for the continual improvement of any program. However, there are several obstacles that can hinder the successful establishment and operation of these feedback loops. Here are the key challenges:</span></p>\n<h4>Time Constraints</h4>\n<p><span>Collecting, analyzing, and acting upon feedback is a time-intensive process. When stakeholders feel pressed for time, they might not be able to provide detailed feedback or might skip the feedback process altogether.</span></p>\n<h4>Incomplete or Inadequate Feedback</h4>\n<p><span>Sometimes, feedback can be vague, lacking specifics, or might not cover all aspects of the assessment. Incomplete feedback can lead to misguided interpretations and actions.</span></p>\n<h4>Defensive Attitudes</h4>\n<p><span>Accepting feedback, especially if it’s critical, requires openness and humility. Stakeholders, especially educators or program leaders, might become defensive, dismissing the feedback rather than taking it constructively.</span></p>\n<h4>Lack of Clear Objectives</h4>\n<p><span>If the goals and objectives of the feedback loop aren’t well-defined from the outset, it can lead to confusion and misdirection.</span></p>\n<h4>Inconsistent Feedback Standards</h4>\n<p><span>Without a clear and consistent set of criteria, like what rubrics provide, feedback might vary widely based on individual perceptions and biases.</span></p>\n<h4>Over-reliance on Quantitative Data</h4>\n<p><span>While quantitative data is important, relying solely on it can overlook qualitative insights that provide depth and context to the feedback.</span></p>\n<h4>Feedback Loop Fatigue</h4>\n<p><span>Continually asking stakeholders for feedback can lead to fatigue, with them either providing perfunctory responses or avoiding the feedback process.</span></p>\n<h4>Inadequate Systems and Infrastructure</h4>\n<p><span>The lack of a systematic approach or technological tools can hinder the smooth operation of feedback loops, from collecting to analyzing and acting on feedback.</span></p>\n<h4>Not Acting on Feedback</h4>\n<p><span>If stakeholders perceive that their feedback isn’t leading to any tangible changes, they might become disillusioned and less likely to participate in future feedback cycles.</span></p>\n<h4>Communication Barriers</h4>\n<p><span>Effective feedback requires clear communication. Language barriers, unclear instructions, or the lack of a safe environment to pro",
        "html": "<div><div>\n<div><ul><li><span></span></li></ul></div><div><div><div><div><p></p><h2>Effective Feedback Loops in College Programs: Bridging the Gaps</h2>\n<p></p></div><div><h3>What are Feedback Loops?</h3>\n<p><span>Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and program administrators.</span></p>\n</div></div><div><h3><i></i>Feedback Loop Stakeholders</h3>\n<p>Feedback loops create communication bridges between all of your program’s key stakeholders. To be effective, everyone must participate. Here’s a look at the roles each stakeholder plays along with their feedback mechanisms and tasks.</p>\n<h4>Program Leaders (Deans and Program Chairs)</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role</b><span>: Program leaders set the vision, goals, and standards for the program.</span></li>\n<li><b>Feedback Mechanism:</b><span> Feedback mechanisms include regular meetings with faculty, field supervisors, and program administrators to gather insights on program performance, challenges in fieldwork, and potential areas of improvement.</span></li>\n<li><b>Tasks:</b><span> Program leaders regularly adjust program content, liaise with accrediting bodies, and mobilize resources based on feedback.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Faculty</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role: </b><span>Faculty are the builders and project managers of feedback loops as they design and deliver the curriculum and guide students before, during, and after their field experience.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include surveys and direct interactions with students pre and post fieldwork. Collaborative meetings with field educators to understand practical challenges and curriculum alignment.</span></li>\n<li><b>Tasks: </b><span>Faculty codify curriculum based on fieldwork insights, integrate real-world examples in lectures, and address students’ concerns regarding fieldwork.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Students</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Students are the main beneficiaries of the program and the bridge between theoretical knowledge and its practical application.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include reflection journals, surveys, and debrief sessions post fieldwork to share their experiences, learnings, and challenges with faculty and external site supervisors.</span></li>\n<li><b>Tasks:</b><span> Students incorporate feedback from university supervisors and external site supervisors to improve their practice while also sharing their experiences with the programs to elevate the field and curricular experiences for future students. </span></li>\n</ul>\n</li>\n</ul>\n<h4>Field Educators, Supervisors, Cooperating Teacher<span>s</span></h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Field educators, supervisors, and cooperating teachers oversee students during fieldwork, ensuring the application of academic knowledge and skill development.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include periodic reports on student performance, challenges faced, and curriculum alignment with real-world needs.</span></li>\n<li><b>Tasks: </b><span>Field educators, supervisors, and cooperating teachers</span> <span>mentor students on site, liaise with faculty for curriculum adjustments, and provide resources for students to overcome challenges while growing their professional skillset.</span></li>\n</ul>\n</li>\n</ul>\n<h4>Program Administrators</h4>\n<ul>\n<li>\n<ul>\n<li><b>Role:</b><span> Program administrators bring everything together including the compilation and analysis of data pertinent to student and program performance and accreditation requirements.</span></li>\n<li><b>Feedback Mechanism: </b><span>Feedback mechanisms include analyzing feedback from all stakeholders, compiling data on program outcomes and standards adherence, and preparing reports for program leaders and accrediting bodies.</span></li>\n<li><b>Tasks:</b><span> Program administrators highlight areas of improvement, ensuring compliance with accreditation standards, and providing data-driven insights to influence program development.</span></li>\n</ul>\n</li>\n</ul>\n<p><span>Incorporating feedback loops in college programs with field experience components ensures holistic development, aligning academic teachings with real-world application, and meeting accreditation standards.</span></p>\n</div><div><h3><i></i>The Key to Effective Feedback Loops: Rubrics</h3>\n<p>Rubrics play an integral role in feedback loops, primarily because they provide a clear and structured method for evaluating performance, be it in assignments, fieldwork, or other assessments. Here’s a breakdown of their importance in feedback loops:</p>\n<h4>Clarifying Expectations and Standards</h4>\n<p><span>Rubrics clearly outline what is expected from students or participants, ensuring that they understand the criteria against which they will be assessed. This transparency promotes a shared understanding between the evaluator and the evaluated.</span></p>\n<h4>Promoting Consistency and Fairness in Evaluation</h4>\n<p><span>By providing specific criteria and descriptors for each performance level, rubrics ensure that evaluations are consistent across different evaluators or over time. This consistency helps in mitigating biases and ensuring fairness.</span></p>\n<h4>Enhancing the Quality of Feedback</h4>\n<p><span>Rubrics offer detailed feedback, highlighting areas of strength and areas needing improvement. This detailed breakdown allows students or participants to understand where they excelled and where they fell short, promoting targeted improvement.</span></p>\n<h4>Improving Course Design and Teaching Strategies</h4>\n<p><span>Based on the collective feedback derived from rubrics, educators can identify areas where many students are facing challenges. This insight can then be used to modify teaching strategies or curriculum components.</span></p>\n<h4>Facilitating Self Assessments</h4>\n<p><span>Rubrics can also be used by students themselves for self-assessment. This promotes critical self-reflection which helps students internalize their learning and better-collaborate with their external supervisors and university faculty. </span></p>\n<h4>Saving Time while Maintaining Quality</h4>\n<p><span>The initial creation of a rubric might be time-consuming, but once developed it streamlines the feedback process. Educators can provide comprehensive feedback in a shorter time frame without compromising on its quality.</span></p>\n<h4>Aligning with Learning Goals</h4>\n<p><span>Rubrics, by their very design, align with specific learning goals or outcomes. This alignment ensures that the feedback provided is directly related to the objectives of the course or program.</span></p>\n<p><span>In the context of the feedback loops described earlier, rubrics offer a structured and consistent mechanism for gathering feedback from various stakeholders, be it students, field educators, or faculty. The insights derived from these rubrics can then be used to modify and enhance the program, creating a continuous loop of improvement.</span></p>\n</div><div><div><h3><i></i>Obstacles to Effective Feedback Loops</h3>\n<p><span>Developing effective feedback loops, as previously described, is fundamental for the continual improvement of any program. However, there are several obstacles that can hinder the successful establishment and operation of these feedback loops. Here are the key challenges:</span></p>\n<h4>Time Constraints</h4>\n<p><span>Collecting, analyzing, and acting upon feedback is a time-intensive process. When stakeholders feel pressed for time, they might not be able to provide detailed feedback or might skip the feedback process altogether.</span></p>\n<h4>Incomplete or Inadequate Feedback</h4>\n<p><span>Sometimes, feedback can be vague, lacking specifics, or might not cover all aspects of the assessment. Incomplete feedback can lead to misguided interpretations and actions.</span></p>\n<h4>Defensive Attitudes</h4>\n<p><span>Accepting feedback, especially if it’s critical, requires openness and humility. Stakeholders, especially educators or program leaders, might become defensive, dismissing the feedback rather than taking it constructively.</span></p>\n<h4>Lack of Clear Objectives</h4>\n<p><span>If the goals and objectives of the feedback loop aren’t well-defined from the outset, it can lead to confusion and misdirection.</span></p>\n<h4>Inconsistent Feedback Standards</h4>\n<p><span>Without a clear and consistent set of criteria, like what rubrics provide, feedback might vary widely based on individual perceptions and biases.</span></p>\n<h4>Over-reliance on Quantitative Data</h4>\n<p><span>While quantitative data is important, relying solely on it can overlook qualitative insights that provide depth and context to the feedback.</span></p>\n<h4>Feedback Loop Fatigue</h4>\n<p><span>Continually asking stakeholders for feedback can lead to fatigue, with them either providing perfunctory responses or avoiding the feedback process.</span></p>\n<h4>Inadequate Systems and Infrastructure</h4>\n<p><span>The lack of a systematic approach or technological tools can hinder the smooth operation of feedback loops, from collecting to analyzing and acting on feedback.</span></p>\n<h4>Not Acting on Feedback</h4>\n<p><span>If stakeholders perceive that their feedback isn’t leading to any tangible changes, they might become disillusioned and less likely to participate in future feedback cycles.</span></p>\n<h4>Communication Barriers</h4>\n<p><span>Effective feedback requires clear communication. Language barriers, unclear instructions, or the lack of a safe environment to pro",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the GapsWhat are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and progra",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the GapsWhat are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and progra",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the GapsWhat are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and progra",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the GapsWhat are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and progra",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the GapsWhat are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and progra",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Effective Feedback Loops in College Programs: Bridging the Gaps",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "What are Feedback Loops?Feedback loops are vital in ensuring the continuous improvement and relevance of university programs, especially those with field experience components. They help in bridging the gap between academic learning and real-world application. In this article, we’ll look at establishing and maintaining feedback loops involving various personas, including program leaders, faculty, students, field educators, and program administrators.",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Feedback Loop StakeholdersFeedback loops create communication bridges between all of your program’s key stakeholders. To be effective, everyone must participate. Here’s a look at the roles each stakeholder plays along with their feedback mechanisms and tasks.Program Leaders (Deans and Program Chairs)Role: Program leaders set the vision, goals, and standards for the program.Feedback Mechanism:Feedback mechanisms include regular meetings with faculty, field supervisors, and program administrators ",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The Key to Effective Feedback Loops: RubricsRubrics play an integral role in feedback loops, primarily because they provide a clear and structured method for evaluating performance, be it in assignments, fieldwork, or other assessments. Here’s a breakdown of their importance in feedback loops:Clarifying Expectations and StandardsRubrics clearly outline what is expected from students or participants, ensuring that they understand the criteria against which they will be assessed. This transparency",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Obstacles to Effective Feedback LoopsDeveloping effective feedback loops, as previously described, is fundamental for the continual improvement of any program. However, there are several obstacles that can hinder the successful establishment and operation of these feedback loops. Here are the key challenges:Time ConstraintsCollecting, analyzing, and acting upon feedback is a time-intensive process. When stakeholders feel pressed for time, they might not be able to provide detailed feedback or mi",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Obstacles to Effective Feedback LoopsDeveloping effective feedback loops, as previously described, is fundamental for the continual improvement of any program. However, there are several obstacles that can hinder the successful establishment and operation of these feedback loops. Here are the key challenges:Time ConstraintsCollecting, analyzing, and acting upon feedback is a time-intensive process. When stakeholders feel pressed for time, they might not be able to provide detailed feedback or mi",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Effective Feedback Loops in College Programs: Bridging the Gaps",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What are Feedback Loops?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Feedback Loop Stakeholders",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Program Leaders (Deans and Program Chairs)",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Faculty",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Students",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Field Educators, Supervisors, Cooperating Teachers",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Program Administrators",
              "id": ""
            },
            {
              "level": "h3",
              "text": "The Key to Effective Feedback Loops: Rubrics",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Clarifying Expectations and Standards",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Promoting Consistency and Fairness in Evaluation",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Enhancing the Quality of Feedback",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Improving Course Design and Teaching Strategies",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Facilitating Self Assessments",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Saving Time while Maintaining Quality",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Aligning with Learning Goals",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Obstacles to Effective Feedback Loops",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Time Constraints",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Incomplete or Inadequate Feedback",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Defensive Attitudes",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Lack of Clear Objectives",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Inconsistent Feedback Standards",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Over-reliance on Quantitative Data",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Feedback Loop Fatigue",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Inadequate Systems and Infrastructure",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Not Acting on Feedback",
              "id": ""
            },
            {
              "level": "h4",
              "text": "Communication Barriers",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://misinforeview.hks.harvard.edu/article/fact-checking-fact-checkers-a-data-driven-approach/",
      "title": "“Fact-checking” fact checkers: A data-driven approach | HKS Misinformation Review",
      "author": "By",
      "published_date": "2023-10-26T00:00:00.000Z",
      "content": {
        "text": "<div>\n<a href=\"#main-content\">Skip to main content</a>\n<header>\n</header>\n<section>\n<article>\n<div>\n<header>\n<p>Peer Reviewed</p>\n</header>\n<div>\n<div><h6>Article Metrics</h6><div><div><p></p><p>1</p></div><p>CrossRef Citations</p></div></div>\n<div><p><em>This study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed 1,820 fact-checking articles from Logically and the Australian Associated Press FactCheck and highlighted the differences in their fact-checking behaviors. Major events like the COVID-19 pandemic and the presidential election drove increased the frequency of fact-checking, with notable variations in ratings and authors across fact checkers. </em></p></div>\n<div>\n<div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/sian-lee\">Sian Lee</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/aiping-xiong\">Aiping Xiong</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/haeseung-seo\">Haeseung Seo</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/dongwon-lee\">Dongwon Lee</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div> </div>\n<figure><figcaption>Image by <a href=\"https://pixabay.com/users/geralt-9301/\">Geralt</a> on<a href=\"https://pixabay.com\"> Pixabay</a></figcaption></figure>\n<div>\n<h2>Research Questions</h2>\n<ul>\n<li>Do different fact checkers exhibit similar or distinct behaviors with respect to the frequency of fact-checking, types of claims selected for fact-checking, and the individuals responsible for conducting fact checks?</li>\n<li>What percentage of statements debunked by fact checkers are overlapping (i.e., matching claims) across multiple fact checkers?</li>\n<li>Is there a reasonable level of agreement among fact checkers in their ratings of matching claims that have been debunked by multiple fact checkers?</li>\n</ul>\n<h2>Essay Summary</h2>\n<ul>\n<li>This study examined four fact-checking organizations (so-called <em>fact checkers</em>)—Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck (AAP)—by analyzing their fact-checking articles from January 1, 2016, to August 31, 2022.</li>\n<li>Results showed an increased number of fact-checking articles during major events, such as the COVID-19 pandemic and the U.S. presidential election, suggesting their influence on fact-checking activities.</li>\n<li>Furthermore, variations were found in ratings and authors of fact-checking articles among fact checkers. While PolitiFact and AAP primarily focused on verifying suspicious claims, Snopes and Logically emphasized affirming truthful claims. The distribution of the number of fact-checking articles per author also differed across fact checkers, likely reflecting variations in their operational scope and scale. </li>\n<li>Critically, we assessed the degree of consensus between Snopes and PolitiFact’s verdicts on matching claims (i.e., the same [mis]information with the wording of the claim being slightly different). Out of 11,639 and 10,710 fact-checking articles from Snopes and PolitiFact, respectively, 6.5% (749) were matching claims, of which 521 (69.6%) received identical ratings, while the remaining 228 (30.4%) had diverging ratings.</li>\n<li>Rating discrepancies are attributed to various systematic factors: 1) differences in the granularity of verdict ratings, 2) differences in focus between two fact checkers, 3) similar claims but subtle differences in the key information to fact-check, and 4) different timing in fact-checking. After adjusting these systematic discrepancies, we found only one case out of 749 matching claims with conflicting verdict ratings.</li>\n<li>Consequently, our findings show high agreement between Snopes and PolitiFact regarding their fact-checking verdicts after adjusting minor rating differences.</li>\n</ul>\n</div>\n<hr/>\n<h2>Implications</h2>\n<p>Misinformation, in a broad sense, refers to information that is presented as factually accurate but includes false or misleading content, irrespective of the intentions of the presenter (van der Linden, 2022). The utilization of social media as a primary source of news consumption has, to some extent, contributed to the dissemination of misinformation (Wu et al., 2019). According to a Pew Research survey in 2021, about half<em> </em>of Americans get their news on social media (Walker &amp; Matsa, 2021). Because online users can upload and share news without verifications, information, especially misinformation, diffuses quickly on social media (Vosoughi et al., 2018). The spread of misinformation can have severe negative impacts on individuals and society, such as COVID-19 vaccination hesitancy (Garett &amp; Young, 2021) or election results manipulation (Allcott &amp; Gentzkow, 2017). Furthermore, individuals characterized as ‘lazy’ are not the only ones susceptible to misinformation (Pennycook &amp; Rand, 2019); specific types of misinformation, such as associatively inferred misinformation, can make individuals with higher cognitive ability levels even more susceptible (Lee et al., 2020, 2023; Xiong et al., 2023).</p>\n<p>Fact-checking organizations, often known as fact checkers, are instrumental in identifying and debunking misinformation. Fact-checking has traditionally been performed by human professionals, either individuals or teams, who manually review and analyze claims using various resources and methods to affirm the information’s accuracy (Amazeen, 2015). Although these human fact checkers can apply considerable expertise and critical thinking, their work can be time-consuming and costly. In response, automated fact-checking techniques have emerged to debunk misinformation on a large scale (Cui et al., 2020; D’Ulizia et al., 2021; Shu et al., 2019; Wu et al., 2019; Zhang &amp; Ghorbani, 2020; Zhou &amp; Zafarani, 2020). These misinformation detection algorithms employ advanced techniques such as natural language processing, machine learning, and deep learning to detect patterns and correlations in large datasets. Despite substantial advancements, these automated techniques face challenges. The sheer volume of data and rapid spread of false claims make timely detection difficult, and the accuracy and effectiveness of these algorithms are limited by the need for high-quality training datasets and the potential for bias (Wu et al., 2019). Furthermore, new types of misinformation, such as deep fakes, remain challenging to detect (Rana et al., 2022), requiring additional human expertise and intervention.</p>\n<p>In practice, a few initiatives employing manual fact-checking have been launched and are playing a vital role in combating misinformation. However, these can invite criticism due to the subjective choice of claims to verify and the inconsistency in the evaluation process (Nieminen &amp; Rapeli, 2019). Specifically, concerns have been raised about the potential uncertainty that may arise among individuals if different fact checkers provide conflicting assessments for the same claim (Marietta et al., 2015). Previous studies evaluated the performance of fact checkers and showed conflicting results. Amazeen’s (2015, 2016) study demonstrated consistency in the verdicts of various fact checkers using manually gathered samples of political ads from the 2008 and 2012 U.S. presidential elections. In contrast, Marietta et al. (2015) found significant discrepancies among three fact checkers—PolitiFact, The Fact Checker,<sup>1</sup><span>The Fact Checker of <em>The Washington Post</em>: <a href=\"https://www.washingtonpost.com/news/fact-checker/\">https://www.washingtonpost.com/news/fact-checker/</a></span> and FactCheck.org<sup>2</sup><span><a href=\"https://www.factcheck.org/\">https://www.factcheck.org/</a></span>—in their assessments of the statements and conclusions on the existence of climate change, the influence of racism, and the consequences of the national debt. Notably, it reported that the fact checkers agreed regarding the existence of climate change, while they disagreed on the issue of the national debt. Additionally, only PolitiFact assessed the influence of racism. The findings indicate that individuals seeking to discern the veracity of disputed claims may not perceive fact-checking to be particularly efficacious, especially in the context of polarized political topics. Lim (2018) also manually collected samples of 2016 U.S. presidential candidates’ statements from two different fact checkers (i.e., The Fact Checker and PolitiFact) and evaluated their performance. This study found that only 10% of statements were fact-checked by both fact checkers, and the fact checkers agreed on obvious truths and falsehoods but had lower agreement rates for statements in the ambiguous rating range. The findings indicate that fact-checking is challenging, and disagreements are common, particularly when politicians use ambiguous language. </p>\n<p>It is important to acknowledge that previous studies have used different samples from fact checkers and different methods, which could explain their conflicting conclusions (Nieminen &amp; Rapeli, 2019). The findings were based on a small number of manually collected claims on specific topics (e.g., presidential candidates, climate change, debt) dur",
        "html": "<div>\n<a href=\"#main-content\">Skip to main content</a>\n<header>\n</header>\n<section>\n<article>\n<div>\n<header>\n<p>Peer Reviewed</p>\n</header>\n<div>\n<div><h6>Article Metrics</h6><div><div><p></p><p>1</p></div><p>CrossRef Citations</p></div></div>\n<div><p><em>This study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed 1,820 fact-checking articles from Logically and the Australian Associated Press FactCheck and highlighted the differences in their fact-checking behaviors. Major events like the COVID-19 pandemic and the presidential election drove increased the frequency of fact-checking, with notable variations in ratings and authors across fact checkers. </em></p></div>\n<div>\n<div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/sian-lee\">Sian Lee</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/aiping-xiong\">Aiping Xiong</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/haeseung-seo\">Haeseung Seo</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div><div><p><a href=\"https://misinforeview.hks.harvard.edu/article/author/dongwon-lee\">Dongwon Lee</a></p><p>College of Information Sciences and Technology, The Pennsylvania State University, USA</p></div> </div>\n<figure><figcaption>Image by <a href=\"https://pixabay.com/users/geralt-9301/\">Geralt</a> on<a href=\"https://pixabay.com\"> Pixabay</a></figcaption></figure>\n<div>\n<h2>Research Questions</h2>\n<ul>\n<li>Do different fact checkers exhibit similar or distinct behaviors with respect to the frequency of fact-checking, types of claims selected for fact-checking, and the individuals responsible for conducting fact checks?</li>\n<li>What percentage of statements debunked by fact checkers are overlapping (i.e., matching claims) across multiple fact checkers?</li>\n<li>Is there a reasonable level of agreement among fact checkers in their ratings of matching claims that have been debunked by multiple fact checkers?</li>\n</ul>\n<h2>Essay Summary</h2>\n<ul>\n<li>This study examined four fact-checking organizations (so-called <em>fact checkers</em>)—Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck (AAP)—by analyzing their fact-checking articles from January 1, 2016, to August 31, 2022.</li>\n<li>Results showed an increased number of fact-checking articles during major events, such as the COVID-19 pandemic and the U.S. presidential election, suggesting their influence on fact-checking activities.</li>\n<li>Furthermore, variations were found in ratings and authors of fact-checking articles among fact checkers. While PolitiFact and AAP primarily focused on verifying suspicious claims, Snopes and Logically emphasized affirming truthful claims. The distribution of the number of fact-checking articles per author also differed across fact checkers, likely reflecting variations in their operational scope and scale. </li>\n<li>Critically, we assessed the degree of consensus between Snopes and PolitiFact’s verdicts on matching claims (i.e., the same [mis]information with the wording of the claim being slightly different). Out of 11,639 and 10,710 fact-checking articles from Snopes and PolitiFact, respectively, 6.5% (749) were matching claims, of which 521 (69.6%) received identical ratings, while the remaining 228 (30.4%) had diverging ratings.</li>\n<li>Rating discrepancies are attributed to various systematic factors: 1) differences in the granularity of verdict ratings, 2) differences in focus between two fact checkers, 3) similar claims but subtle differences in the key information to fact-check, and 4) different timing in fact-checking. After adjusting these systematic discrepancies, we found only one case out of 749 matching claims with conflicting verdict ratings.</li>\n<li>Consequently, our findings show high agreement between Snopes and PolitiFact regarding their fact-checking verdicts after adjusting minor rating differences.</li>\n</ul>\n</div>\n<hr/>\n<h2>Implications</h2>\n<p>Misinformation, in a broad sense, refers to information that is presented as factually accurate but includes false or misleading content, irrespective of the intentions of the presenter (van der Linden, 2022). The utilization of social media as a primary source of news consumption has, to some extent, contributed to the dissemination of misinformation (Wu et al., 2019). According to a Pew Research survey in 2021, about half<em> </em>of Americans get their news on social media (Walker &amp; Matsa, 2021). Because online users can upload and share news without verifications, information, especially misinformation, diffuses quickly on social media (Vosoughi et al., 2018). The spread of misinformation can have severe negative impacts on individuals and society, such as COVID-19 vaccination hesitancy (Garett &amp; Young, 2021) or election results manipulation (Allcott &amp; Gentzkow, 2017). Furthermore, individuals characterized as ‘lazy’ are not the only ones susceptible to misinformation (Pennycook &amp; Rand, 2019); specific types of misinformation, such as associatively inferred misinformation, can make individuals with higher cognitive ability levels even more susceptible (Lee et al., 2020, 2023; Xiong et al., 2023).</p>\n<p>Fact-checking organizations, often known as fact checkers, are instrumental in identifying and debunking misinformation. Fact-checking has traditionally been performed by human professionals, either individuals or teams, who manually review and analyze claims using various resources and methods to affirm the information’s accuracy (Amazeen, 2015). Although these human fact checkers can apply considerable expertise and critical thinking, their work can be time-consuming and costly. In response, automated fact-checking techniques have emerged to debunk misinformation on a large scale (Cui et al., 2020; D’Ulizia et al., 2021; Shu et al., 2019; Wu et al., 2019; Zhang &amp; Ghorbani, 2020; Zhou &amp; Zafarani, 2020). These misinformation detection algorithms employ advanced techniques such as natural language processing, machine learning, and deep learning to detect patterns and correlations in large datasets. Despite substantial advancements, these automated techniques face challenges. The sheer volume of data and rapid spread of false claims make timely detection difficult, and the accuracy and effectiveness of these algorithms are limited by the need for high-quality training datasets and the potential for bias (Wu et al., 2019). Furthermore, new types of misinformation, such as deep fakes, remain challenging to detect (Rana et al., 2022), requiring additional human expertise and intervention.</p>\n<p>In practice, a few initiatives employing manual fact-checking have been launched and are playing a vital role in combating misinformation. However, these can invite criticism due to the subjective choice of claims to verify and the inconsistency in the evaluation process (Nieminen &amp; Rapeli, 2019). Specifically, concerns have been raised about the potential uncertainty that may arise among individuals if different fact checkers provide conflicting assessments for the same claim (Marietta et al., 2015). Previous studies evaluated the performance of fact checkers and showed conflicting results. Amazeen’s (2015, 2016) study demonstrated consistency in the verdicts of various fact checkers using manually gathered samples of political ads from the 2008 and 2012 U.S. presidential elections. In contrast, Marietta et al. (2015) found significant discrepancies among three fact checkers—PolitiFact, The Fact Checker,<sup>1</sup><span>The Fact Checker of <em>The Washington Post</em>: <a href=\"https://www.washingtonpost.com/news/fact-checker/\">https://www.washingtonpost.com/news/fact-checker/</a></span> and FactCheck.org<sup>2</sup><span><a href=\"https://www.factcheck.org/\">https://www.factcheck.org/</a></span>—in their assessments of the statements and conclusions on the existence of climate change, the influence of racism, and the consequences of the national debt. Notably, it reported that the fact checkers agreed regarding the existence of climate change, while they disagreed on the issue of the national debt. Additionally, only PolitiFact assessed the influence of racism. The findings indicate that individuals seeking to discern the veracity of disputed claims may not perceive fact-checking to be particularly efficacious, especially in the context of polarized political topics. Lim (2018) also manually collected samples of 2016 U.S. presidential candidates’ statements from two different fact checkers (i.e., The Fact Checker and PolitiFact) and evaluated their performance. This study found that only 10% of statements were fact-checked by both fact checkers, and the fact checkers agreed on obvious truths and falsehoods but had lower agreement rates for statements in the ambiguous rating range. The findings indicate that fact-checking is challenging, and disagreements are common, particularly when politicians use ambiguous language. </p>\n<p>It is important to acknowledge that previous studies have used different samples from fact checkers and different methods, which could explain their conflicting conclusions (Nieminen &amp; Rapeli, 2019). The findings were based on a small number of manually collected claims on specific topics (e.g., presidential candidates, climate change, debt) dur",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Skip to main contentPeer ReviewedArticle Metrics1CrossRef CitationsThis study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating difference",
              "class": [],
              "id": ""
            },
            {
              "type": "section",
              "content": "Peer ReviewedArticle Metrics1CrossRef CitationsThis study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed",
              "class": [],
              "id": ""
            },
            {
              "type": "article",
              "content": "Peer ReviewedArticle Metrics1CrossRef CitationsThis study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Peer ReviewedArticle Metrics1CrossRef CitationsThis study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Article Metrics1CrossRef CitationsThis study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed 1,820 fact-c",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Article Metrics1CrossRef Citations",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "1CrossRef Citations",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "1",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "This study examined four fact checkers (Snopes, PolitiFact, Logically, and the Australian Associated Press FactCheck) using a data-driven approach. First, we scraped 22,349 fact-checking articles from Snopes and PolitiFact and compared their results and agreement on verdicts. Generally, the two fact checkers agreed with each other, with only one conflicting verdict among 749 matching claims after adjusting minor rating differences. Next, we assessed 1,820 fact-checking articles from Logically an",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sian LeeCollege of Information Sciences and Technology, The Pennsylvania State University, USAAiping XiongCollege of Information Sciences and Technology, The Pennsylvania State University, USAHaeseung SeoCollege of Information Sciences and Technology, The Pennsylvania State University, USADongwon LeeCollege of Information Sciences and Technology, The Pennsylvania State University, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Sian LeeCollege of Information Sciences and Technology, The Pennsylvania State University, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Aiping XiongCollege of Information Sciences and Technology, The Pennsylvania State University, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Haeseung SeoCollege of Information Sciences and Technology, The Pennsylvania State University, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Dongwon LeeCollege of Information Sciences and Technology, The Pennsylvania State University, USA",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Research QuestionsDo different fact checkers exhibit similar or distinct behaviors with respect to the frequency of fact-checking, types of claims selected for fact-checking, and the individuals responsible for conducting fact checks?What percentage of statements debunked by fact checkers are overlapping (i.e., matching claims) across multiple fact checkers?Is there a reasonable level of agreement among fact checkers in their ratings of matching claims that have been debunked by multiple fact ch",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h6",
              "text": "Article Metrics",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Research Questions",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Essay Summary",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Implications",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://lincs.ed.gov/state-resources/federal-initiatives/teal/guide/selfregulated",
      "title": "TEAL Center Fact Sheet No. 3: Self-Regulated Learning",
      "author": "",
      "published_date": "2019-04-08T14:30:31.000Z",
      "content": {
        "text": "<div><div><p>\nGood self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies.\n</p>\n<h3>\nAbout Self-Regulated Learning\n</h3>\n<p>\nSelf-regulated learning refers to one’s ability to understand and control one’s learning environment. Self-regulation abilities include goal setting, self-monitoring, self-instruction, and self-reinforcement (Harris &amp; Graham, 1999; Schraw, Crippen, &amp; Hartley, 2006; Shunk, 1996). Self-regulation should not be confused with a mental ability or an academic performance skill. Instead, self-regulation is a self-directive process and set of behaviors whereby learners transform their mental abilities into skills (Zimmerman, Bonnor, &amp; Kovach, 2002) and habits through a developmental process (Butler, 1995, 1998, 2002) that emerges from guided practice and feedback (Paris &amp; Paris, 2001).\n</p>\n<h3>\nElements of Self-Regulated Learning\n</h3>\n<p>\nEffective learners are self-regulating; analyzing task requirements; setting productive goals; and selecting, adapting, or inventing strategies to achieve their objectives. These learners also monitor progress as they work through the task, managing intrusive emotions and waning motivation as well as adjusting strategies processed to foster success. These are the students who ask questions, take notes, and allocate their time and their resources in ways that help them to be in charge of their own learning (Paris &amp; Paris, 2001).\n</p>\n<h3>\nWhy Teach Self-Regulated Learning to Adults?\n</h3>\n<p>\nGood self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies for adult education settings.\n</p>\n<p>\nSelf-regulated learning strategies help to prepare learners for lifelong learning and the important capacity to transfer skills, knowledge, and abilities from one domain or setting to another.\n</p>\n<h3>\nWhat’s the Research?\n</h3>\n<p>\nIn the 1980s, the term <em>self-regulated learning</em> originated from the increased focus on self-regulation in academic settings (Dinsmore, Alexander, &amp; Loughlin, 2008). A large base of literature has been established on self-regulated learning since the mid-1980s when researchers first began to look at how students become masters of their own learning processes (Zimmerman &amp; Schunk, 2001). Today, most models of self-regulated learning incorporate aspects of both Metacognition and self-regulation focusing on self-monitoring (Dinsmore, Alexander, &amp; Loughlin, 2008). Zimmerman and Schunk (2001, 2008) directly link motivation to self-regulation. According to these researchers, self-regulated students are those students who are metacognitively, motivationally, and behaviorally active in their own learning processes and in achieving their own goals.\n</p>\n<h3>\nRecommended Instructional Strategies\n</h3>\n<p>\nWhen strategy instruction for academic learning is paired with self-regulation, called SRSD or self-regulated strategy development, learners become more confident at adapting strategies reflectively and flexibly within recursive cycles of task analysis, strategy use, and monitoring.\n</p>\n<p>\nMany of the self-regulated learning strategies are useful across various content domains. Specifically, self-regulated learning consists of three components: <strong>cognition</strong>, <strong>Metacognition</strong>, and <strong>motivation</strong>. The cognition component includes the skills and habits that are necessary to encode, memorize, and recall information as well as think critically. Within the Metacognition component are skills that enable learners to understand and monitor their cognitive processes. The motivation component surfaces the beliefs and attitudes that affect the use and development of both the cognitive and metacognitive skills.\n</p>\n<p>\nSuggestions for developing self-regulation in the adult education classroom follow:\n</p>\n<ul>\n<li>\n<strong>Cognitive strategies,</strong> include learning strategies that can be specific to a domain or content. Problem solving strategies and critical thinking skills are also important. Critical thinking involves a variety of skills such as identifying a particular source of information and reflecting on whether or not that information is consistent with one’s prior knowledge. Activities to help adults articulate and practice critical thinking include comprehension activities such as student-generated questions before or during reading to focus the learner’s attention, constructing graphs and tables of real-world issues, and engaging in classroom debate to articulate arguments for writing a persuasive essay.\n</li>\n<li>\nThe<strong> metacognitive </strong>component is comprised of <em>declarative knowledge</em> (knowledge about oneself as a learner—the factors that influence performance), procedural knowledge (knowledge about strategies and other procedures), and conditional knowledge (knowledge of why and when to use a particular strategy). Adults often struggle to articulate their knowledge or to transfer domain-specific knowledge to a new setting. The goal of self-regulated learning is for these strategies to first become visible and eventually automated for the adult learner. One way to make the three types of knowledge visible in the classroom is to have learners do a demonstration. When demonstrating (such as cooking a particular dish), it is easier to find the specific words needed to articulate what one is doing and how one knows to do it. Questions will draw out more language. Debriefing after the demonstration can make visible the difference between declarative, procedural, and conditional knowledge so that one can make explicit points about how to transfer that knowledge to an academic task.\n</li>\n<li>\nThe <strong>motivation</strong> component includes both self-efficacy (degree to which one is confident that one can perform a task or accomplish a goal) and epistemological beliefs (beliefs about the origin and nature of knowledge). Working with adults who have failed in school or with specific academic tasks necessitates deliberate discussion of their sense of self-efficacy. Many adult learners have shared with teachers and researchers how difficult it can be to overcome ingrained, virulent, negative self-talk. Making self-regulated strategy development (SRSD), including goal setting, monitoring and displaying of progress, an everyday feature of instruction can assist these learners to replace negative self-talk with positive self-instruction and a sense of self as an effective learner. Building new habits reinforces adults’ persistence and motivation.\n</li>\n</ul>\n<p>\nAdult educators work diligently to help adults become successful, independent learners. Self-regulated learning strategies are research-based instructional techniques to help learners monitor and manage their own learning skills and habits. When paired with strategy instruction and metacogntive processes, instructors have a powerful learning toolkit to share with learners.\n</p>\n<p>\n<a href=\"https://lincs.ed.gov/programs/teal/guide/metacognitive\">Next</a>\n</p>\n<h4>\nReferences\n</h4>\n<p>\nButler, D. (1995). Promoting strategic learning by postsecondary students with learning disabilities. <em>Journal of Learning </em><em>Disabilities, 25, </em>226–229.\n</p>\n<p>\nButler, D. (2002). Individualizing instruction in self-regulated learning. <em>Theory into Practice, 41,</em> 81–92.\n</p>\n<p>\nButler, D. (1998). A strategic content learning approach to promoting self-regulated learning by students with learning disabilities. In D. Shunk &amp; B. Zimmerman (Eds.), <em>Self-regulating Learning: From teaching to self-reflective </em><em>practice</em> (pp. 160–183). New York: Guilford Press.\n</p>\n<p>\nDinsmore, D., Alexander, P., &amp; Loughlin, S. (2008). Focusing the conceptual lens on metacognition, self-regulation, and self-regulated learning. <em>Educational Psychology Review, </em><em>20,</em> 391–409.\n</p>\n<p>\nHarris, K., &amp; Graham, S. (1999). Programmatic intervention research: Illustrations from the evolution of self-regulated strategy development. <em>Learning Disability Quarterly, 22, </em>251–262.\n</p>\n<p>\nParis, S., &amp; Paris, A. (2001). Classroom applications of research on self-regulated learning. <em>Educational Psychology, 36, </em>89–101.\n</p>\n<p>\nSchraw, G., Crippen, K., &amp; Hartley, K. (2006). Promoting self-regulation in science education: Metacognition as part of a broader perspective on learning. <em>Research in Science </em><em>Education, 36,</em> 111–139.\n</p>\n<p>\nShunk, D. (1996). Goal and self-evaluative influences during children’s cognitive skill learning. <em>American Educational </em><em>Research Journal, 33, </em>359–382.\n</p>\n<p>\nZimmerman, B., Bonner, S., &amp; Kovach, R. (2002). <em>Developing </em><em>self-regulated learners: Beyond achievement to self-</em><em>efficacy</em>. Washington, DC: American Psychological Association.\n</p>\n<p>\nZimmerman, B., &amp; Schunk, D. (2001). <em>Self-regulated learning </em><em>and academic achievement: Theoretical perspectives</em>. Mahwah, NJ: Lawrence Erlbaum Associates.\n</p>\n<p>\nZimmerman, B., &amp; Schunk, D. (2008). Motivation: An essential dimension of self-regulated learning. In D. Schunk &amp; B. Zimmerman, <em>Motivation and self-regulated learning: </em><em>Theory, research, and application.</em> Mahwah, NJ: Lawrence Erlbaum Associates.\n</p>\n<p>\nAuthors: Tanya Shuy, OCTAE; and TEAL staff\n</p>\n<p>\nAbout the TEAL Center: The Teaching Excellence in Adult Literacy (TEAL) Center is a project of the U.S. Department of Education, Office of Career, Technical, and Adult ",
        "html": "<div><div><p>\nGood self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies.\n</p>\n<h3>\nAbout Self-Regulated Learning\n</h3>\n<p>\nSelf-regulated learning refers to one’s ability to understand and control one’s learning environment. Self-regulation abilities include goal setting, self-monitoring, self-instruction, and self-reinforcement (Harris &amp; Graham, 1999; Schraw, Crippen, &amp; Hartley, 2006; Shunk, 1996). Self-regulation should not be confused with a mental ability or an academic performance skill. Instead, self-regulation is a self-directive process and set of behaviors whereby learners transform their mental abilities into skills (Zimmerman, Bonnor, &amp; Kovach, 2002) and habits through a developmental process (Butler, 1995, 1998, 2002) that emerges from guided practice and feedback (Paris &amp; Paris, 2001).\n</p>\n<h3>\nElements of Self-Regulated Learning\n</h3>\n<p>\nEffective learners are self-regulating; analyzing task requirements; setting productive goals; and selecting, adapting, or inventing strategies to achieve their objectives. These learners also monitor progress as they work through the task, managing intrusive emotions and waning motivation as well as adjusting strategies processed to foster success. These are the students who ask questions, take notes, and allocate their time and their resources in ways that help them to be in charge of their own learning (Paris &amp; Paris, 2001).\n</p>\n<h3>\nWhy Teach Self-Regulated Learning to Adults?\n</h3>\n<p>\nGood self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies for adult education settings.\n</p>\n<p>\nSelf-regulated learning strategies help to prepare learners for lifelong learning and the important capacity to transfer skills, knowledge, and abilities from one domain or setting to another.\n</p>\n<h3>\nWhat’s the Research?\n</h3>\n<p>\nIn the 1980s, the term <em>self-regulated learning</em> originated from the increased focus on self-regulation in academic settings (Dinsmore, Alexander, &amp; Loughlin, 2008). A large base of literature has been established on self-regulated learning since the mid-1980s when researchers first began to look at how students become masters of their own learning processes (Zimmerman &amp; Schunk, 2001). Today, most models of self-regulated learning incorporate aspects of both Metacognition and self-regulation focusing on self-monitoring (Dinsmore, Alexander, &amp; Loughlin, 2008). Zimmerman and Schunk (2001, 2008) directly link motivation to self-regulation. According to these researchers, self-regulated students are those students who are metacognitively, motivationally, and behaviorally active in their own learning processes and in achieving their own goals.\n</p>\n<h3>\nRecommended Instructional Strategies\n</h3>\n<p>\nWhen strategy instruction for academic learning is paired with self-regulation, called SRSD or self-regulated strategy development, learners become more confident at adapting strategies reflectively and flexibly within recursive cycles of task analysis, strategy use, and monitoring.\n</p>\n<p>\nMany of the self-regulated learning strategies are useful across various content domains. Specifically, self-regulated learning consists of three components: <strong>cognition</strong>, <strong>Metacognition</strong>, and <strong>motivation</strong>. The cognition component includes the skills and habits that are necessary to encode, memorize, and recall information as well as think critically. Within the Metacognition component are skills that enable learners to understand and monitor their cognitive processes. The motivation component surfaces the beliefs and attitudes that affect the use and development of both the cognitive and metacognitive skills.\n</p>\n<p>\nSuggestions for developing self-regulation in the adult education classroom follow:\n</p>\n<ul>\n<li>\n<strong>Cognitive strategies,</strong> include learning strategies that can be specific to a domain or content. Problem solving strategies and critical thinking skills are also important. Critical thinking involves a variety of skills such as identifying a particular source of information and reflecting on whether or not that information is consistent with one’s prior knowledge. Activities to help adults articulate and practice critical thinking include comprehension activities such as student-generated questions before or during reading to focus the learner’s attention, constructing graphs and tables of real-world issues, and engaging in classroom debate to articulate arguments for writing a persuasive essay.\n</li>\n<li>\nThe<strong> metacognitive </strong>component is comprised of <em>declarative knowledge</em> (knowledge about oneself as a learner—the factors that influence performance), procedural knowledge (knowledge about strategies and other procedures), and conditional knowledge (knowledge of why and when to use a particular strategy). Adults often struggle to articulate their knowledge or to transfer domain-specific knowledge to a new setting. The goal of self-regulated learning is for these strategies to first become visible and eventually automated for the adult learner. One way to make the three types of knowledge visible in the classroom is to have learners do a demonstration. When demonstrating (such as cooking a particular dish), it is easier to find the specific words needed to articulate what one is doing and how one knows to do it. Questions will draw out more language. Debriefing after the demonstration can make visible the difference between declarative, procedural, and conditional knowledge so that one can make explicit points about how to transfer that knowledge to an academic task.\n</li>\n<li>\nThe <strong>motivation</strong> component includes both self-efficacy (degree to which one is confident that one can perform a task or accomplish a goal) and epistemological beliefs (beliefs about the origin and nature of knowledge). Working with adults who have failed in school or with specific academic tasks necessitates deliberate discussion of their sense of self-efficacy. Many adult learners have shared with teachers and researchers how difficult it can be to overcome ingrained, virulent, negative self-talk. Making self-regulated strategy development (SRSD), including goal setting, monitoring and displaying of progress, an everyday feature of instruction can assist these learners to replace negative self-talk with positive self-instruction and a sense of self as an effective learner. Building new habits reinforces adults’ persistence and motivation.\n</li>\n</ul>\n<p>\nAdult educators work diligently to help adults become successful, independent learners. Self-regulated learning strategies are research-based instructional techniques to help learners monitor and manage their own learning skills and habits. When paired with strategy instruction and metacogntive processes, instructors have a powerful learning toolkit to share with learners.\n</p>\n<p>\n<a href=\"https://lincs.ed.gov/programs/teal/guide/metacognitive\">Next</a>\n</p>\n<h4>\nReferences\n</h4>\n<p>\nButler, D. (1995). Promoting strategic learning by postsecondary students with learning disabilities. <em>Journal of Learning </em><em>Disabilities, 25, </em>226–229.\n</p>\n<p>\nButler, D. (2002). Individualizing instruction in self-regulated learning. <em>Theory into Practice, 41,</em> 81–92.\n</p>\n<p>\nButler, D. (1998). A strategic content learning approach to promoting self-regulated learning by students with learning disabilities. In D. Shunk &amp; B. Zimmerman (Eds.), <em>Self-regulating Learning: From teaching to self-reflective </em><em>practice</em> (pp. 160–183). New York: Guilford Press.\n</p>\n<p>\nDinsmore, D., Alexander, P., &amp; Loughlin, S. (2008). Focusing the conceptual lens on metacognition, self-regulation, and self-regulated learning. <em>Educational Psychology Review, </em><em>20,</em> 391–409.\n</p>\n<p>\nHarris, K., &amp; Graham, S. (1999). Programmatic intervention research: Illustrations from the evolution of self-regulated strategy development. <em>Learning Disability Quarterly, 22, </em>251–262.\n</p>\n<p>\nParis, S., &amp; Paris, A. (2001). Classroom applications of research on self-regulated learning. <em>Educational Psychology, 36, </em>89–101.\n</p>\n<p>\nSchraw, G., Crippen, K., &amp; Hartley, K. (2006). Promoting self-regulation in science education: Metacognition as part of a broader perspective on learning. <em>Research in Science </em><em>Education, 36,</em> 111–139.\n</p>\n<p>\nShunk, D. (1996). Goal and self-evaluative influences during children’s cognitive skill learning. <em>American Educational </em><em>Research Journal, 33, </em>359–382.\n</p>\n<p>\nZimmerman, B., Bonner, S., &amp; Kovach, R. (2002). <em>Developing </em><em>self-regulated learners: Beyond achievement to self-</em><em>efficacy</em>. Washington, DC: American Psychological Association.\n</p>\n<p>\nZimmerman, B., &amp; Schunk, D. (2001). <em>Self-regulated learning </em><em>and academic achievement: Theoretical perspectives</em>. Mahwah, NJ: Lawrence Erlbaum Associates.\n</p>\n<p>\nZimmerman, B., &amp; Schunk, D. (2008). Motivation: An essential dimension of self-regulated learning. In D. Schunk &amp; B. Zimmerman, <em>Motivation and self-regulated learning: </em><em>Theory, research, and application.</em> Mahwah, NJ: Lawrence Erlbaum Associates.\n</p>\n<p>\nAuthors: Tanya Shuy, OCTAE; and TEAL staff\n</p>\n<p>\nAbout the TEAL Center: The Teaching Excellence in Adult Literacy (TEAL) Center is a project of the U.S. Department of Education, Office of Career, Technical, and Adult ",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "Good self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies.About Self-Regulated LearningSelf-regulated learning refers to one’s ability to understand and control one’s learning environment. Self-regulation abilities include goal setting, self-monitoring, self-",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Good self-regulators have developed the skills and habits to be effective learners, exhibiting effective learning strategies, effort, and persistence. The key for instructors is to understand how to foster and train these skills in all students. This fact sheet offers some instructional strategies.About Self-Regulated LearningSelf-regulated learning refers to one’s ability to understand and control one’s learning environment. Self-regulation abilities include goal setting, self-monitoring, self-",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h3",
              "text": "About Self-Regulated Learning",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Elements of Self-Regulated Learning",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Why Teach Self-Regulated Learning to Adults?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "What’s the Research?",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Recommended Instructional Strategies",
              "id": ""
            },
            {
              "level": "h4",
              "text": "References",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://proceedings.systemdynamics.org/2023/papers/P1208.pdf",
      "title": "Microsoft Word - KastenShipley_SDS2023_Rev20jun2023.docx",
      "author": "",
      "published_date": null,
      "content": {
        "text": "1\nHow does the Human Mind Think and Learn about Feedback Loops?\nKim A. Kastens\nLamont-Doherty Earth Observatory of Columbia University\n(kastens@ldeo.columbia.edu)\nand\nThomas F. Shipley\nDepartment of Psychology Temple University\nthomas.shipley@temple.edu\n2023 Systems Dynamics Society Conference\nAbstract\nThis paper considers how concepts and approaches from cognitive and learning sciences might\nshed light on how humans think and learn about feedback loops. Cognitive biases that might\ninterfere with one's ability to accurately form individual causal links include tendencies to\nperceive covariation that is direct rather than inverse, to overweight information about presence\nrather than absence, to weight a plausible mechanism over empirical evidence, and to fail to\naccount for what other influences might matter. Cognitive limitations that might compromise\none's ability to merge individual links into a closed loop include working memory limitations,\ntendency to see sequences as linear chains, tendency to look for explanations of phenomena at\nthe level of components, and difficulty comprehending exponential growth. On the more\noptimistic side, cognitive affordances that make feedback loop thinking possible include\nanalogical reasoning, language and categorization, ability to create runnable mental models, and\ndistributed cognition. The paper concludes that the realm of how humans think and learn with\nand about feedback loops is ripe for further cognitive, learning science, and neuroscience\nresearch, and suggests research questions that have the potential to both advance systems\neducation practice and elucidate under-researched capabilities of the mind.\n2\nProblem Statement:\nThe authors have not been able to locate a body of literature or a community of scholars\nin the domain of discipline-based education research (DBER) focused on research into how the\nhuman mind thinks and learns about feedback loops or other key aspects of systems thinking.\nDBER combines rigorous methods of cognitive and learning science research with deep\ngrounding in discipline-specific knowledge, practices, priorities and worldview of a scientific\ndiscipline (National Research Council, 2012). This paper aims to catalyze further DBER-style\nresearch on feedback loop thinking, and to surface researchers who are already working on this\nor related topics in hopes of coalescing a community of practice (Kastens & Manduca, 2017).\nMethodological Approach\nDiscipline-based Education Research (DBER) is now well established in physics,\nchemistry, life sciences and geosciences. This paper's authors were among the founders and early\nadvocates for the field of geoscience education research (GER), with a particular focus on spatial\nthinking (e.g. Kastens, et al, 2009; Shipley, et al, 2013; Resnick, et al, 2018). Although each of\nthe DBERs followed a somewhat different historical pathway to its current status, each grew\nfrom a body of \"practitioners' wisdom,\" comprising informal observations shared among highly\u0002motivated educators about common student misconceptions, effective teaching strategies, student\nattitudes, and so on. The Creative Learning Exchange (http://www.clexchange.org) is rich in\nsuch practitioners' wisdom for systems thinking in K-12 education.\nDuring its formative years, each of the DBERs benefited from a process of introspection\nand collaborative discourse, during which were identified aspects of thinking and practice that\nwere important in the discipline and that could be conceptualized in terms of known capabilities\nof the mind. Kastens & Ishikawa (2006) and Kastens & Manduca (2012) are exemplars of this\napproach and the model for the current paper.\nWe focus this paper around feedback loops, because they are extremely important in\nsystems that are consequential for humanity, are central to systems dynamics, and present\ninteresting cognitive challenges. We begin with cognitive limitations that may make it hard to\nthink about feedback loops, move on to cognitive affordances that nonetheless make it possible,\nand conclude with ideas towards an ambitious research agenda of foundational and applied\nresearch on feedback loop thinking. Our intent is not to be all-encompassing, but rather to make\nthe case that there is an underpopulated research domain with fascinating research questions and\nthe potential to both improve systems education practice and elucidate under-appreciated\ncapabilities of the mind.\nCognitive Limitations:\nReaders who sometimes wear an instructor's hat will probably have encountered learners\nwho struggle to understand the concept of feedback loops in general, or to distinguish between\nreinforcing and balancing loops, or to recognize feedback loops in real world contexts, or to use\nthe feedback loop concept to explain real world phenomena, or to use feedback loops to make\npredictions about how a system will behave under not-yet-observed conditions — or all of the\nabove. This section of the paper explores some of the cognitive limitations that may make such\nthinking challenging.\n3\nWe begin with cognitive biases and limitations that interfere with the ability to create an\nisolated A-->B causal link that is veridical and plausible, and then expand to problems that\nemerge when people try to think about a feedback loop in its entirety. Further problems surely\nemerge when trying to think about multi-loop systems, but those are beyond the scope of this\npaper.\nProblems and biases in considering single A-->B causal links\nEvery feedback loop is made up of multiple individual links of the form A causes or\ninfluences B. In reaching a conclusion that A influences B, humans rely on three sources of\ninsight: observations that A and B consistently covary, pre-existing knowledge about plausible\ncausal mechanisms, and counterfactual reasoning about what else might be responsible for B.\nEach of these sources of information can go astray.\nTendency to perceive covariation that is direct rather than inverse: Kareev (1995)\npublished both a theoretical analysis and empirical evidence that humans find it easier to discern\na positive (direct) correlation than a negative (inverse) correlation between observable attributes\nwithin a series of events or a group of objects. In the objects task, he asked participants to draw\nenvelopes, one by one, from an opaque bag. Each envelope was colored red or green, and each\ncontained a small coin marked with either an X or O. After drawing the envelope, the participant\nhad to guess whether it would contain an X or O coin. They were then allowed to open the\nenvelop and look at the coin. Across the 56 student participants, the experimenters varied the\nrelative frequency of the X's and O's (symmetry) and the strength and sign of the relationship\nbetween the predictor (envelope color) and the criterion (X/O). Over the course of 128\nenvelopes, participants began to pick up the nature of the system they were exploring and guess\ncorrectly more often. Across the full range of relationships, from strongly negative, through\nzero, to strongly positive, the participants' guesses in the final quartile of the data collection\nindicated that they had inferred a more positive correlation between color and coins than the\nactual correlation they had experienced. Might this cognitive bias make it harder for learners to\nperceive \"opposite\" links (aka \"O\" or \"-\" links) than \"same\" links (aka \"S\" or \"+\" links)?\nTendency to overweight information about presence rather than absence: Events that\nhappened or objects that are present are more salient than events that didn't happen or objects\nthat are not present (Spellman & Mandel, 2005). A famous example is in the Sherlock Holmes\nstory, The Adventure of Silver Blaze: \"Inspector Gregory of Scotland Yard: Is there any other\npoint to which you would wish to draw my attention? Holmes: To the curious incident of the dog\nin the night-time. Gregory: The dog did nothing in the night-time. Holmes: That was the curious\nincident.\" Gregory, and everyone else in the story, had failed to note that the dog had not\nbarked. But Holmes did notice this detail, and thus deduced that the horse-thief had been\nsomeone that the dog knew well, the horse's trainer (Doyle, 1894). This bias might make it easier\nto perceive links that enable an effect and harder to perceive links that disable an effect.\nTendency to weight a plausible mechanism over empirical evidence: The strongest causal\nlinks are those that are underlain by both a plausible mechanism that the influence should occur\nand observational evidence that the influence does occur. However, these two kinds of\ninformation come by different pathways, may solidify at different times, and may be handled\ndifferently by the mind. A robust body of research by psychologist Deanna Kuhn and her\nstudents makes the case that when humans judge whether one phenomenon is or is not a cause of\nanother phenomenon, they tend to weight plausible theory more heavily than empirical evidence.\n4\nKuhn's experimental design is to present a group of participants with information about an\nidealized system in which there is one observable outcome and multiple potentially causal\nfactors. Kuhn's team has presented variants of this experiment to all sorts of people: children,\nundergraduates, jurors, people sitting in the waiting room of a train station (Kuhn, 2001, 2004,\n2007, 2010; Kuhn, et al, 2000). They have varied the nature of the system, using both natural\nphenomena (e.g. floods, earthquakes) and human-made (e.g. parties, factories). The\ngeneralizable finding is that if a person believes a plausible, logical story about why A should\ncause B, they are likely to maintain that belief even in the presence of data and observations\nmore consistent with an interpretation that A does not cause B. Conversely, if a person has a\nplausible mechanism for why A should not cause B, they are likely to maintain that belief even\nin the presence of data and observations supporting that A does cause B. Does th",
        "html": "1\nHow does the Human Mind Think and Learn about Feedback Loops?\nKim A. Kastens\nLamont-Doherty Earth Observatory of Columbia University\n(kastens@ldeo.columbia.edu)\nand\nThomas F. Shipley\nDepartment of Psychology Temple University\nthomas.shipley@temple.edu\n2023 Systems Dynamics Society Conference\nAbstract\nThis paper considers how concepts and approaches from cognitive and learning sciences might\nshed light on how humans think and learn about feedback loops. Cognitive biases that might\ninterfere with one's ability to accurately form individual causal links include tendencies to\nperceive covariation that is direct rather than inverse, to overweight information about presence\nrather than absence, to weight a plausible mechanism over empirical evidence, and to fail to\naccount for what other influences might matter. Cognitive limitations that might compromise\none's ability to merge individual links into a closed loop include working memory limitations,\ntendency to see sequences as linear chains, tendency to look for explanations of phenomena at\nthe level of components, and difficulty comprehending exponential growth. On the more\noptimistic side, cognitive affordances that make feedback loop thinking possible include\nanalogical reasoning, language and categorization, ability to create runnable mental models, and\ndistributed cognition. The paper concludes that the realm of how humans think and learn with\nand about feedback loops is ripe for further cognitive, learning science, and neuroscience\nresearch, and suggests research questions that have the potential to both advance systems\neducation practice and elucidate under-researched capabilities of the mind.\n2\nProblem Statement:\nThe authors have not been able to locate a body of literature or a community of scholars\nin the domain of discipline-based education research (DBER) focused on research into how the\nhuman mind thinks and learns about feedback loops or other key aspects of systems thinking.\nDBER combines rigorous methods of cognitive and learning science research with deep\ngrounding in discipline-specific knowledge, practices, priorities and worldview of a scientific\ndiscipline (National Research Council, 2012). This paper aims to catalyze further DBER-style\nresearch on feedback loop thinking, and to surface researchers who are already working on this\nor related topics in hopes of coalescing a community of practice (Kastens & Manduca, 2017).\nMethodological Approach\nDiscipline-based Education Research (DBER) is now well established in physics,\nchemistry, life sciences and geosciences. This paper's authors were among the founders and early\nadvocates for the field of geoscience education research (GER), with a particular focus on spatial\nthinking (e.g. Kastens, et al, 2009; Shipley, et al, 2013; Resnick, et al, 2018). Although each of\nthe DBERs followed a somewhat different historical pathway to its current status, each grew\nfrom a body of \"practitioners' wisdom,\" comprising informal observations shared among highly\u0002motivated educators about common student misconceptions, effective teaching strategies, student\nattitudes, and so on. The Creative Learning Exchange (http://www.clexchange.org) is rich in\nsuch practitioners' wisdom for systems thinking in K-12 education.\nDuring its formative years, each of the DBERs benefited from a process of introspection\nand collaborative discourse, during which were identified aspects of thinking and practice that\nwere important in the discipline and that could be conceptualized in terms of known capabilities\nof the mind. Kastens & Ishikawa (2006) and Kastens & Manduca (2012) are exemplars of this\napproach and the model for the current paper.\nWe focus this paper around feedback loops, because they are extremely important in\nsystems that are consequential for humanity, are central to systems dynamics, and present\ninteresting cognitive challenges. We begin with cognitive limitations that may make it hard to\nthink about feedback loops, move on to cognitive affordances that nonetheless make it possible,\nand conclude with ideas towards an ambitious research agenda of foundational and applied\nresearch on feedback loop thinking. Our intent is not to be all-encompassing, but rather to make\nthe case that there is an underpopulated research domain with fascinating research questions and\nthe potential to both improve systems education practice and elucidate under-appreciated\ncapabilities of the mind.\nCognitive Limitations:\nReaders who sometimes wear an instructor's hat will probably have encountered learners\nwho struggle to understand the concept of feedback loops in general, or to distinguish between\nreinforcing and balancing loops, or to recognize feedback loops in real world contexts, or to use\nthe feedback loop concept to explain real world phenomena, or to use feedback loops to make\npredictions about how a system will behave under not-yet-observed conditions — or all of the\nabove. This section of the paper explores some of the cognitive limitations that may make such\nthinking challenging.\n3\nWe begin with cognitive biases and limitations that interfere with the ability to create an\nisolated A-->B causal link that is veridical and plausible, and then expand to problems that\nemerge when people try to think about a feedback loop in its entirety. Further problems surely\nemerge when trying to think about multi-loop systems, but those are beyond the scope of this\npaper.\nProblems and biases in considering single A-->B causal links\nEvery feedback loop is made up of multiple individual links of the form A causes or\ninfluences B. In reaching a conclusion that A influences B, humans rely on three sources of\ninsight: observations that A and B consistently covary, pre-existing knowledge about plausible\ncausal mechanisms, and counterfactual reasoning about what else might be responsible for B.\nEach of these sources of information can go astray.\nTendency to perceive covariation that is direct rather than inverse: Kareev (1995)\npublished both a theoretical analysis and empirical evidence that humans find it easier to discern\na positive (direct) correlation than a negative (inverse) correlation between observable attributes\nwithin a series of events or a group of objects. In the objects task, he asked participants to draw\nenvelopes, one by one, from an opaque bag. Each envelope was colored red or green, and each\ncontained a small coin marked with either an X or O. After drawing the envelope, the participant\nhad to guess whether it would contain an X or O coin. They were then allowed to open the\nenvelop and look at the coin. Across the 56 student participants, the experimenters varied the\nrelative frequency of the X's and O's (symmetry) and the strength and sign of the relationship\nbetween the predictor (envelope color) and the criterion (X/O). Over the course of 128\nenvelopes, participants began to pick up the nature of the system they were exploring and guess\ncorrectly more often. Across the full range of relationships, from strongly negative, through\nzero, to strongly positive, the participants' guesses in the final quartile of the data collection\nindicated that they had inferred a more positive correlation between color and coins than the\nactual correlation they had experienced. Might this cognitive bias make it harder for learners to\nperceive \"opposite\" links (aka \"O\" or \"-\" links) than \"same\" links (aka \"S\" or \"+\" links)?\nTendency to overweight information about presence rather than absence: Events that\nhappened or objects that are present are more salient than events that didn't happen or objects\nthat are not present (Spellman & Mandel, 2005). A famous example is in the Sherlock Holmes\nstory, The Adventure of Silver Blaze: \"Inspector Gregory of Scotland Yard: Is there any other\npoint to which you would wish to draw my attention? Holmes: To the curious incident of the dog\nin the night-time. Gregory: The dog did nothing in the night-time. Holmes: That was the curious\nincident.\" Gregory, and everyone else in the story, had failed to note that the dog had not\nbarked. But Holmes did notice this detail, and thus deduced that the horse-thief had been\nsomeone that the dog knew well, the horse's trainer (Doyle, 1894). This bias might make it easier\nto perceive links that enable an effect and harder to perceive links that disable an effect.\nTendency to weight a plausible mechanism over empirical evidence: The strongest causal\nlinks are those that are underlain by both a plausible mechanism that the influence should occur\nand observational evidence that the influence does occur. However, these two kinds of\ninformation come by different pathways, may solidify at different times, and may be handled\ndifferently by the mind. A robust body of research by psychologist Deanna Kuhn and her\nstudents makes the case that when humans judge whether one phenomenon is or is not a cause of\nanother phenomenon, they tend to weight plausible theory more heavily than empirical evidence.\n4\nKuhn's experimental design is to present a group of participants with information about an\nidealized system in which there is one observable outcome and multiple potentially causal\nfactors. Kuhn's team has presented variants of this experiment to all sorts of people: children,\nundergraduates, jurors, people sitting in the waiting room of a train station (Kuhn, 2001, 2004,\n2007, 2010; Kuhn, et al, 2000). They have varied the nature of the system, using both natural\nphenomena (e.g. floods, earthquakes) and human-made (e.g. parties, factories). The\ngeneralizable finding is that if a person believes a plausible, logical story about why A should\ncause B, they are likely to maintain that belief even in the presence of data and observations\nmore consistent with an interpretation that A does not cause B. Conversely, if a person has a\nplausible mechanism for why A should not cause B, they are likely to maintain that belief even\nin the presence of data and observations supporting that A does cause B. Does th",
        "metadata": {
          "sections": [],
          "headings": [],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Self-Regulation_Theory",
      "title": "Self-regulation theory",
      "author": "Contributors to Wikimedia projects",
      "published_date": "2024-08-19T12:56:50.000Z",
      "content": {
        "text": "<div><div>\n<p>From Wikipedia, the free encyclopedia</p>\n</div><div>\n<p><b>Self-regulation theory</b> (<b>SRT</b>) is a system of conscious, personal management that involves the process of guiding one's own thoughts, behaviors and feelings to reach goals. Self-regulation consists of several stages. In the stages individuals must function as contributors to their own motivation, behavior, and development within a network of reciprocally interacting influences.\n</p><p><a href=\"https://en.wikipedia.org/wiki/Roy_Baumeister\">Roy Baumeister</a>, one of the leading <a href=\"https://en.wikipedia.org/wiki/Social_psychologist\">social psychologists</a> who have studied self-regulation, claims it has four components: standards of desirable behavior, motivation to meet standards, monitoring of situations and thoughts that precede breaking said standards and lastly, willpower.<sup><a href=\"#cite_note-:0-1\"><span>[</span>1<span>]</span></a></sup> Baumeister along with other colleagues developed three models of self-regulation designed to explain its cognitive accessibility: self-regulation as a knowledge structure, strength, or skill. Studies have been conducted to determine that the strength model is generally supported, because it is a limited resource in the brain and only a given amount of self-regulation can occur until that resource is depleted.<sup><a href=\"#cite_note-:2-2\"><span>[</span>2<span>]</span></a></sup>\n</p><p>SRT can be applied to:\n</p>\n<ul><li>Impulse control, the management of short-term desires. People with low impulse control are prone to acting on immediate desires. This is one route for such people to find their way to jail as many criminal acts occur in the heat of the moment. For non-violent people it can lead to losing friends through careless outbursts, or financial problems caused by making too many impulsive purchases.</li>\n<li>The <a href=\"https://en.wikipedia.org/wiki/Cognitive_bias\">cognitive bias</a> known as <a href=\"https://en.wikipedia.org/wiki/Illusion_of_control\">illusion of control</a>. To the extent that people are driven by internal goals concerned with the exercise of control over their environment, they will seek to reassert control in conditions of chaos, uncertainty or stress. Failing genuine control, one coping strategy will be to fall back on defensive attributions of control—leading to illusions of control (Fenton-O'Creevy et al., 2003).</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Goal_orientation\">Goal attainment and motivation</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Sickness_behavior\">Sickness behavior</a></li></ul>\n<p>SRT consists of several stages. First, the patient deliberately monitors one's own behavior and evaluates how this behavior affects one's health. If the desired effect is not realized, the patient changes personal behavior. If the desired effect is realized, the patient reinforces the effect by continuing the behavior. (Kanfer 1970;1971;1980)<sup>[<i><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Please_clarify\"><span>clarification needed</span></a></i>]</sup>\n</p><p>Another approach is for the patient to realize a personal health issue and understand the factors involved in that issue. The patient must decide upon an action plan for resolving the health issue. The patient will need to deliberately monitor the results in order to appraise the effects, checking for any necessary changes in the action plan. (Leventhal &amp; Nerenz 1984)<sup>[<i><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Please_clarify\"><span>clarification needed</span></a></i>]</sup>\n</p><p>Another factor that can help the patient reach his/her own goal of personal health is to relate to the patient the following: Help them figure out the personal/community views of the illness, appraise the risks involved and give them potential problem-solving/<a href=\"https://en.wikipedia.org/wiki/Coping_skill\">coping skills</a>.<sup><a href=\"#cite_note-Baumeister,_R._F.,_Heatherton,_T._F.,_&amp;_Tice,_D._M._(1994)._''Losing_Control:_How_and_Why_People_Fail_at_Self-Regulation''._San_Diego,_CA:_Academic_Press-3\"><span>[</span>3<span>]</span></a></sup> Four components of self-regulation described by Baumeister et al. (2007) are:\n</p>\n<ul><li><b>Standards</b>: Of desirable behavior.</li>\n<li><b>Motivation</b>: To meet standards.</li>\n<li><b>Monitoring</b>: Of situations and thoughts that precede breaking standards.</li>\n<li><b>Willpower</b>: Internal strength to control urges</li></ul>\n<div><h2>History and contributors</h2><p><span><span>[</span><a href=\"https://en.wikipedia.org/w/index.php?title=Self-regulation_theory&amp;action=edit&amp;section=1\"><span>edit</span></a><span>]</span></span></p></div>\n<p>There have been numerous researchers, psychologists and scientists who have studied self-regulatory processes. <a href=\"https://en.wikipedia.org/wiki/Albert_Bandura\">Albert Bandura</a>, a cognitive psychologist had significant contributions focusing on the acquisition of behaviors that led to the <a href=\"https://en.wikipedia.org/wiki/Social_cognitive_theory\">social cognitive theory</a> and <a href=\"https://en.wikipedia.org/wiki/Social_learning_theory\">social learning theory</a>. His work brought together behavioral and cognitive components in which he concluded that \"humans are able to control their behavior through a process known as self-regulation.\"<sup><a href=\"#cite_note-:1-4\"><span>[</span>4<span>]</span></a></sup> This led to his known process that contained: self observation, judgment and self response. Self observation (also known as <a href=\"https://en.wikipedia.org/wiki/Introspection\">introspection</a>) is a process involving assessing one's own thoughts and feelings in order to inform and motivate the individual to work towards goal setting and become influenced by behavioral changes. <a href=\"https://en.wikipedia.org/wiki/Judgement\">Judgement</a> involves an individual comparing his or her performance to their personal or created standards. Lastly, self-response is applied, in which an individual may reward or punish his or herself for success or failure in meeting standard(s). An example of self-response would be rewarding oneself with an extra slice of pie for doing well on an exam.\n</p>\n<p>According to Schunk (2012), Lev Vygotsky who was a Russian psychologist and was a major influence on the rise of constructivism, believed that self-regulation involves the coordination of cognitive processes such as planning, synthesizing and formulating concepts (Henderson &amp; Cunningham, 1994); however, such coordination does not proceed independently of the individual's social environment and culture. In fact, self-regulation is inclusive of the gradual internalization of language and concepts. Schunk's <i>Learning Theories: An Educational Perspective</i> is stated to give a contemporary and historical overview of learning theories for undergraduate and graduate learners <sup><a href=\"#cite_note-5\"><span>[</span>5<span>]</span></a></sup>\n</p>\n<p>As a widely studied theory, SRT was also greatly impacted by the well-known social psychologist <a href=\"https://en.wikipedia.org/wiki/Roy_Baumeister\">Roy Baumeister</a>. He described the ability to self-regulate as limited in capacity and through this he coined the term <a href=\"https://en.wikipedia.org/wiki/Ego_depletion\">ego depletion</a>. The four components of self-regulation theory described by Roy Baumeister are standards of desirable behavior, motivation to meet standards, monitoring of situations and thoughts that precede breaking standards and willpower, or the internal strength to control urges.<sup><a href=\"#cite_note-:0-1\"><span>[</span>1<span>]</span></a></sup> In Baumeister's paper titled <i>Self-Regulation Failure: An Overview</i>, he express that self-regulation is complex and multifaceted. Baumeister lays out his “three ingredients” of self-regulation as a case for self-regulation failure.\n</p>\n<p>Many studies have been done to test different variables regarding self-regulation. Albert Bandura studied self-regulation before, after and during the response. He created the triangle of reciprocal determinism that includes behavior, environment and the person (cognitive, emotional and physical factors) that all influence one another. Bandura concluded that the processes of goal attainment and motivation stem from an equal interaction of self-observation, self-reaction, self-evaluation and <a href=\"https://en.wikipedia.org/wiki/Self-efficacy\">self-efficacy</a>.<sup><a href=\"#cite_note-:1-4\"><span>[</span>4<span>]</span></a></sup>\n</p><p>In addition to Bandura's work, psychologists Muraven, Tice and Baumeister conducted a study for self control as a limited resource.<sup><a href=\"#cite_note-6\"><span>[</span>6<span>]</span></a></sup> They suggested there were three competing models to self-regulation: self-regulation as a strength, knowledge structure and a skill. In the strength model, they indicated it is possible self-regulation could be considered a strength because it requires willpower and thus is a limited resource. Failure to self-regulate could then be explained by depletion of this resource. For self-regulation as a knowledge structure, they theorized it involves a certain amount of knowledge to exert self control, so as with any learned technique, failure to self-regulate could be explained by insufficient knowledge. Lastly, the model involving self-regulation as a skill referred to self-regulation being built up over time and unable to be diminished; therefore, failure to exert would be explained by a lack of skill. They found that self-regulation as a strength is the most feasible model due to studies that have suggested self-regulation is a limited resource.<sup><a href=\"#cite_note-:2-2\"><span>[</span>2<span>]</span></a></sup>\n</p><p>Dewall, Baumeister, Gailliot and Maner performed a series of experiments instructing participants to perform ego depletion tasks to diminish the self-regulatory resource in the brain, t",
        "html": "<div><div>\n<p>From Wikipedia, the free encyclopedia</p>\n</div><div>\n<p><b>Self-regulation theory</b> (<b>SRT</b>) is a system of conscious, personal management that involves the process of guiding one's own thoughts, behaviors and feelings to reach goals. Self-regulation consists of several stages. In the stages individuals must function as contributors to their own motivation, behavior, and development within a network of reciprocally interacting influences.\n</p><p><a href=\"https://en.wikipedia.org/wiki/Roy_Baumeister\">Roy Baumeister</a>, one of the leading <a href=\"https://en.wikipedia.org/wiki/Social_psychologist\">social psychologists</a> who have studied self-regulation, claims it has four components: standards of desirable behavior, motivation to meet standards, monitoring of situations and thoughts that precede breaking said standards and lastly, willpower.<sup><a href=\"#cite_note-:0-1\"><span>[</span>1<span>]</span></a></sup> Baumeister along with other colleagues developed three models of self-regulation designed to explain its cognitive accessibility: self-regulation as a knowledge structure, strength, or skill. Studies have been conducted to determine that the strength model is generally supported, because it is a limited resource in the brain and only a given amount of self-regulation can occur until that resource is depleted.<sup><a href=\"#cite_note-:2-2\"><span>[</span>2<span>]</span></a></sup>\n</p><p>SRT can be applied to:\n</p>\n<ul><li>Impulse control, the management of short-term desires. People with low impulse control are prone to acting on immediate desires. This is one route for such people to find their way to jail as many criminal acts occur in the heat of the moment. For non-violent people it can lead to losing friends through careless outbursts, or financial problems caused by making too many impulsive purchases.</li>\n<li>The <a href=\"https://en.wikipedia.org/wiki/Cognitive_bias\">cognitive bias</a> known as <a href=\"https://en.wikipedia.org/wiki/Illusion_of_control\">illusion of control</a>. To the extent that people are driven by internal goals concerned with the exercise of control over their environment, they will seek to reassert control in conditions of chaos, uncertainty or stress. Failing genuine control, one coping strategy will be to fall back on defensive attributions of control—leading to illusions of control (Fenton-O'Creevy et al., 2003).</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Goal_orientation\">Goal attainment and motivation</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Sickness_behavior\">Sickness behavior</a></li></ul>\n<p>SRT consists of several stages. First, the patient deliberately monitors one's own behavior and evaluates how this behavior affects one's health. If the desired effect is not realized, the patient changes personal behavior. If the desired effect is realized, the patient reinforces the effect by continuing the behavior. (Kanfer 1970;1971;1980)<sup>[<i><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Please_clarify\"><span>clarification needed</span></a></i>]</sup>\n</p><p>Another approach is for the patient to realize a personal health issue and understand the factors involved in that issue. The patient must decide upon an action plan for resolving the health issue. The patient will need to deliberately monitor the results in order to appraise the effects, checking for any necessary changes in the action plan. (Leventhal &amp; Nerenz 1984)<sup>[<i><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Please_clarify\"><span>clarification needed</span></a></i>]</sup>\n</p><p>Another factor that can help the patient reach his/her own goal of personal health is to relate to the patient the following: Help them figure out the personal/community views of the illness, appraise the risks involved and give them potential problem-solving/<a href=\"https://en.wikipedia.org/wiki/Coping_skill\">coping skills</a>.<sup><a href=\"#cite_note-Baumeister,_R._F.,_Heatherton,_T._F.,_&amp;_Tice,_D._M._(1994)._''Losing_Control:_How_and_Why_People_Fail_at_Self-Regulation''._San_Diego,_CA:_Academic_Press-3\"><span>[</span>3<span>]</span></a></sup> Four components of self-regulation described by Baumeister et al. (2007) are:\n</p>\n<ul><li><b>Standards</b>: Of desirable behavior.</li>\n<li><b>Motivation</b>: To meet standards.</li>\n<li><b>Monitoring</b>: Of situations and thoughts that precede breaking standards.</li>\n<li><b>Willpower</b>: Internal strength to control urges</li></ul>\n<div><h2>History and contributors</h2><p><span><span>[</span><a href=\"https://en.wikipedia.org/w/index.php?title=Self-regulation_theory&amp;action=edit&amp;section=1\"><span>edit</span></a><span>]</span></span></p></div>\n<p>There have been numerous researchers, psychologists and scientists who have studied self-regulatory processes. <a href=\"https://en.wikipedia.org/wiki/Albert_Bandura\">Albert Bandura</a>, a cognitive psychologist had significant contributions focusing on the acquisition of behaviors that led to the <a href=\"https://en.wikipedia.org/wiki/Social_cognitive_theory\">social cognitive theory</a> and <a href=\"https://en.wikipedia.org/wiki/Social_learning_theory\">social learning theory</a>. His work brought together behavioral and cognitive components in which he concluded that \"humans are able to control their behavior through a process known as self-regulation.\"<sup><a href=\"#cite_note-:1-4\"><span>[</span>4<span>]</span></a></sup> This led to his known process that contained: self observation, judgment and self response. Self observation (also known as <a href=\"https://en.wikipedia.org/wiki/Introspection\">introspection</a>) is a process involving assessing one's own thoughts and feelings in order to inform and motivate the individual to work towards goal setting and become influenced by behavioral changes. <a href=\"https://en.wikipedia.org/wiki/Judgement\">Judgement</a> involves an individual comparing his or her performance to their personal or created standards. Lastly, self-response is applied, in which an individual may reward or punish his or herself for success or failure in meeting standard(s). An example of self-response would be rewarding oneself with an extra slice of pie for doing well on an exam.\n</p>\n<p>According to Schunk (2012), Lev Vygotsky who was a Russian psychologist and was a major influence on the rise of constructivism, believed that self-regulation involves the coordination of cognitive processes such as planning, synthesizing and formulating concepts (Henderson &amp; Cunningham, 1994); however, such coordination does not proceed independently of the individual's social environment and culture. In fact, self-regulation is inclusive of the gradual internalization of language and concepts. Schunk's <i>Learning Theories: An Educational Perspective</i> is stated to give a contemporary and historical overview of learning theories for undergraduate and graduate learners <sup><a href=\"#cite_note-5\"><span>[</span>5<span>]</span></a></sup>\n</p>\n<p>As a widely studied theory, SRT was also greatly impacted by the well-known social psychologist <a href=\"https://en.wikipedia.org/wiki/Roy_Baumeister\">Roy Baumeister</a>. He described the ability to self-regulate as limited in capacity and through this he coined the term <a href=\"https://en.wikipedia.org/wiki/Ego_depletion\">ego depletion</a>. The four components of self-regulation theory described by Roy Baumeister are standards of desirable behavior, motivation to meet standards, monitoring of situations and thoughts that precede breaking standards and willpower, or the internal strength to control urges.<sup><a href=\"#cite_note-:0-1\"><span>[</span>1<span>]</span></a></sup> In Baumeister's paper titled <i>Self-Regulation Failure: An Overview</i>, he express that self-regulation is complex and multifaceted. Baumeister lays out his “three ingredients” of self-regulation as a case for self-regulation failure.\n</p>\n<p>Many studies have been done to test different variables regarding self-regulation. Albert Bandura studied self-regulation before, after and during the response. He created the triangle of reciprocal determinism that includes behavior, environment and the person (cognitive, emotional and physical factors) that all influence one another. Bandura concluded that the processes of goal attainment and motivation stem from an equal interaction of self-observation, self-reaction, self-evaluation and <a href=\"https://en.wikipedia.org/wiki/Self-efficacy\">self-efficacy</a>.<sup><a href=\"#cite_note-:1-4\"><span>[</span>4<span>]</span></a></sup>\n</p><p>In addition to Bandura's work, psychologists Muraven, Tice and Baumeister conducted a study for self control as a limited resource.<sup><a href=\"#cite_note-6\"><span>[</span>6<span>]</span></a></sup> They suggested there were three competing models to self-regulation: self-regulation as a strength, knowledge structure and a skill. In the strength model, they indicated it is possible self-regulation could be considered a strength because it requires willpower and thus is a limited resource. Failure to self-regulate could then be explained by depletion of this resource. For self-regulation as a knowledge structure, they theorized it involves a certain amount of knowledge to exert self control, so as with any learned technique, failure to self-regulate could be explained by insufficient knowledge. Lastly, the model involving self-regulation as a skill referred to self-regulation being built up over time and unable to be diminished; therefore, failure to exert would be explained by a lack of skill. They found that self-regulation as a strength is the most feasible model due to studies that have suggested self-regulation is a limited resource.<sup><a href=\"#cite_note-:2-2\"><span>[</span>2<span>]</span></a></sup>\n</p><p>Dewall, Baumeister, Gailliot and Maner performed a series of experiments instructing participants to perform ego depletion tasks to diminish the self-regulatory resource in the brain, t",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "From Wikipedia, the free encyclopediaSelf-regulation theory(SRT) is a system of conscious, personal management that involves the process of guiding one's own thoughts, behaviors and feelings to reach goals. Self-regulation consists of several stages. In the stages individuals must function as contributors to their own motivation, behavior, and development within a network of reciprocally interacting influences.Roy Baumeister, one of the leadingsocial psychologistswho have studied self-regulation",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "From Wikipedia, the free encyclopedia",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Self-regulation theory(SRT) is a system of conscious, personal management that involves the process of guiding one's own thoughts, behaviors and feelings to reach goals. Self-regulation consists of several stages. In the stages individuals must function as contributors to their own motivation, behavior, and development within a network of reciprocally interacting influences.Roy Baumeister, one of the leadingsocial psychologistswho have studied self-regulation, claims it has four components: stan",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "History and contributors[edit]",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "History and contributors",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "research"
    },
    {
      "url": "https://helio.app/ux-research/laws-of-ux/coherence-principle/",
      "title": "The Coherence Principle: Streamline Instructional Design",
      "author": "",
      "published_date": "2024-10-02T21:47:37.000Z",
      "content": {
        "text": "<div><div>\n<p>The Power of the Coherence Principle. This is a powerful concept that has been shown to significantly enhance learning outcomes. By understanding and applying this principle, educators and instructional designers can create more effective learning experiences that promote better understanding, knowledge retention, and transfer of information.</p>\n<div>\n<ul>\n<li>ia-le=””&gt;vel=”1″&gt;<strong>The coherence principle enhances learning efficiency.</strong> By removing irrelevant information and focusing on key concepts, the coherence principle allows learners to absorb essential content more quickly and effectively.</li>\n<li><strong>Clear and logically structured content boosts retention.</strong> When instructional materials are presented in a cohesive, well-organized format, learners can more easily connect new information with prior knowledge, improving long-term retention.</li>\n<li><strong>The coherence principle prevents cognitive overload.</strong> By presenting information in a manageable way and eliminating distractions, learners can process content more easily, reducing mental strain and improving focus.</li>\n<li><strong>Visual aids can support the coherence principle.</strong> Well-designed visual elements such as diagrams and charts can enhance the clarity of instructional materials, helping learners understand relationships between key concepts.</li>\n<li><strong>Coherence facilitates deeper learning and comprehension.</strong> By organizing content logically and clearly, learners can form mental models that promote a deeper understanding of complex subjects, improving knowledge application.</li>\n<li><strong>The coherence principle applies to diverse learning environments.</strong> Beyond text and multimedia, the coherence principle is relevant in simulations, hands-on activities, and virtual reality, where clear objectives and tasks ensure effective learning experiences.</li>\n</ul>\n</div>\n<h2><strong>Understanding the Coherence Principle</strong></h2>\n<p>This is based on the idea that instructional materials should be presented in a cohesive and logically organized manner. When information is presented in a clear, cohesive, and meaningful way, learners are better able to make sense of the material and connect new information to their existing knowledge and experiences.</p>\n<p>Implementation involves structuring content in a way that flows logically, with each piece of information building upon the previous one. This sequential arrangement helps learners grasp complex concepts more easily and enhances their retention of the material over time.</p>\n<p><strong>The Basics of the Coherence Principle</strong></p>\n<p>At its core, this emphasizes the importance of eliminating extraneous and unnecessary information from instructional materials. By removing irrelevant and distracting elements, learners can focus their attention on the essential content and key concepts, leading to better learning outcomes.</p>\n<p>Furthermore, this advocates for the use of visual aids, such as diagrams, charts, and graphs, to supplement textual information. Visual representations can enhance understanding by providing additional context and illustrating relationships between different pieces of information.</p>\n<p><strong>The Psychological Foundation of Coherence Principle</strong></p>\n<p>The coherence principle is rooted in cognitive psychology and information processing theories. It takes into account how the human brain processes and stores information. By aligning instructional design with the natural processes of cognition, this can optimize learning and knowledge acquisition.</p>\n<p>Moreover, the coherence principle underscores the importance of providing learners with opportunities for active engagement and reflection. By encouraging learners to apply newly acquired knowledge in practical exercises or real-world scenarios, educators can reinforce learning outcomes and facilitate deeper understanding of the subject matter.</p>\n<div>\n<p>Subscribe to Closing the Gap—a newsletter to help makers and doers get closer to customers. <a href=\"https://helio.app/newsletters/\">Learn more.</a> </p>\n<p>\n</p><h3>\nThank you for subscribing to Closing the Gap!\n</h3>\n<p></p>\n<p>We believe in protecting your data. Here’s our <a href=\"https://helio.app/privacy/\">Privacy Policy.</a></p>\n</div>\n<h2><strong>The Impact of Coherence Principle on Learning</strong></h2>\n<p>The coherence principle has a profound impact on learning by improving both efficiency and retention of information. By applying this principle, educators can enhance the learning experience and help learners achieve their educational goals more effectively.</p>\n<p>Understanding the coherence principle involves recognizing the importance of presenting information in a clear, logical, and interconnected manner. When educational materials are structured cohesively, learners can more easily follow the flow of information and make meaningful connections between different concepts.</p>\n<p><strong>Enhancing Learning Efficiency</strong></p>\n<p>By eliminating irrelevant and extraneous information, it streamlines the learning process. Learners can focus on relevant concepts and ideas, spend less time on unnecessary details, and grasp the core content more quickly.</p>\n<p>Moreover, the coherence principle helps prevent cognitive overload by presenting information in a manageable and digestible format. This allows learners to process and internalize the material more effectively, leading to improved learning outcomes.</p>\n<p><strong>Improving Information Retention</strong></p>\n<p>When instructional materials are presented cohesively and logically, learners find it easier to organize and connect new information with their existing knowledge. This enhanced organization and connection result in better retention and recall of the material over the long term.</p>\n<p>Furthermore, the coherence principle promotes deeper understanding and comprehension of complex topics. By presenting information in a coherent manner, educators facilitate the construction of mental models that aid in information retention and application in real-world scenarios.<strong> </strong></p>\n<h2><strong>Application of the Coherence Principle</strong></h2>\n<p>The coherence principle is a fundamental concept that holds significant importance in the field of instructional design. It serves as a guiding principle for creating effective learning materials that enhance comprehension and retention among learners. By ensuring that all elements within a learning environment are coherent and aligned with the main message, instructional designers can create a more engaging and impactful educational experience.</p>\n<p>Furthermore, the coherence principle extends beyond just multimedia and text-based learning environments. It can also be applied to other forms of instruction, such as hands-on activities, simulations, and virtual reality experiences. In these contexts, coherence involves ensuring that the learning objectives, tasks, and feedback mechanisms are all seamlessly integrated to provide a cohesive and immersive learning experience.</p>\n<h3><strong>Coherence Principle in Multimedia Learning</strong></h3>\n<p>In multimedia learning, the coherence principle plays a crucial role in guiding the design and development of instructional materials. It emphasizes the importance of aligning all media elements, such as images, animations, and audio, with the main message and learning objectives of the instruction. By maintaining coherence among these elements, multimedia designers can create a more engaging and effective learning environment that enhances student understanding and knowledge retention.</p>\n<p>Moreover, the coherence principle in multimedia learning also highlights the significance of avoiding extraneous or irrelevant information that may distract learners from the core concepts being taught. By focusing on coherence and relevance in multimedia design, instructional designers can create a more streamlined and impactful learning experience for students.</p>\n<h3><strong>Coherence Principle in Text-Based Learning</strong></h3>\n<p>When it comes to text-based learning, the coherence principle underscores the importance of clear organization and logical structure in instructional materials. Well-defined headings, subheadings, and transitions help learners navigate through the content more easily, facilitating better comprehension and retention of information. By following the principles of coherence in text-based learning, instructional designers can create materials that are not only informative but also engaging and accessible to a wide range of learners.</p>\n<p>Furthermore, the coherence principle in text-based learning also emphasizes the need for consistency in terminology, formatting, and style throughout instructional materials. By maintaining a cohesive and uniform presentation, learners are better able to focus on the content itself without being distracted by inconsistencies or discrepancies in the material. This attention to coherence in text-based learning can significantly enhance the overall learning experience and contribute to improved learning outcomes for students.</p>\n<h3><strong>The Limitations of the Coherence Principle</strong></h3>\n<p>While the coherence principle is a powerful tool in instructional design, it is not without its limitations. It is important to understand and address these limitations to maximize the benefits of the coherence principle.</p>\n<p>One key aspect to consider when exploring the limitations of the coherence principle is the potential impact on different learning styles. While some learners may thrive in a streamlined and coherent learning environment, others may struggle with the lack of detailed information and context. This highlights the importance of recognizing the diverse needs of learners and adapting instructional design strategies accordingly.</p>\n<h3><strong",
        "html": "<div><div>\n<p>The Power of the Coherence Principle. This is a powerful concept that has been shown to significantly enhance learning outcomes. By understanding and applying this principle, educators and instructional designers can create more effective learning experiences that promote better understanding, knowledge retention, and transfer of information.</p>\n<div>\n<ul>\n<li>ia-le=””&gt;vel=”1″&gt;<strong>The coherence principle enhances learning efficiency.</strong> By removing irrelevant information and focusing on key concepts, the coherence principle allows learners to absorb essential content more quickly and effectively.</li>\n<li><strong>Clear and logically structured content boosts retention.</strong> When instructional materials are presented in a cohesive, well-organized format, learners can more easily connect new information with prior knowledge, improving long-term retention.</li>\n<li><strong>The coherence principle prevents cognitive overload.</strong> By presenting information in a manageable way and eliminating distractions, learners can process content more easily, reducing mental strain and improving focus.</li>\n<li><strong>Visual aids can support the coherence principle.</strong> Well-designed visual elements such as diagrams and charts can enhance the clarity of instructional materials, helping learners understand relationships between key concepts.</li>\n<li><strong>Coherence facilitates deeper learning and comprehension.</strong> By organizing content logically and clearly, learners can form mental models that promote a deeper understanding of complex subjects, improving knowledge application.</li>\n<li><strong>The coherence principle applies to diverse learning environments.</strong> Beyond text and multimedia, the coherence principle is relevant in simulations, hands-on activities, and virtual reality, where clear objectives and tasks ensure effective learning experiences.</li>\n</ul>\n</div>\n<h2><strong>Understanding the Coherence Principle</strong></h2>\n<p>This is based on the idea that instructional materials should be presented in a cohesive and logically organized manner. When information is presented in a clear, cohesive, and meaningful way, learners are better able to make sense of the material and connect new information to their existing knowledge and experiences.</p>\n<p>Implementation involves structuring content in a way that flows logically, with each piece of information building upon the previous one. This sequential arrangement helps learners grasp complex concepts more easily and enhances their retention of the material over time.</p>\n<p><strong>The Basics of the Coherence Principle</strong></p>\n<p>At its core, this emphasizes the importance of eliminating extraneous and unnecessary information from instructional materials. By removing irrelevant and distracting elements, learners can focus their attention on the essential content and key concepts, leading to better learning outcomes.</p>\n<p>Furthermore, this advocates for the use of visual aids, such as diagrams, charts, and graphs, to supplement textual information. Visual representations can enhance understanding by providing additional context and illustrating relationships between different pieces of information.</p>\n<p><strong>The Psychological Foundation of Coherence Principle</strong></p>\n<p>The coherence principle is rooted in cognitive psychology and information processing theories. It takes into account how the human brain processes and stores information. By aligning instructional design with the natural processes of cognition, this can optimize learning and knowledge acquisition.</p>\n<p>Moreover, the coherence principle underscores the importance of providing learners with opportunities for active engagement and reflection. By encouraging learners to apply newly acquired knowledge in practical exercises or real-world scenarios, educators can reinforce learning outcomes and facilitate deeper understanding of the subject matter.</p>\n<div>\n<p>Subscribe to Closing the Gap—a newsletter to help makers and doers get closer to customers. <a href=\"https://helio.app/newsletters/\">Learn more.</a> </p>\n<p>\n</p><h3>\nThank you for subscribing to Closing the Gap!\n</h3>\n<p></p>\n<p>We believe in protecting your data. Here’s our <a href=\"https://helio.app/privacy/\">Privacy Policy.</a></p>\n</div>\n<h2><strong>The Impact of Coherence Principle on Learning</strong></h2>\n<p>The coherence principle has a profound impact on learning by improving both efficiency and retention of information. By applying this principle, educators can enhance the learning experience and help learners achieve their educational goals more effectively.</p>\n<p>Understanding the coherence principle involves recognizing the importance of presenting information in a clear, logical, and interconnected manner. When educational materials are structured cohesively, learners can more easily follow the flow of information and make meaningful connections between different concepts.</p>\n<p><strong>Enhancing Learning Efficiency</strong></p>\n<p>By eliminating irrelevant and extraneous information, it streamlines the learning process. Learners can focus on relevant concepts and ideas, spend less time on unnecessary details, and grasp the core content more quickly.</p>\n<p>Moreover, the coherence principle helps prevent cognitive overload by presenting information in a manageable and digestible format. This allows learners to process and internalize the material more effectively, leading to improved learning outcomes.</p>\n<p><strong>Improving Information Retention</strong></p>\n<p>When instructional materials are presented cohesively and logically, learners find it easier to organize and connect new information with their existing knowledge. This enhanced organization and connection result in better retention and recall of the material over the long term.</p>\n<p>Furthermore, the coherence principle promotes deeper understanding and comprehension of complex topics. By presenting information in a coherent manner, educators facilitate the construction of mental models that aid in information retention and application in real-world scenarios.<strong> </strong></p>\n<h2><strong>Application of the Coherence Principle</strong></h2>\n<p>The coherence principle is a fundamental concept that holds significant importance in the field of instructional design. It serves as a guiding principle for creating effective learning materials that enhance comprehension and retention among learners. By ensuring that all elements within a learning environment are coherent and aligned with the main message, instructional designers can create a more engaging and impactful educational experience.</p>\n<p>Furthermore, the coherence principle extends beyond just multimedia and text-based learning environments. It can also be applied to other forms of instruction, such as hands-on activities, simulations, and virtual reality experiences. In these contexts, coherence involves ensuring that the learning objectives, tasks, and feedback mechanisms are all seamlessly integrated to provide a cohesive and immersive learning experience.</p>\n<h3><strong>Coherence Principle in Multimedia Learning</strong></h3>\n<p>In multimedia learning, the coherence principle plays a crucial role in guiding the design and development of instructional materials. It emphasizes the importance of aligning all media elements, such as images, animations, and audio, with the main message and learning objectives of the instruction. By maintaining coherence among these elements, multimedia designers can create a more engaging and effective learning environment that enhances student understanding and knowledge retention.</p>\n<p>Moreover, the coherence principle in multimedia learning also highlights the significance of avoiding extraneous or irrelevant information that may distract learners from the core concepts being taught. By focusing on coherence and relevance in multimedia design, instructional designers can create a more streamlined and impactful learning experience for students.</p>\n<h3><strong>Coherence Principle in Text-Based Learning</strong></h3>\n<p>When it comes to text-based learning, the coherence principle underscores the importance of clear organization and logical structure in instructional materials. Well-defined headings, subheadings, and transitions help learners navigate through the content more easily, facilitating better comprehension and retention of information. By following the principles of coherence in text-based learning, instructional designers can create materials that are not only informative but also engaging and accessible to a wide range of learners.</p>\n<p>Furthermore, the coherence principle in text-based learning also emphasizes the need for consistency in terminology, formatting, and style throughout instructional materials. By maintaining a cohesive and uniform presentation, learners are better able to focus on the content itself without being distracted by inconsistencies or discrepancies in the material. This attention to coherence in text-based learning can significantly enhance the overall learning experience and contribute to improved learning outcomes for students.</p>\n<h3><strong>The Limitations of the Coherence Principle</strong></h3>\n<p>While the coherence principle is a powerful tool in instructional design, it is not without its limitations. It is important to understand and address these limitations to maximize the benefits of the coherence principle.</p>\n<p>One key aspect to consider when exploring the limitations of the coherence principle is the potential impact on different learning styles. While some learners may thrive in a streamlined and coherent learning environment, others may struggle with the lack of detailed information and context. This highlights the importance of recognizing the diverse needs of learners and adapting instructional design strategies accordingly.</p>\n<h3><strong",
        "metadata": {
          "sections": [
            {
              "type": "div",
              "content": "The Power of the Coherence Principle. This is a powerful concept that has been shown to significantly enhance learning outcomes. By understanding and applying this principle, educators and instructional designers can create more effective learning experiences that promote better understanding, knowledge retention, and transfer of information.ia-le=””>vel=”1″>The coherence principle enhances learning efficiency.By removing irrelevant information and focusing on key concepts, the coherence princip",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "The Power of the Coherence Principle. This is a powerful concept that has been shown to significantly enhance learning outcomes. By understanding and applying this principle, educators and instructional designers can create more effective learning experiences that promote better understanding, knowledge retention, and transfer of information.ia-le=””>vel=”1″>The coherence principle enhances learning efficiency.By removing irrelevant information and focusing on key concepts, the coherence princip",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "ia-le=””>vel=”1″>The coherence principle enhances learning efficiency.By removing irrelevant information and focusing on key concepts, the coherence principle allows learners to absorb essential content more quickly and effectively.Clear and logically structured content boosts retention.When instructional materials are presented in a cohesive, well-organized format, learners can more easily connect new information with prior knowledge, improving long-term retention.The coherence principle preven",
              "class": [],
              "id": ""
            },
            {
              "type": "div",
              "content": "Subscribe to Closing the Gap—a newsletter to help makers and doers get closer to customers.Learn more.Thank you for subscribing to Closing the Gap!We believe in protecting your data. Here’s ourPrivacy Policy.",
              "class": [],
              "id": ""
            }
          ],
          "headings": [
            {
              "level": "h2",
              "text": "Understanding the Coherence Principle",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Thank you for subscribing to Closing the Gap!",
              "id": ""
            },
            {
              "level": "h2",
              "text": "The Impact of Coherence Principle on Learning",
              "id": ""
            },
            {
              "level": "h2",
              "text": "Application of the Coherence Principle",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Coherence Principle in Multimedia Learning",
              "id": ""
            },
            {
              "level": "h3",
              "text": "Coherence Principle in Text-Based Learning",
              "id": ""
            },
            {
              "level": "h3",
              "text": "The Limitations of the Coherence Principle",
              "id": ""
            },
            {
              "level": "h3",
              "text": "<strong",
              "id": ""
            }
          ],
          "media_counts": {
            "images": 0,
            "videos": 0,
            "audio": 0
          }
        }
      },
      "category": "transcription"
    }
  ]
}